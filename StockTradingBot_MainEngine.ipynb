{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8a377c5",
   "metadata": {},
   "source": [
    "# Parameter Optimizing & Main Engine Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd97885",
   "metadata": {},
   "source": [
    "> *The rests are in private mode.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "013a657a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \u001b[1mThe market is closed or underway.\u001b[1m \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "from datetime import date, datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lazypredict.Supervised import LazyClassifier  # import regression if needed \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, PolynomialFeatures\n",
    "from sklearn.metrics import make_scorer, accuracy_score, fbeta_score, recall_score, f1_score, fbeta_score, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "tfds.disable_progress_bar()\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "\n",
    "def day_check():\n",
    "    checkday = datetime.today().strftime('%A')\n",
    "    return checkday\n",
    "\n",
    "\n",
    "def timechecknow():\n",
    "    # time\n",
    "    ttoday = date.today()\n",
    "    tz_NY = pytz.timezone('America/New_York') \n",
    "    datetime_NY = datetime.now(tz_NY) \n",
    "    speed_hour = int(datetime_NY.strftime(\"%H\"))\n",
    "    speed_minute = int(datetime_NY.strftime(\"%M\"))\n",
    "    speed_second = int(datetime_NY.strftime(\"%S\"))\n",
    "    minutepassed = np.round((speed_hour-9)*60 + (speed_minute-30) + speed_second/60,30)\n",
    "    return minutepassed\n",
    "\n",
    "\n",
    "def tensorflow_model_processing(days, h, o, l, tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features):\n",
    "    # initiate\n",
    "    tf.keras.backend.clear_session() \n",
    "\n",
    "    # build model\n",
    "    layer_neurons = [256, 128, 64, 32, 16, 8]\n",
    "    input_layers_features = tf_X_train.shape[1]\n",
    "    output_layers_features = 1\n",
    "\n",
    "    # model design\n",
    "    tf_model = tf.keras.Sequential()\n",
    "    tf_model.add(tf.keras.layers.Flatten(input_shape=(input_layers_features, 1)))\n",
    "    for neurons in layer_neurons:\n",
    "        tf_model.add(tf.keras.layers.Dense(neurons, activation = h))\n",
    "        tf_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    tf_model.add(tf.keras.layers.Dense(output_layers_features, activation = o))\n",
    "    \n",
    "    # compile\n",
    "    tf_model.compile(optimizer='adam', \n",
    "                     loss = l, \n",
    "                     metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=0.5, threshold=0.5)])\n",
    "\n",
    "    # Stop training when there is no improvement in the validation loss for n consecutive epochs\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_fbeta_score', patience = 10)\n",
    "\n",
    "    # Save the Model with the lowest validation loss\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                                   monitor = 'val_fbeta_score',\n",
    "                                                   save_best_only=True)\n",
    "\n",
    "#         # evaluate loss and fbeta before tuning\n",
    "#         loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'\\n\\nTensorFlow Model Evalution before training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "\n",
    "    # train the model\n",
    "    EPOCHS = 500    \n",
    "    history = tf_model.fit(tf_X_train, tf_y_train, \n",
    "                           epochs = EPOCHS, \n",
    "                           validation_data = (tf_X_val, tf_y_val), \n",
    "                           batch_size = 8, \n",
    "                           verbose = 0, \n",
    "                           callbacks = [early_stopping, save_best])\n",
    "    \n",
    "    # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1, callbacks = [early_stopping])\n",
    "    # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1)\n",
    "\n",
    "    # evaluate loss and fbeta after tuning\n",
    "    loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "    print(f'\\nTensorFlow Model Evalution after training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n\\n\\n\\n')\n",
    "\n",
    "    # predict up or down\n",
    "    prediction = tf_model.predict(pred_features)\n",
    "    print(prediction)\n",
    "    print(f'{days} days moving average applied\\n\\n\\n')\n",
    "    return [days, h, o, l, loss, fbeta, prediction[0][0]]\n",
    "\n",
    "\n",
    "def opt_data_processing(df):\n",
    "    # features to predict\n",
    "    pred_features = df[-1:]\n",
    "\n",
    "    # get the outcome from the tomorrow price\n",
    "    df['Tmr_price'] = df['Stock_price'].shift(-1)\n",
    "    df['classifier_result'] = (df['Tmr_price'] > df['Stock_price']).astype(int)\n",
    "    df.drop(columns = 'Tmr_price', inplace = True)\n",
    "\n",
    "    # features to train &test\n",
    "    df = df[:-1]\n",
    "\n",
    "    # outcome \n",
    "    outcomes = df.pop('classifier_result').values\n",
    "    # filter out unwanted columns\n",
    "    features = df.values\n",
    "\n",
    "    # for non-tf\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 0, stratify = outcomes)\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)  \n",
    "    non_tf_pred_features = sc.transform(pred_features)\n",
    "\n",
    "    # for tf\n",
    "    tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 0, stratify = outcomes)\n",
    "\n",
    "    def tf_normalize(set):  \n",
    "        return tf.keras.utils.normalize(set)\n",
    "    tf_X_train = tf_normalize(tf_X_train)\n",
    "    tf_X_test = tf_normalize(X_test[:int(len(X_test)/2)])\n",
    "    tf_X_val = tf_normalize(X_test[int(len(X_test)/2):])\n",
    "    tf_y_test = y_test[:int(len(y_test)/2)]\n",
    "    tf_y_val = y_test[int(len(y_test)/2):] \n",
    "    tf_pred_features = tf_normalize(pred_features)\n",
    "    \n",
    "    return tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, tf_pred_features\n",
    "\n",
    "\n",
    "def param_optimizer(df):\n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0]) \n",
    "    \n",
    "    # for hidden layer model\n",
    "#     hidden_activation_f = ['relu', 'tanh']\n",
    "#     output_activation_f = ['softmax', 'sigmoid']\n",
    "#     loss_f = ['BinaryCrossentropy', 'Hinge', 'MeanSquaredError']\n",
    "    \n",
    "    # for LSTM model\n",
    "    hidden_activation_f = ['relu'] # n/a\n",
    "    output_activation_f = ['sigmoid']    \n",
    "    loss_f = ['BinaryCrossentropy', 'Hinge', 'MeanSquaredError']\n",
    "\n",
    "    param_tune = []\n",
    "    for days in range(moving_avg_value, moving_avg_value+1):\n",
    "        print(f'\\n\\n########## Initial setting: {days} days moving average ##########\\n\\n')\n",
    "        # ML-Classifier\n",
    "        # Get moving average\n",
    "        mvp = days\n",
    "        mavg = pd.DataFrame()\n",
    "        for column in df.columns[1:]:\n",
    "            mv_change = np.array(df[column])\n",
    "            mv = []\n",
    "            for i in range(len(mv_change)-mvp+1):\n",
    "                mv.append(np.average(mv_change[i:mvp+i]))\n",
    "                i+=1\n",
    "            mavg[column] = pd.DataFrame(mv)\n",
    "        tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features = opt_data_processing(mavg)\n",
    "        \n",
    "        for h in hidden_activation_f:\n",
    "            print(f'>>> Hidden Function: {h}')\n",
    "            for o in output_activation_f:\n",
    "                print(f'>> Output Function: {o}')\n",
    "                for l in loss_f:\n",
    "                    print(f'> Loss Function: {l}')\n",
    "                    param_tune.append(tensorflow_model_processing(days, h, o, l, tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features))\n",
    "                    print('----------------------------------------------------')\n",
    "    return param_tune\n",
    "\n",
    "\n",
    "def mvg_optimizer(df, hidden_activation, output_activation, loss):\n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0])    \n",
    "        \n",
    "        if moving_avg_value == 3:\n",
    "            moving_avg_value = 4\n",
    "    \n",
    "    mvg_tune = []\n",
    "    for days in range(moving_avg_value-3, moving_avg_value+5):\n",
    "        if days < 1:\n",
    "            days = 1\n",
    "        try:\n",
    "            print(f'\\n\\n\\n##### {days} days moving average #####')\n",
    "            # ML-Classifier\n",
    "            # Get moving average\n",
    "            mvp = days\n",
    "            mavg = pd.DataFrame()\n",
    "            for column in df.columns[1:]:\n",
    "                mv_change = np.array(df[column])\n",
    "                mv = []\n",
    "                for i in range(len(mv_change)-mvp+1):\n",
    "                    mv.append(np.average(mv_change[i:mvp+i]))\n",
    "                    i+=1\n",
    "                mavg[column] = pd.DataFrame(mv)\n",
    "            tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features = opt_data_processing(mavg)\n",
    "            tensorflow_model_processing_result = tensorflow_model_processing(days, hidden_activation, output_activation, loss, tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features)\n",
    "            mvg_tune.append(tensorflow_model_processing_result)\n",
    "            print('====================================================')   \n",
    "            print('Loop breaker:', tensorflow_model_processing_result[5])            \n",
    "            if tensorflow_model_processing_result[5] > moving_avg_searchbreak:\n",
    "                print('\\n\\n\\n\\n\\n########## Moving average searching: Break ##########\\n\\n\\n\\n\\n')\n",
    "                break                \n",
    "        except:\n",
    "            print(f'Except >>> {days} moving average')\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(mvg_tune)\n",
    "    df.columns = ['moving_avg', 'hidden_activation', 'output_activation', 'loss_compile', 'loss', 'fbeta', 'result']\n",
    "    \n",
    "    df = df.sort_values('loss').reset_index(drop = True)\n",
    "    df.to_csv('para_mvg_tuned.csv', index = False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def main_param():\n",
    "    voice_message(\"\"\"\\\n",
    "    TensorFlow optimizing starts\"\"\")\n",
    "    from os.path import exists\n",
    "    snp_source_exists = exists('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    if snp_source_exists == True:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    else:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_snp.csv')\n",
    "        \n",
    "    # parameter optimize\n",
    "    df = df_source.copy()\n",
    "    df = pd.DataFrame(param_optimizer(df))\n",
    "    df.columns = ['moving_avg', 'hidden_activation', 'output_activation', 'loss_compile', 'loss', 'fbeta', 'result']\n",
    "    df = df.sort_values('loss').reset_index(drop = True)\n",
    "\n",
    "    param_top_ten = df[df['loss'] == df.head(1).loss.values[0]].sort_values('loss').reset_index(drop = True).head(10)\n",
    "    param_top_ten.to_csv('param_top_ten.csv', index = False)\n",
    "    print(param_top_ten)\n",
    "    \n",
    "    hidden_activation = param_top_ten.hidden_activation[0] # hidden_activation\n",
    "    output_activation = param_top_ten.output_activation[0] # output_activation\n",
    "    loss = param_top_ten.loss_compile[0]      # loss_compile\n",
    "    \n",
    "    print('\\n\\n\\n##################################################')\n",
    "    print(f'• Hidden Layer Activator ==> {hidden_activation}')\n",
    "    print(f'• Output Layer Activator ==> {output_activation}')\n",
    "    print(f'• Loss Compiler          ==> {loss}')\n",
    "    print('##################################################\\n\\n\\n\\n\\n')\n",
    "    \n",
    "    # moving average optimize\n",
    "    final_opt_df = mvg_optimizer(df_source, hidden_activation, output_activation, loss)  \n",
    "    df = final_opt_df.copy()\n",
    "    print(df.sort_values('loss').reset_index(drop = True))\n",
    "    \n",
    "    # plot\n",
    "    df = df.sort_values('moving_avg', ascending = False)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 9)\n",
    "\n",
    "    ax.plot(df.moving_avg, df.fbeta, color = 'blue', marker = 'X')\n",
    "    ax.set_xlabel('Moving Average (days)')\n",
    "    ax.set_ylabel('fbeta', color = 'blue')\n",
    "    # ax.axvline(x = 5, color = 'black', linestyle = '--')\n",
    "    # ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(df.moving_avg, df.loss, color = 'red', marker = 'X')\n",
    "    ax1.set_ylabel('Loss', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    # ax.set_xticklabels(df.Date, rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def pred_tensorflow(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, pred_features):\n",
    "    with tf.device('/GPU:0'):\n",
    "        \n",
    "        # parameter input\n",
    "        opt_param_verified_result = opt_param_verified()\n",
    "        hidden_activation = opt_param_verified_result[0]\n",
    "        output_activation = opt_param_verified_result[1]\n",
    "        loss_compile = opt_param_verified_result[2]        \n",
    "#         print(f'• Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Loss Compiler                    ==> {loss_compile}\\n')\n",
    "        \n",
    "        # initiate\n",
    "        tf.keras.backend.clear_session() \n",
    "        \n",
    "        # build model\n",
    "        layer_neurons = [256, 128, 64, 32, 16, 8]\n",
    "        input_layers_features = tf_X_train.shape[1]\n",
    "        output_layers_features = 1\n",
    "        \n",
    "        # model design\n",
    "        tf_model = tf.keras.Sequential()\n",
    "        tf_model.add(tf.keras.layers.Flatten(input_shape=(input_layers_features, 1)))\n",
    "        for neurons in layer_neurons:\n",
    "            tf_model.add(tf.keras.layers.Dense(neurons, activation = hidden_activation))\n",
    "            tf_model.add(tf.keras.layers.Dropout(0.2))\n",
    "        tf_model.add(tf.keras.layers.Dense(output_layers_features, activation = output_activation))\n",
    "        \n",
    "        # compile\n",
    "        tf_model.compile(optimizer='adam', \n",
    "                         loss = loss_compile, \n",
    "                         metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=0.5, threshold=0.5)])\n",
    "\n",
    "        # Stop training when there is no improvement in the validation loss for n consecutive epochs\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_fbeta_score', patience = 10)\n",
    "\n",
    "        # Save the Model with the lowest validation loss\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                                       monitor = 'val_fbeta_score',\n",
    "                                                       save_best_only=True)\n",
    "\n",
    "#         # evaluate loss and fbeta before tuning\n",
    "#         loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'\\n\\nTensorFlow Model Evalution before training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "\n",
    "        # train the model\n",
    "        EPOCHS = 500    \n",
    "        history = tf_model.fit(tf_X_train, tf_y_train, \n",
    "                               epochs = EPOCHS, \n",
    "                               validation_data = (tf_X_val, tf_y_val), \n",
    "                               batch_size = 8, \n",
    "                               verbose = 0, \n",
    "                               callbacks = [early_stopping, save_best])\n",
    "        \n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1, callbacks = [early_stopping])\n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1)\n",
    "\n",
    "        # evaluate loss and fbeta after tuning\n",
    "        loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'TensorFlow Model Evalution after training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "#         print(f'• Applied_Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Applied_Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Applied_Loss Compiler                    ==> {loss_compile}\\n')\n",
    "#         print(f'• Applied_Moving Average Value (optimized) ==> {moving_avg_value} days\\n')\n",
    "\n",
    "        # predict up or down\n",
    "        prediction = tf_model.predict(pred_features)\n",
    "    \n",
    "    return [fbeta[0], prediction[0][0]]\n",
    "\n",
    "\n",
    "def pred_tensorflow_RNN(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, pred_features):\n",
    "    with tf.device('/GPU:0'):\n",
    "        \n",
    "        # parameter input\n",
    "        opt_param_verified_result = opt_param_verified()\n",
    "        hidden_activation = opt_param_verified_result[0]\n",
    "        output_activation = opt_param_verified_result[1]\n",
    "        loss_compile = opt_param_verified_result[2]        \n",
    "#         print(f'• Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Loss Compiler                    ==> {loss_compile}\\n')\n",
    "        \n",
    "        # initiate\n",
    "        tf.keras.backend.clear_session() \n",
    "        \n",
    "        # build model\n",
    "        layer_neurons = [256, 128, 64, 32, 16, 8]\n",
    "        input_layers_features = tf_X_train.shape[1]\n",
    "        output_layers_features = 1\n",
    "        \n",
    "#         # model design\n",
    "#         tf_model = tf.keras.Sequential()\n",
    "#         tf_model.add(tf.keras.layers.Flatten(input_shape=(input_layers_features, 1)))\n",
    "#         for neurons in layer_neurons:\n",
    "#             tf_model.add(tf.keras.layers.Dense(neurons, activation = hidden_activation))\n",
    "#             tf_model.add(tf.keras.layers.Dropout(0.2))\n",
    "#         tf_model.add(tf.keras.layers.Dense(output_layers_features, activation = output_activation))\n",
    "\n",
    "        # LSTM model design\n",
    "        tf_model = tf.keras.Sequential()    \n",
    "        tf_model.add(LSTM(100, return_sequences = True, input_shape = (input_layers_features, 1)))\n",
    "        tf_model.add(LSTM(100, return_sequences = False))\n",
    "        tf_model.add(Dense(25))\n",
    "        tf_model.add(Dense(output_layers_features, activation = output_activation))\n",
    "        \n",
    "        # compile\n",
    "        tf_model.compile(optimizer='adam', \n",
    "                         loss = loss_compile, \n",
    "                         metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=0.5, threshold=0.5)])\n",
    "\n",
    "        # Stop training when there is no improvement in the validation loss for n consecutive epochs\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_fbeta_score', patience = 10)\n",
    "\n",
    "        # Save the Model with the lowest validation loss\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                                       monitor = 'val_fbeta_score',\n",
    "                                                       save_best_only=True)\n",
    "\n",
    "#         # evaluate loss and fbeta before tuning\n",
    "#         loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'\\n\\nTensorFlow Model Evalution before training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "\n",
    "        # train the model\n",
    "        EPOCHS = 500    \n",
    "        history = tf_model.fit(tf_X_train, tf_y_train, \n",
    "                               epochs = EPOCHS, \n",
    "                               validation_data = (tf_X_val, tf_y_val), \n",
    "                               batch_size = 8, \n",
    "                               verbose = 0, \n",
    "                               callbacks = [early_stopping, save_best])\n",
    "        \n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1, callbacks = [early_stopping])\n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1)\n",
    "\n",
    "        # evaluate loss and fbeta after tuning\n",
    "        loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'TensorFlow Model Evalution after training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "#         print(f'• Applied_Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Applied_Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Applied_Loss Compiler                    ==> {loss_compile}\\n')\n",
    "#         print(f'• Applied_Moving Average Value (optimized) ==> {moving_avg_value} days\\n')\n",
    "\n",
    "        # predict up or down\n",
    "        prediction = tf_model.predict(pred_features)\n",
    "    \n",
    "    return [fbeta[0], prediction[0][0]]\n",
    "\n",
    "\n",
    "def pred_logi(X_train, X_test, y_train, y_test, pred_features):\n",
    "    pipe = Pipeline(steps = [('classifier', LogisticRegression(random_state = 0))])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred) \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = pipe.predict_proba(pred_features)[:,1][0]\n",
    "    \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_svc(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_svc = SVC(random_state = 0, probability = True)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {\"C\": [0.1, 0.5, 1, 3, 5],\n",
    "                  \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  \"degree\": [1, 4]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "    \n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_svc, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)   \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "     \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_rf(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_rf = RandomForestClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {\"max_depth\": [3, None],\n",
    "                  \"n_estimators\": list(range(10, 200)),\n",
    "                  \"max_features\": list(range(1, X_test.shape[1]+1)),\n",
    "                  \"min_samples_split\": list(range(2, 11)),\n",
    "                  \"min_samples_leaf\": list(range(1, 11)),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "    \n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_rf, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)   \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "     \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_ada(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_ada = AdaBoostClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    # look at  setting up your search for n_estimators, learning_rate\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "    param_dist = {\"n_estimators\": [10, 100, 200, 400],\n",
    "                  \"learning_rate\": [0.001, 0.005, .01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 10, 20]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "    \n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_ada, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)       \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "    \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_grd(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_grd = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {'learning_rate': [0.01, 0.02, 0.03],\n",
    "                  'subsample'    : [0.9, 0.5, 0.2],\n",
    "                  'n_estimators' : [100, 500, 1000], \n",
    "                  'max_depth'    : [4, 6, 8]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "\n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_grd, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)       \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "\n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_xgb(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_xg = xgb.XGBClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {'learning_rate'   : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "                  'max_depth'       : [3, 4, 5, 6, 8, 10, 12, 15], \n",
    "                  'min_child_weight': [1, 3, 5, 7],\n",
    "                  'gamma'           : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "                  'colsample_bytree': [0.3, 0.4, 0.5, 0.7]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "\n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_xg, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)       \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "\n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_lr_opt(df):\n",
    "    df = df.drop(columns = ['Date'])\n",
    "\n",
    "    # features to predict\n",
    "    pred_features = df[-1:].drop(columns = ['Stock_price'])\n",
    "\n",
    "    # features to train &test\n",
    "    df = df[:-1]         \n",
    "\n",
    "    # outcome \n",
    "    outcomes = df.pop('Stock_price').values\n",
    "    # filter out unwanted columns\n",
    "    features = df.values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 0)\n",
    "\n",
    "    mse_opt = {}\n",
    "    for degree_value in range(1,4):    \n",
    "        # find the poly degree having the lowest MSE\n",
    "        poly_feat = PolynomialFeatures(degree = degree_value)\n",
    "        X_train_poly = poly_feat.fit_transform(X_train)\n",
    "        X_test_poly = poly_feat.transform(X_test)\n",
    "        poly_model = LinearRegression(fit_intercept = False).fit(X_train_poly, y_train)\n",
    "        y_pred = poly_model.predict(X_test_poly)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_opt[degree_value] = mse\n",
    "\n",
    "    mse = pd.DataFrame([mse_opt]).T\n",
    "    mse.rename(columns = {0:'mse'}, inplace = True)\n",
    "    mse = mse.reset_index()\n",
    "    degree_opt = mse[mse['mse'] == mse.mse.min()]['index'].values[0]\n",
    "\n",
    "    poly_feat = PolynomialFeatures(degree = degree_opt)\n",
    "    X_train_poly = poly_feat.fit_transform(X_train)\n",
    "    X_test_poly = poly_feat.transform(X_test)\n",
    "    poly_model = LinearRegression(fit_intercept = False).fit(X_train_poly, y_train)\n",
    "    y_pred = poly_model.predict(X_test_poly)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    pred_features_poly = poly_feat.fit_transform(pred_features)\n",
    "    prediction = float(poly_model.predict(pred_features_poly)[0])\n",
    "    return [int(features.shape[0]-1), degree_opt, r2, mse, prediction]\n",
    "\n",
    "\n",
    "def pred_lr(df):\n",
    "    opt_tail_value = {}\n",
    "    for i in range(df.shape[0]-30, df.shape[0]):\n",
    "        pred_lr_opt_result = pred_lr_opt(df.tail(i))\n",
    "        opt_tail_value[i] = pred_lr_opt_result[3]\n",
    "#         if pred_lr_opt_result[3] < pred_lr_opt_result[4]*0.03:\n",
    "#             break\n",
    "\n",
    "    tail_value = pd.DataFrame([opt_tail_value]).T\n",
    "    tail_value.rename(columns = {0:'mse'}, inplace = True)\n",
    "    tail_value = tail_value.reset_index()\n",
    "    tail_value = tail_value[tail_value['mse'] == tail_value.mse.min()]['index'].values[0]\n",
    "        \n",
    "    lr_result = pred_lr_opt(df.tail(tail_value))\n",
    "    print('\\n\\n#################################')\n",
    "    print(f'• No. Observations: {lr_result[0]} records')\n",
    "    print(f'• Most recent {tail_value} days')\n",
    "    print(f'• Poly degree: {lr_result[1]}')\n",
    "    print(f'• R squared: {lr_result[2]}')\n",
    "    print(f'• MSE: {lr_result[3]}')\n",
    "    print(f'• Prediction: {lr_result[4]}')    \n",
    "    print('#################################')\n",
    "        \n",
    "    return lr_result[4]\n",
    "\n",
    "\n",
    "def classifer_data_input(df, moving_avg_value):\n",
    "\n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0]) \n",
    "\n",
    "    # ML-Classifier\n",
    "    # Get moving average\n",
    "    mvp = moving_avg_value\n",
    "    mavg = pd.DataFrame()\n",
    "    for column in df.columns[1:]:\n",
    "        mv_change = np.array(df[column])\n",
    "        mv = []\n",
    "        for i in range(len(mv_change)-mvp+1):\n",
    "            mv.append(np.average(mv_change[i:mvp+i]))\n",
    "            i+=1\n",
    "        mavg[column] = pd.DataFrame(mv)\n",
    "    df = mavg\n",
    "\n",
    "    # features to predict\n",
    "    pred_features = df[-1:]\n",
    "\n",
    "    # get the outcome from the tomorrow price\n",
    "    df['Tmr_price'] = df['Stock_price'].shift(-1)\n",
    "    df['classifier_result'] = (df['Tmr_price'] > df['Stock_price']).astype(int)\n",
    "    df.drop(columns = 'Tmr_price', inplace = True)\n",
    "\n",
    "    # features to train &test\n",
    "    df = df[:-1]\n",
    "\n",
    "    # outcome \n",
    "    outcomes = df.pop('classifier_result').values\n",
    "    # filter out unwanted columns\n",
    "    features = df.values\n",
    "\n",
    "    # for non-tf\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 42, stratify = outcomes)\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)  \n",
    "    non_tf_pred_features = sc.transform(pred_features)\n",
    "\n",
    "    # for tf\n",
    "    tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(features, outcomes, test_size=0.2, random_state= 42, stratify = outcomes)\n",
    "\n",
    "    tf_X_train = sc.transform(tf_X_train)\n",
    "    tf_X_test = sc.transform(X_test[:int(len(X_test)/2)])\n",
    "    tf_X_val = sc.transform(X_test[int(len(X_test)/2):])\n",
    "    tf_y_test = y_test[:int(len(y_test)/2)]\n",
    "    tf_y_val = y_test[int(len(y_test)/2):] \n",
    "    tf_pred_features = sc.transform(pred_features)\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    print('tf', end = '                               \\r')\n",
    "    tensorflow = pred_tensorflow(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, tf_pred_features)\n",
    "    print(end = '                                     \\r')    \n",
    "    \n",
    "    print('tf RNN', end = '                               \\r')\n",
    "    tensorflow_RNN = pred_tensorflow_RNN(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('logi', end = '                               \\r')\n",
    "    logi = pred_logi(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('svc', end = '                               \\r')\n",
    "    svc = pred_svc(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('rf', end = '                               \\r')\n",
    "    rf = pred_rf(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('ada', end = '                               \\r')\n",
    "    ada = pred_ada(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('grd', end = '                               \\r')\n",
    "    grd = pred_grd(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')    \n",
    "\n",
    "    print('xgb', end = '                               \\r')\n",
    "    grd = pred_xgb(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')        \n",
    "\n",
    "    result = {}\n",
    "    result['tensorflow'] = tensorflow[1]\n",
    "    result['tensorflow_RNN'] = tensorflow_RNN[1]\n",
    "    result['logi'] = logi[1]\n",
    "    result['svc'] = svc[1]\n",
    "    result['rf'] = rf[1]\n",
    "    result['ada'] = ada[1]\n",
    "    result['grd'] = grd[1]\n",
    "    result['xgb'] = grd[1]\n",
    "\n",
    "    fbeta = {}\n",
    "    fbeta['tensorflow'] = tensorflow[0]\n",
    "    fbeta['tensorflow_RNN'] = tensorflow_RNN[0]\n",
    "    fbeta['logi'] = logi[0]\n",
    "    fbeta['svc'] = svc[0]\n",
    "    fbeta['rf'] = rf[0]\n",
    "    fbeta['ada'] = ada[0]\n",
    "    fbeta['grd'] = grd[0]    \n",
    "    fbeta['xgb'] = grd[0]   \n",
    "\n",
    "    rank = pd.DataFrame([fbeta, result]).T.rename(columns = {0:'fbeta' , 1:'result'})\n",
    "    rank.sort_values('fbeta', ascending = False, inplace = True)\n",
    "    print(f'\\n\\n• Moving Average Days: {mvp}')\n",
    "    print('====================================\\n', rank)\n",
    "    print('====================================')\n",
    "    \n",
    "    print('\\n========[Bullish Conditions]========')\n",
    "    print(f'• Min. fbeta value:  {np.round(modelchoice_minimum_fbeta, 2)}')\n",
    "    print(f'• Min. Result value: {np.round(predict_proba_value, 2)}')\n",
    "    print('------------------------------------')\n",
    "    print(f'• Average fbeta: {min_average_fbeta}')\n",
    "    print(f'• Number of (+) Models: {min_num_positive_models}')\n",
    "    print(f'• Pred. (+) Percent: {min_positive_percent}')    \n",
    "    print('====================================')\n",
    "    rank = rank[rank['fbeta'] >= modelchoice_minimum_fbeta]\n",
    "    \n",
    "    # Proba conversion\n",
    "    rank_prediction = []\n",
    "    for result_prediction in rank['result'].values:\n",
    "        if result_prediction >= np.round(predict_proba_value, 2):\n",
    "            rank_prediction.append(1)\n",
    "        else:\n",
    "            rank_prediction.append(0)\n",
    "    rank['sign'] = rank_prediction\n",
    "    \n",
    "    print(rank)\n",
    "    \n",
    "    average_fbeta = np.round(rank.fbeta.mean(), 2)\n",
    "    print('\\n==============[Result]==============')\n",
    "    print(f'• Average fbeta: {average_fbeta}')\n",
    "    \n",
    "    positive_modes = rank['sign'].sum()\n",
    "    print(f'• Number of (+) Models: {positive_modes}')\n",
    "    \n",
    "    average_result = np.round(rank['sign'].mean(), 2)\n",
    "    print(f'• Pred. (+) Percent: {average_result}')\n",
    "    print('====================================')\n",
    "    \n",
    "    if average_result >= min_positive_percent and positive_modes >= min_num_positive_models and average_fbeta > min_average_fbeta:\n",
    "        print(\"• Result: \" + '\\033[1m' + '(+)' + '\\033[0m')\n",
    "        print('====================================\\n\\n\\n')\n",
    "        return [average_fbeta, 1]\n",
    "    else:\n",
    "        print(\"• Result: \" + '\\033[1m' + '(-)' + '\\033[0m')\n",
    "        print('====================================\\n\\n\\n')\n",
    "        return [average_fbeta, 0]   \n",
    "    \n",
    "\n",
    "def opt_param_verified(): # decide parameters and moving average value\n",
    "    global moving_avg_value\n",
    "    \n",
    "    df = pd.read_csv('para_mvg_tuned.csv')\n",
    "    df.sort_values('loss', inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    hidden_activation = df.hidden_activation[0]\n",
    "    output_activation = df.output_activation[0]\n",
    "    loss_compile = df.loss_compile[0]\n",
    "\n",
    "#     # moving average value based on 'loss'\n",
    "#     df.sort_values('loss', inplace = True)\n",
    "#     df.reset_index(drop = True, inplace = True)\n",
    "#     moving_avg_value = df[df['loss'] == df.head(1).loss.values[0]].sort_values('moving_avg').head(1).moving_avg.values[0]\n",
    "#     with open(\"moving_avg_value.txt\", 'w') as f:\n",
    "#         f.write(str(moving_avg_value))\n",
    "        \n",
    "    # moving average value based on 'fbeta'   \n",
    "    df.sort_values('fbeta', ascending = False, inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    moving_avg_value = df[df['fbeta'] == df.head(1).fbeta.values[0]].sort_values('moving_avg').head(1).moving_avg.values[0]\n",
    "    if moving_avg_value < 0:\n",
    "        moving_avg_value = 1\n",
    "        \n",
    "    if moving_avg_value > moving_avg_value_max:\n",
    "        moving_avg_value = moving_avg_value_max \n",
    "    if moving_avg_value < moving_avg_value_min:\n",
    "        moving_avg_value = moving_avg_value_min \n",
    "    else:\n",
    "        pass        \n",
    "    with open(\"moving_avg_value.txt\", 'w') as f:\n",
    "        f.write(str(moving_avg_value))\n",
    "            \n",
    "    return [hidden_activation, output_activation, loss_compile, moving_avg_value]\n",
    "            \n",
    " \n",
    "def main_engine():    \n",
    "    print('\\u2022 TensorFlow version:', tf.__version__)\n",
    "    print('\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "    print('\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')\n",
    "\n",
    "    print('\\n\\nAlgorithm Test with S&P500 ...')\n",
    "    moving_avg_value = opt_param_verified()[3]\n",
    "\n",
    "    from os.path import exists\n",
    "    snp_source_exists = exists('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    if snp_source_exists == True:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    else:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_snp.csv')\n",
    "\n",
    "    al_test_results = classifer_data_input(df_source, moving_avg_value)\n",
    "    if al_test_results[0] < minimum_fbeta:\n",
    "        print(f'\\n\\nExcept >>> The fbeta average is less than the minimum value:\\n{al_test_results[0]} ==(Improve)==> {minimum_fbeta}')\n",
    "        voice_message(\"\"\"\\\n",
    "        The highest prediction fbeta is less than the mininum value. Code cell break \"\"\")\n",
    "        # code cell break ###############\n",
    "        class StopExecution(Exception):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "\n",
    "        raise StopExecution\n",
    "        # code cell break ###############\n",
    "\n",
    "    else:\n",
    "        print('\\n\\nPrediction Result: Passed\\n\\n')\n",
    "        voice_message(\"\"\"\\\n",
    "        TensorFlow has been optimized\"\"\")\n",
    "        pass\n",
    "\n",
    "    param_result = opt_param_verified()\n",
    "    print(f'• Hidden_activation:   {param_result[0]}')\n",
    "    print(f'• Output_activation:   {param_result[1]}')\n",
    "    print(f'• Loss_compile:        {param_result[2]}')\n",
    "    print(f'• Moving Average Days: {param_result[3]}')\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_engine()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[1m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe11fb1",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a4c7f",
   "metadata": {},
   "source": [
    "*Execution Video*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba35c11",
   "metadata": {},
   "source": [
    "`Settings`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f26cdb",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=4pRMAbvSjow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31a0b4",
   "metadata": {},
   "source": [
    "`Realizing Gain`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0a913",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=QapupAMCR7U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e810c2",
   "metadata": {},
   "source": [
    "`Daily Blue-Chip Finding`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9b8cc",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=kwU3WXKIdAQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dfb2c8",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
