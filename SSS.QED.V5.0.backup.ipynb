{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d12e0ae3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fbf8b68b",
   "metadata": {
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#########################\n",
    "mostrecentdate = 20230824\n",
    "currentdate =    20230825\n",
    "#########################\n",
    "\n",
    "import calendar\n",
    "calendar.setfirstweekday(calendar.SUNDAY)\n",
    "print('• mostrecentdate: ', mostrecentdate)\n",
    "print('• currentdate:    ', currentdate)\n",
    "print('\\n', calendar.month(int(str(currentdate)[:4]), int(str(currentdate)[4:6])))\n",
    "\n",
    "# code cell warpping\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Input Date\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245dae8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d90687e",
   "metadata": {},
   "source": [
    "Robin_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32576ea9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Contains all functions for placing orders for stocks, options, and crypto.\"\"\"\n",
    "from uuid import uuid4\n",
    "\n",
    "from robin_stocks.robinhood.crypto import *\n",
    "from robin_stocks.robinhood.helper import *\n",
    "from robin_stocks.robinhood.profiles import *\n",
    "from robin_stocks.robinhood.stocks import *\n",
    "from robin_stocks.robinhood.urls import *\n",
    "\n",
    "@login_required\n",
    "def get_all_stock_orders(info=None):\n",
    "    \"\"\"Returns a list of all the orders that have been processed for the account.\n",
    "\n",
    "    :param info: Will filter the results to get a specific value.\n",
    "    :type info: Optional[str]\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for each order. If info parameter is provided, \\\n",
    "    a list of strings is returned where the strings are the value of the key that matches info.\n",
    "\n",
    "    \"\"\"\n",
    "    url = orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "    return(filter_data(data, info))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_all_option_orders(info=None):\n",
    "    \"\"\"Returns a list of all the option orders that have been processed for the account.\n",
    "\n",
    "    :param info: Will filter the results to get a specific value.\n",
    "    :type info: Optional[str]\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for each option order. If info parameter is provided, \\\n",
    "    a list of strings is returned where the strings are the value of the key that matches info.\n",
    "\n",
    "    \"\"\"\n",
    "    url = option_orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "    return(filter_data(data, info))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_all_crypto_orders(info=None):\n",
    "    \"\"\"Returns a list of all the crypto orders that have been processed for the account.\n",
    "\n",
    "    :param info: Will filter the results to get a specific value.\n",
    "    :type info: Optional[str]\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for each option order. If info parameter is provided, \\\n",
    "    a list of strings is returned where the strings are the value of the key that matches info.\n",
    "\n",
    "    \"\"\"\n",
    "    url = crypto_orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "    return(filter_data(data, info))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_all_open_stock_orders(info=None):\n",
    "    \"\"\"Returns a list of all the orders that are currently open.\n",
    "\n",
    "    :param info: Will filter the results to get a specific value.\n",
    "    :type info: Optional[str]\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for each order. If info parameter is provided, \\\n",
    "    a list of strings is returned where the strings are the value of the key that matches info.\n",
    "\n",
    "    \"\"\"\n",
    "    url = orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    data = [item for item in data if item['cancel'] is not None]\n",
    "\n",
    "    return(filter_data(data, info))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_all_open_option_orders(info=None, account_number=None):\n",
    "    \"\"\"Returns a list of all the orders that are currently open.\n",
    "\n",
    "    :param info: Will filter the results to get a specific value.\n",
    "    :type info: Optional[str]\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for each order. If info parameter is provided, \\\n",
    "    a list of strings is returned where the strings are the value of the key that matches info.\n",
    "\n",
    "    \"\"\"\n",
    "    url = option_orders_url(account_number=account_number)\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    data = [item for item in data if item['cancel_url'] is not None]\n",
    "\n",
    "    return(filter_data(data, info))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_all_open_crypto_orders(info=None):\n",
    "    \"\"\"Returns a list of all the crypto orders that have been processed for the account.\n",
    "\n",
    "    :param info: Will filter the results to get a specific value.\n",
    "    :type info: Optional[str]\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for each option order. If info parameter is provided, \\\n",
    "    a list of strings is returned where the strings are the value of the key that matches info.\n",
    "\n",
    "    \"\"\"\n",
    "    url = crypto_orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    data = [item for item in data if item['cancel_url'] is not None]\n",
    "\n",
    "    return(filter_data(data, info))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_stock_order_info(orderID):\n",
    "    \"\"\"Returns the information for a single order.\n",
    "\n",
    "    :param orderID: The ID associated with the order. Can be found using get_all_orders(info=None) or get_all_orders(info=None).\n",
    "    :type orderID: str\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for the order.\n",
    "\n",
    "    \"\"\"\n",
    "    url = orders_url(orderID)\n",
    "    data = request_get(url)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_option_order_info(order_id):\n",
    "    \"\"\"Returns the information for a single option order.\n",
    "\n",
    "    :param order_id: The ID associated with the option order.\n",
    "    :type order_id: str\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for the order.\n",
    "\n",
    "    \"\"\"\n",
    "    url = option_orders_url(order_id)\n",
    "    data = request_get(url)\n",
    "    return data\n",
    "\n",
    "\n",
    "@login_required\n",
    "def get_crypto_order_info(order_id):\n",
    "    \"\"\"Returns the information for a single crypto order.\n",
    "\n",
    "    :param order_id: The ID associated with the option order.\n",
    "    :type order_id: str\n",
    "    :returns: Returns a list of dictionaries of key/value pairs for the order.\n",
    "\n",
    "    \"\"\"\n",
    "    url = crypto_orders_url(order_id)\n",
    "    data = request_get(url)\n",
    "    return data\n",
    "\n",
    "\n",
    "@login_required\n",
    "def find_stock_orders(**arguments):\n",
    "    \"\"\"Returns a list of orders that match the keyword parameters.\n",
    "\n",
    "    :param arguments: Variable length of keyword arguments. EX. find_orders(symbol='FB',cancel=None,quantity=1)\n",
    "    :type arguments: str\n",
    "    :returns: Returns a list of orders.\n",
    "\n",
    "    \"\"\" \n",
    "    url = orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    if (len(arguments) == 0):\n",
    "        return(data)\n",
    "\n",
    "    for item in data:\n",
    "        item['quantity'] = str(float(item['quantity']))\n",
    "        item['cumulative_quantity'] = str(float(item['cumulative_quantity']))\n",
    "\n",
    "    if 'symbol' in arguments.keys():\n",
    "        arguments['instrument'] = get_instruments_by_symbols(\n",
    "            arguments['symbol'], info='url')[0]\n",
    "        del arguments['symbol']\n",
    "\n",
    "    if 'quantity' in arguments.keys():\n",
    "        arguments['quantity'] = str(arguments['quantity'])\n",
    "\n",
    "    stop = len(arguments.keys())-1\n",
    "    list_of_orders = []\n",
    "    for item in data:\n",
    "        for i, (key, value) in enumerate(arguments.items()):\n",
    "            if key not in item:\n",
    "                print(error_argument_not_key_in_dictionary(key), file=get_output())\n",
    "                return([None])\n",
    "            if value != item[key]:\n",
    "                break\n",
    "            if i == stop:\n",
    "                list_of_orders.append(item)\n",
    "\n",
    "    return(list_of_orders)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def cancel_stock_order(orderID):\n",
    "    \"\"\"Cancels a specific order.\n",
    "\n",
    "    :param orderID: The ID associated with the order. Can be found using get_all_stock_orders(info=None).\n",
    "    :type orderID: str\n",
    "    :returns: Returns the order information for the order that was cancelled.\n",
    "\n",
    "    \"\"\" \n",
    "    url = cancel_url(orderID)\n",
    "    data = request_post(url)\n",
    "\n",
    "    if data:\n",
    "        print('Order '+str(orderID)+' cancelled', file=get_output())\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def cancel_option_order(orderID):\n",
    "    \"\"\"Cancels a specific option order.\n",
    "\n",
    "    :param orderID: The ID associated with the order. Can be found using get_all_option_orders(info=None).\n",
    "    :type orderID: str\n",
    "    :returns: Returns the order information for the order that was cancelled.\n",
    "\n",
    "    \"\"\" \n",
    "    url = option_cancel_url(orderID)\n",
    "    data = request_post(url)\n",
    "\n",
    "    if data:\n",
    "        print('Order '+str(orderID)+' cancelled', file=get_output())\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def cancel_crypto_order(orderID):\n",
    "    \"\"\"Cancels a specific crypto order.\n",
    "\n",
    "    :param orderID: The ID associated with the order. Can be found using get_all_crypto_orders(info=None).\n",
    "    :type orderID: str\n",
    "    :returns: Returns the order information for the order that was cancelled.\n",
    "\n",
    "    \"\"\" \n",
    "    url = crypto_cancel_url(orderID)\n",
    "    data = request_post(url)\n",
    "\n",
    "    if data:\n",
    "        print('Order '+str(orderID)+' cancelled', file=get_output())\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def cancel_all_stock_orders():\n",
    "    \"\"\"Cancels all stock orders.\n",
    "\n",
    "    :returns: The list of orders that were cancelled.\n",
    "\n",
    "    \"\"\" \n",
    "    url = orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    data = [item for item in data if item['cancel'] is not None]\n",
    "\n",
    "    for item in data:\n",
    "        request_post(item['cancel'])\n",
    "\n",
    "    print('All Stock Orders Cancelled', file=get_output())\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def cancel_all_option_orders():\n",
    "    \"\"\"Cancels all option orders.\n",
    "\n",
    "    :returns: Returns the order information for the orders that were cancelled.\n",
    "\n",
    "    \"\"\" \n",
    "    url = option_orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    data = [item for item in data if item['cancel_url'] is not None]\n",
    "\n",
    "    for item in data:\n",
    "        request_post(item['cancel_url'])\n",
    "\n",
    "    print('All Option Orders Cancelled', file=get_output())\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def cancel_all_crypto_orders():\n",
    "    \"\"\"Cancels all crypto orders.\n",
    "\n",
    "    :returns: Returns the order information for the orders that were cancelled.\n",
    "\n",
    "    \"\"\" \n",
    "    url = crypto_orders_url()\n",
    "    data = request_get(url, 'pagination')\n",
    "\n",
    "    data = [item for item in data if item['cancel_url'] is not None]\n",
    "\n",
    "    for item in data:\n",
    "        request_post(item['cancel_url'])\n",
    "\n",
    "    print('All Crypto Orders Cancelled', file=get_output())\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_market(symbol, quantity, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a market order to be executed immediately.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to buy.\n",
    "    :type quantity: int\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"buy\", None, None, account_number, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_fractional_by_quantity(symbol, quantity, account_number=None, timeInForce='gfd', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a market order to be executed immediately for fractional shares by specifying the amount that you want to trade.\n",
    "    Good for share fractions up to 6 decimal places. Robinhood does not currently support placing limit, stop, or stop loss orders\n",
    "    for fractional trades.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param quantity: The amount of the fractional shares you want to buy.\n",
    "    :type quantity: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"buy\", account_number, None, None, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_fractional_by_price(symbol, amountInDollars, account_number=None, timeInForce='gfd', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a market order to be executed immediately for fractional shares by specifying the amount in dollars that you want to trade.\n",
    "    Good for share fractions up to 6 decimal places. Robinhood does not currently support placing limit, stop, or stop loss orders\n",
    "    for fractional trades.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param amountInDollars: The amount in dollars of the fractional shares you want to buy.\n",
    "    :type amountInDollars: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    if amountInDollars < 1:\n",
    "        print(\"ERROR: Fractional share price should meet minimum 1.00.\", file=get_output())\n",
    "        return None\n",
    "\n",
    "    # turn the money amount into decimal number of shares\n",
    "    price = next(iter(get_latest_price(symbol, 'ask_price', extendedHours)), 0.00)\n",
    "    fractional_shares = 0 if (price == 0.00) else round_price(amountInDollars/float(price))\n",
    "    \n",
    "    return order(symbol, fractional_shares, \"buy\", account_number, None, None, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_limit(symbol, quantity, limitPrice, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a limit order to be executed once a certain price is reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to buy.\n",
    "    :type quantity: int\n",
    "    :param limitPrice: The price to trigger the buy order.\n",
    "    :type limitPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"buy\", account_number, limitPrice, None, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_stop_loss(symbol, quantity, stopPrice, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a stop order to be turned into a market order once a certain stop price is reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to buy.\n",
    "    :type quantity: int\n",
    "    :param stopPrice: The price to trigger the market order.\n",
    "    :type stopPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"buy\", account_number, None, stopPrice, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_stop_limit(symbol, quantity, limitPrice, stopPrice, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a stop order to be turned into a limit order once a certain stop price is reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to buy.\n",
    "    :type quantity: int\n",
    "    :param limitPrice: The price to trigger the market order.\n",
    "    :type limitPrice: float\n",
    "    :param stopPrice: The price to trigger the limit order.\n",
    "    :type stopPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"buy\", account_number, limitPrice, stopPrice, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_trailing_stop(symbol, quantity, trailAmount, trailType='percentage', timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a trailing stop buy order to be turned into a market order when traling stop price reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to buy.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to buy.\n",
    "    :type quantity: int\n",
    "    :param trailAmount: how much to trail by; could be percentage or dollar value depending on trailType\n",
    "    :type trailAmount: float\n",
    "    :param trailType: could be \"amount\" or \"percentage\"\n",
    "    :type trailType: str\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "    \"\"\"\n",
    "    return order_trailing_stop(symbol, quantity, \"buy\", trailAmount, trailType, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_market(symbol, quantity, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a market order to be executed immediately.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to sell.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to sell.\n",
    "    :type quantity: int\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"sell\", None, None, account_number, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_fractional_by_quantity(symbol, quantity, account_number=None, timeInForce='gfd', priceType='bid_price', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a market order to be executed immediately for fractional shares by specifying the amount that you want to trade.\n",
    "    Good for share fractions up to 6 decimal places. Robinhood does not currently support placing limit, stop, or stop loss orders\n",
    "    for fractional trades.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param quantity: The amount of the fractional shares you want to buy.\n",
    "    :type quantity: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"sell\", account_number, None, None, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_fractional_by_price(symbol, amountInDollars, account_number=None, timeInForce='gfd', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a market order to be executed immediately for fractional shares by specifying the amount in dollars that you want to trade.\n",
    "    Good for share fractions up to 6 decimal places. Robinhood does not currently support placing limit, stop, or stop loss orders\n",
    "    for fractional trades.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to purchase.\n",
    "    :type symbol: str\n",
    "    :param amountInDollars: The amount in dollars of the fractional shares you want to buy.\n",
    "    :type amountInDollars: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    if amountInDollars < 1:\n",
    "        print(\"ERROR: Fractional share price should meet minimum 1.00.\", file=get_output())\n",
    "        return None\n",
    "    # turn the money amount into decimal number of shares\n",
    "    price = next(iter(get_latest_price(symbol, 'bid_price', extendedHours)), 0.00)\n",
    "    fractional_shares = 0 if (price == 0.00) else round_price(amountInDollars/float(price))\n",
    "\n",
    "    return order(symbol, fractional_shares, \"sell\", account_number, None, None, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_limit(symbol, quantity, limitPrice, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a limit order to be executed once a certain price is reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to sell.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to sell.\n",
    "    :type quantity: int\n",
    "    :param limitPrice: The price to trigger the sell order.\n",
    "    :type limitPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"sell\", account_number, limitPrice, None, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_stop_loss(symbol, quantity, stopPrice, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a stop order to be turned into a market order once a certain stop price is reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to sell.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to sell.\n",
    "    :type quantity: int\n",
    "    :param stopPrice: The price to trigger the market order.\n",
    "    :type stopPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"sell\", account_number, None, stopPrice, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_stop_limit(symbol, quantity, limitPrice, stopPrice, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a stop order to be turned into a limit order once a certain stop price is reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to sell.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to sell.\n",
    "    :type quantity: int\n",
    "    :param limitPrice: The price to trigger the market order.\n",
    "    :type limitPrice: float\n",
    "    :param stopPrice: The price to trigger the limit order.\n",
    "    :type stopPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order(symbol, quantity, \"sell\", account_number, limitPrice, stopPrice, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_trailing_stop(symbol, quantity, trailAmount, trailType='percentage', timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a trailing stop sell order to be turned into a market order when traling stop price reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to sell.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to sell.\n",
    "    :type quantity: int\n",
    "    :param trailAmount: how much to trail by; could be percentage or dollar value depending on trailType\n",
    "    :type trailAmount: float\n",
    "    :param trailType: could be \"amount\" or \"percentage\"\n",
    "    :type trailType: str\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "    \"\"\"\n",
    "    return order_trailing_stop(symbol, quantity, \"sell\", trailAmount, trailType, timeInForce, extendedHours, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_trailing_stop(symbol, quantity, side, trailAmount, trailType='percentage', account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"Submits a trailing stop order to be turned into a market order when traling stop price reached.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to trade.\n",
    "    :type quantity: int\n",
    "    :param side: buy or sell\n",
    "    :type side: str\n",
    "    :param trailAmount: how much to trail by; could be percentage or dollar value depending on trailType\n",
    "    :type trailAmount: float\n",
    "    :param trailType: could be \"amount\" or \"percentage\"\n",
    "    :type trailType: str\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "        trailAmount = float(trailAmount)\n",
    "    except AttributeError as message:\n",
    "        print(message)\n",
    "        return None\n",
    "\n",
    "    stock_price = round_price(get_latest_price(symbol, extendedHours)[0])\n",
    "\n",
    "    # find stop price based on whether trailType is \"amount\" or \"percentage\" and whether its buy or sell\n",
    "    percentage = 0\n",
    "    try:\n",
    "        if trailType == 'amount':\n",
    "            margin = trailAmount\n",
    "        else:\n",
    "            margin = stock_price * trailAmount * 0.01\n",
    "            percentage = trailAmount\n",
    "    except Exception as e:\n",
    "        print('ERROR: {}'.format(e))\n",
    "        return None\n",
    "\n",
    "    stopPrice = stock_price + margin if side == \"buy\" else stock_price - margin\n",
    "    stopPrice = round_price(stopPrice)\n",
    "\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'instrument': get_instruments_by_symbols(symbol, info='url')[0],\n",
    "        'symbol': symbol,\n",
    "        'quantity': quantity,\n",
    "        'ref_id': str(uuid4()),\n",
    "        'type': 'market',\n",
    "        'stop_price': stopPrice,\n",
    "        'time_in_force': timeInForce,\n",
    "        'trigger': 'stop',\n",
    "        'side': side,\n",
    "        'extended_hours': extendedHours\n",
    "    }\n",
    "\n",
    "    if side == \"buy\":\n",
    "        # price should be greater than stopPrice, adding a 5% threshold\n",
    "        payload['price'] = round_price(stopPrice * 1.05)\n",
    "\n",
    "    if trailType == 'amount':\n",
    "        payload['trailing_peg'] = {'type': 'price', 'price': {'amount': trailAmount, 'currency_code': 'USD'}}\n",
    "    else:\n",
    "        payload['trailing_peg'] = {'type': 'percentage', 'percentage': str(percentage)}\n",
    "\n",
    "    url = orders_url()\n",
    "    data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "\n",
    "    return (data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order(symbol, quantity, side, limitPrice=None, stopPrice=None, account_number=None, timeInForce='gtc', extendedHours=False, jsonify=True):\n",
    "    \"\"\"A generic order function.\n",
    "\n",
    "    :param symbol: The stock ticker of the stock to sell.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of stocks to sell.\n",
    "    :type quantity: int\n",
    "    :param side: Either 'buy' or 'sell'\n",
    "    :type side: str\n",
    "    :param limitPrice: The price to trigger the market order.\n",
    "    :type limitPrice: float\n",
    "    :param stopPrice: The price to trigger the limit or market order.\n",
    "    :type stopPrice: float\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day.\n",
    "    :type timeInForce: str\n",
    "    :param extendedHours: Premium users only. Allows trading during extended hours. Should be true or false.\n",
    "    :type extendedHours: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the purchase or selling of stocks, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "\n",
    "    orderType = \"market\"\n",
    "    trigger = \"immediate\"\n",
    "\n",
    "    if side == \"buy\":\n",
    "        priceType = \"ask_price\"\n",
    "    else:\n",
    "        priceType = \"bid_price\"\n",
    "\n",
    "    if limitPrice and stopPrice:\n",
    "        price = round_price(limitPrice)\n",
    "        stopPrice = round_price(stopPrice)\n",
    "        orderType = \"limit\"\n",
    "        trigger = \"stop\"\n",
    "    elif limitPrice:\n",
    "        price = round_price(limitPrice)\n",
    "        orderType = \"limit\"\n",
    "    elif stopPrice:\n",
    "        stopPrice = round_price(stopPrice)\n",
    "        if side == \"buy\":\n",
    "            price = stopPrice\n",
    "        else:\n",
    "            price = None\n",
    "        trigger = \"stop\"\n",
    "    else:\n",
    "        price = round_price(next(iter(get_latest_price(symbol, priceType, extendedHours)), 0.00))\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'instrument': get_instruments_by_symbols(symbol, info='url')[0],\n",
    "        'symbol': symbol,\n",
    "        'price': price,\n",
    "        'quantity': quantity,\n",
    "        'ref_id': str(uuid4()),\n",
    "        'type': orderType,\n",
    "        'stop_price': stopPrice,\n",
    "        'time_in_force': timeInForce,\n",
    "        'trigger': trigger,\n",
    "        'side': side,\n",
    "        'extended_hours': extendedHours\n",
    "    }\n",
    "    # BEGIN PATCH FOR NEW ROBINHOOD BUY FORM (GuitarGuyChrisB 5/26/2023)\n",
    "    if side == \"buy\":\n",
    "        payload['order_form_version'] = \"4\"\n",
    "        payload['preset_percent_limit'] = \"0.05\"\n",
    "    # END PATCH FOR NEW ROBINHOOD BUY FORM (GuitarGuyChrisB 5/26/2023)\n",
    "\n",
    "     \n",
    "    ##################################################################\n",
    "    # FIX MARKEY SELL AND BUY ORDER\n",
    "    if orderType == 'market' and side == 'buy':\n",
    "        payload['type'] = 'limit' \n",
    "    if orderType == 'market' and side == 'sell':\n",
    "        payload['type'] = 'limit' \n",
    "    ##################################################################\n",
    "    \n",
    "    url = orders_url()\n",
    "\n",
    "\n",
    "    data = request_post(url, payload, jsonify_data=jsonify)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_option_credit_spread(price, symbol, quantity, spread, timeInForce='gtc', account_number=None, jsonify=True):\n",
    "    \"\"\"Submits a limit order for an option credit spread.\n",
    "\n",
    "    :param price: The limit price to trigger a sell of the option.\n",
    "    :type price: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to sell.\n",
    "    :type quantity: int\n",
    "    :param spread: A dictionary of spread options with the following keys: \\n\n",
    "        - expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\\n\n",
    "        - strike: The strike price of the option.\\n\n",
    "        - optionType: This should be 'call' or 'put'.\\n\n",
    "        - effect: This should be 'open' or 'close'.\\n\n",
    "        - action: This should be 'buy' or 'sell'.\n",
    "    :type spread: dict\n",
    "    :param timeInForce: Changes how long the order will be in effect for. \\\n",
    "     'gtc' = good until cancelled. \\\n",
    "     'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' = execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the trading of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "    \"\"\"\n",
    "    return(order_option_spread(\"credit\", price, symbol, quantity, spread, timeInForce, account_number, jsonify))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_option_debit_spread(price, symbol, quantity, spread, timeInForce='gtc', account_number=None, jsonify=True):\n",
    "    \"\"\"Submits a limit order for an option debit spread.\n",
    "\n",
    "    :param price: The limit price to trigger a sell of the option.\n",
    "    :type price: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to sell.\n",
    "    :type quantity: int\n",
    "    :param spread: A dictionary of spread options with the following keys: \\n\n",
    "        - expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\\n\n",
    "        - strike: The strike price of the option.\\n\n",
    "        - optionType: This should be 'call' or 'put'.\\n\n",
    "        - effect: This should be 'open' or 'close'.\\n\n",
    "        - action: This should be 'buy' or 'sell'.\n",
    "    :type spread: dict\n",
    "    :param timeInForce: Changes how long the order will be in effect for.\n",
    "     'gtc' = good until cancelled. \\\n",
    "     'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the trading of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "    \"\"\"\n",
    "    return(order_option_spread(\"debit\", price, symbol, quantity, spread, timeInForce, account_number, jsonify))\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_option_spread(direction, price, symbol, quantity, spread, account_number=None, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for an option spread. i.e. place a debit / credit spread\n",
    "\n",
    "    :param direction: Can be \"credit\" or \"debit\".\n",
    "    :type direction: str\n",
    "    :param price: The limit price to trigger a trade of the option.\n",
    "    :type price: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to trade.\n",
    "    :type quantity: int\n",
    "    :param spread: A dictionary of spread options with the following keys: \\n\n",
    "        - expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\\n\n",
    "        - strike: The strike price of the option.\\n\n",
    "        - optionType: This should be 'call' or 'put'.\\n\n",
    "        - effect: This should be 'open' or 'close'.\\n\n",
    "        - action: This should be 'buy' or 'sell'.\n",
    "    :type spread: dict\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for.\n",
    "     'gtc' = good until cancelled. \\\n",
    "     'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the trading of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "    \"\"\" \n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "    legs = []\n",
    "    for each in spread:\n",
    "        optionID = id_for_option(symbol,\n",
    "                                        each['expirationDate'],\n",
    "                                        each['strike'],\n",
    "                                        each['optionType'])\n",
    "        legs.append({'position_effect': each['effect'],\n",
    "                     'side': each['action'],\n",
    "                     'ratio_quantity': 1,\n",
    "                     'option': option_instruments_url(optionID)})\n",
    "\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'direction': direction,\n",
    "        'time_in_force': timeInForce,\n",
    "        'legs': legs,\n",
    "        'type': 'limit',\n",
    "        'trigger': 'immediate',\n",
    "        'price': price,\n",
    "        'quantity': quantity,\n",
    "        'override_day_trade_checks': False,\n",
    "        'override_dtbp_checks': False,\n",
    "        'ref_id': str(uuid4()),\n",
    "    }\n",
    "\n",
    "    url = option_orders_url()\n",
    "    data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_option_limit(positionEffect, creditOrDebit, price, symbol, quantity, expirationDate, strike, optionType='both', account_number=None, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for an option. i.e. place a long call or a long put.\n",
    "\n",
    "    :param positionEffect: Either 'open' for a buy to open effect or 'close' for a buy to close effect.\n",
    "    :type positionEffect: str\n",
    "    :param creditOrDebit: Either 'debit' or 'credit'.\n",
    "    :type creditOrDebit: str\n",
    "    :param price: The limit price to trigger a buy of the option.\n",
    "    :type price: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to buy.\n",
    "    :type quantity: int\n",
    "    :param expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\n",
    "    :type expirationDate: str\n",
    "    :param strike: The strike price of the option.\n",
    "    :type strike: float\n",
    "    :param optionType: This should be 'call' or 'put'\n",
    "    :type optionType: str\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "\n",
    "    optionID = id_for_option(symbol, expirationDate, strike, optionType)\n",
    "\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'direction': creditOrDebit,\n",
    "        'time_in_force': timeInForce,\n",
    "        'legs': [\n",
    "            {'position_effect': positionEffect, 'side': 'buy',\n",
    "                'ratio_quantity': 1, 'option': option_instruments_url(optionID)},\n",
    "        ],\n",
    "        'type': 'limit',\n",
    "        'trigger': 'immediate',\n",
    "        'price': price,\n",
    "        'quantity': quantity,\n",
    "        'override_day_trade_checks': False,\n",
    "        'override_dtbp_checks': False,\n",
    "        'ref_id': str(uuid4()),\n",
    "    }\n",
    "\n",
    "    url = option_orders_url()\n",
    "    data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_option_stop_limit(positionEffect, creditOrDebit, limitPrice, stopPrice, symbol, quantity, expirationDate, strike, optionType='both', account_number=None, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a stop order to be turned into a limit order once a certain stop price is reached.\n",
    "\n",
    "    :param positionEffect: Either 'open' for a buy to open effect or 'close' for a buy to close effect.\n",
    "    :type positionEffect: str\n",
    "    :param creditOrDebit: Either 'debit' or 'credit'.\n",
    "    :type creditOrDebit: str\n",
    "    :param limitPrice: The limit price to trigger a buy of the option.\n",
    "    :type limitPrice: float\n",
    "    :param stopPrice: The price to trigger the limit order.\n",
    "    :type stopPrice: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to buy.\n",
    "    :type quantity: int\n",
    "    :param expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\n",
    "    :type expirationDate: str\n",
    "    :param strike: The strike price of the option.\n",
    "    :type strike: float\n",
    "    :param optionType: This should be 'call' or 'put'\n",
    "    :type optionType: str\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "\n",
    "    optionID = id_for_option(symbol, expirationDate, strike, optionType)\n",
    "\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'direction': creditOrDebit,\n",
    "        'time_in_force': timeInForce,\n",
    "        'legs': [\n",
    "            {'position_effect': positionEffect, 'side': 'buy',\n",
    "                'ratio_quantity': 1, 'option': option_instruments_url(optionID)},\n",
    "        ],\n",
    "        'type': 'limit',\n",
    "        'trigger': 'stop',\n",
    "        'price': limitPrice,\n",
    "        'stop_price': stopPrice,\n",
    "        'quantity': quantity,\n",
    "        'override_day_trade_checks': False,\n",
    "        'override_dtbp_checks': False,\n",
    "        'ref_id': str(uuid4()),\n",
    "    }\n",
    "\n",
    "    url = option_orders_url()\n",
    "    data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "def order_sell_option_stop_limit(positionEffect, creditOrDebit, limitPrice, stopPrice, symbol, quantity, expirationDate, strike, optionType='both', account_number=None, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a stop order to be turned into a limit order once a certain stop price is reached.\n",
    "\n",
    "    :param positionEffect: Either 'open' for a buy to open effect or 'close' for a buy to close effect.\n",
    "    :type positionEffect: str\n",
    "    :param creditOrDebit: Either 'debit' or 'credit'.\n",
    "    :type creditOrDebit: str\n",
    "    :param limitPrice: The limit price to trigger a buy of the option.\n",
    "    :type limitPrice: float\n",
    "    :param stopPrice: The price to trigger the limit order.\n",
    "    :type stopPrice: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to buy.\n",
    "    :type quantity: int\n",
    "    :param expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\n",
    "    :type expirationDate: str\n",
    "    :param strike: The strike price of the option.\n",
    "    :type strike: float\n",
    "    :param optionType: This should be 'call' or 'put'\n",
    "    :type optionType: str\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "\n",
    "    optionID = id_for_option(symbol, expirationDate, strike, optionType)\n",
    "\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'direction': creditOrDebit,\n",
    "        'time_in_force': timeInForce,\n",
    "        'legs': [\n",
    "            {'position_effect': positionEffect, 'side': 'sell',\n",
    "                'ratio_quantity': 1, 'option': option_instruments_url(optionID)},\n",
    "        ],\n",
    "        'type': 'limit',\n",
    "        'trigger': 'stop',\n",
    "        'price': limitPrice,\n",
    "        'stop_price': stopPrice,\n",
    "        'quantity': quantity,\n",
    "        'override_day_trade_checks': False,\n",
    "        'override_dtbp_checks': False,\n",
    "        'ref_id': str(uuid4()),\n",
    "    }\n",
    "\n",
    "    url = option_orders_url()\n",
    "    data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_option_limit(positionEffect, creditOrDebit, price, symbol, quantity, expirationDate, strike, optionType='both', account_number=None, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for an option. i.e. place a short call or a short put.\n",
    "\n",
    "    :param positionEffect: Either 'open' for a sell to open effect or 'close' for a sell to close effect.\n",
    "    :type positionEffect: str\n",
    "    :param creditOrDebit: Either 'debit' or 'credit'.\n",
    "    :type creditOrDebit: str\n",
    "    :param price: The limit price to trigger a sell of the option.\n",
    "    :type price: float\n",
    "    :param symbol: The stock ticker of the stock to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The number of options to sell.\n",
    "    :type quantity: int\n",
    "    :param expirationDate: The expiration date of the option in 'YYYY-MM-DD' format.\n",
    "    :type expirationDate: str\n",
    "    :param strike: The strike price of the option.\n",
    "    :type strike: float\n",
    "    :param optionType: This should be 'call' or 'put'\n",
    "    :type optionType: str\n",
    "    :param account_number: the robinhood account number.\n",
    "    :type account_number: Optional[str]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled. \\\n",
    "    'gfd' = good for the day. 'ioc' = immediate or cancel. 'opg' execute at opening.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of options, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "\n",
    "    optionID = id_for_option(symbol, expirationDate, strike, optionType)\n",
    "\n",
    "    payload = {\n",
    "        'account': load_account_profile(account_number=account_number, info='url'),\n",
    "        'direction': creditOrDebit,\n",
    "        'time_in_force': timeInForce,\n",
    "        'legs': [\n",
    "            {'position_effect': positionEffect, 'side': 'sell',\n",
    "                'ratio_quantity': 1, 'option': option_instruments_url(optionID)},\n",
    "        ],\n",
    "        'type': 'limit',\n",
    "        'trigger': 'immediate',\n",
    "        'price': price,\n",
    "        'quantity': quantity,\n",
    "        'override_day_trade_checks': False,\n",
    "        'override_dtbp_checks': False,\n",
    "        'ref_id': str(uuid4()),\n",
    "    }\n",
    "\n",
    "    url = option_orders_url()\n",
    "    data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "\n",
    "    return(data)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_crypto_by_price(symbol, amountInDollars, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a market order for a crypto by specifying the amount in dollars that you want to trade.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param amountInDollars: The amount in dollars of the crypto you want to buy.\n",
    "    :type amountInDollars: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order_crypto(symbol, \"buy\", amountInDollars, \"price\", None, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_crypto_by_quantity(symbol, quantity, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a market order for a crypto by specifying the decimal amount of shares to buy.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The decimal amount of shares to buy.\n",
    "    :type quantity: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order_crypto(symbol, \"buy\", quantity, \"quantity\", None, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_crypto_limit(symbol, quantity, limitPrice, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for a crypto by specifying the decimal amount of shares to buy.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The decimal amount of shares to buy.\n",
    "    :type quantity: float\n",
    "    :param limitPrice: The limit price to set for the crypto.\n",
    "    :type limitPrice: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order_crypto(symbol, \"buy\", quantity, \"quantity\", limitPrice, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_buy_crypto_limit_by_price(symbol, amountInDollars, limitPrice, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for a crypto by specifying the decimal price to buy.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param amountInDollars: The amount in dollars of the crypto you want to buy.\n",
    "    :type amountInDollars: float\n",
    "    :param limitPrice: The limit price to set for the crypto.\n",
    "    :type limitPrice: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\"\n",
    "    return order_crypto(symbol, \"buy\", amountInDollars, \"price\", limitPrice, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_crypto_by_price(symbol, amountInDollars, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a market order for a crypto by specifying the amount in dollars that you want to trade.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param amountInDollars: The amount in dollars of the crypto you want to sell.\n",
    "    :type amountInDollars: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order_crypto(symbol, \"sell\", amountInDollars, \"price\", None, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_crypto_by_quantity(symbol, quantity, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a market order for a crypto by specifying the decimal amount of shares to buy.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The decimal amount of shares to sell.\n",
    "    :type quantity: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\" \n",
    "    return order_crypto(symbol, \"sell\", quantity, \"quantity\", None, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_crypto_limit(symbol, quantity, limitPrice, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for a crypto by specifying the decimal amount of shares to sell.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param quantity: The decimal amount of shares to sell.\n",
    "    :type quantity: float\n",
    "    :param limitPrice: The limit price to set for the crypto.\n",
    "    :type limitPrice: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\"\n",
    "    return order_crypto(symbol, \"sell\", quantity, \"quantity\", limitPrice, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_sell_crypto_limit_by_price(symbol, amountInDollars, limitPrice, timeInForce='gtc', jsonify=True):\n",
    "    \"\"\"Submits a limit order for a crypto by specifying the decimal price to sell.\n",
    "    Good for share fractions up to 8 decimal places.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param amountInDollars: The amount in dollars of the crypto you want to sell.\n",
    "    :type amountInDollars: float\n",
    "    :param limitPrice: The limit price to set for the crypto.\n",
    "    :type limitPrice: float\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the buying of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\"\n",
    "    return order_crypto(symbol, \"sell\", amountInDollars, \"price\", limitPrice, timeInForce, jsonify)\n",
    "\n",
    "\n",
    "@login_required\n",
    "def order_crypto(symbol, side, quantityOrPrice, amountIn=\"quantity\", limitPrice=None, timeInForce=\"gtc\", jsonify=True):\n",
    "    \"\"\"Submits an order for a crypto.\n",
    "\n",
    "    :param symbol: The crypto ticker of the crypto to trade.\n",
    "    :type symbol: str\n",
    "    :param side: Either 'buy' or 'sell'\n",
    "    :type side: str\n",
    "    :param quantityOrPrice: Either the decimal price of shares to trade or the decimal quantity of shares.\n",
    "    :type quantityOrPrice: float\n",
    "    :param amountIn: If left default value of 'quantity', order will attempt to trade cryptos by the amount of crypto \\\n",
    "        you want to trade. If changed to 'price', order will attempt to trade cryptos by the price you want to buy or sell.\n",
    "    :type amountIn: Optional[str]\n",
    "    :param limitPrice: The price to trigger the market order.\n",
    "    :type limitPrice: Optional[float]\n",
    "    :param timeInForce: Changes how long the order will be in effect for. 'gtc' = good until cancelled.\n",
    "    :type timeInForce: Optional[str]\n",
    "    :param jsonify: If set to False, function will return the request object which contains status code and headers.\n",
    "    :type jsonify: Optional[str]\n",
    "    :returns: Dictionary that contains information regarding the selling of crypto, \\\n",
    "    such as the order id, the state of order (queued, confired, filled, failed, canceled, etc.), \\\n",
    "    the price, and the quantity.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        symbol = symbol.upper().strip()\n",
    "    except AttributeError as message:\n",
    "        print(message, file=get_output())\n",
    "        return None\n",
    "\n",
    "    crypto_id = get_crypto_id(symbol)\n",
    "    orderType = \"market\"\n",
    "\n",
    "    if side == \"buy\":\n",
    "        priceType = \"ask_price\"\n",
    "    else:\n",
    "        priceType = \"bid_price\"\n",
    "\n",
    "    if limitPrice:\n",
    "        price = limitPrice\n",
    "        orderType = \"limit\"\n",
    "    else:\n",
    "        price = round_price(get_crypto_quote_from_id(crypto_id, info=priceType))\n",
    "\n",
    "    if amountIn == \"quantity\":\n",
    "        quantity = quantityOrPrice\n",
    "    else:\n",
    "        quantity = round_price(quantityOrPrice/price)\n",
    "\n",
    "    payload = {\n",
    "        'account_id': load_crypto_profile(info=\"id\"),\n",
    "        'currency_pair_id': crypto_id,\n",
    "        'price': price,\n",
    "        'quantity': quantity,\n",
    "        'ref_id': str(uuid4()),\n",
    "        'side': side,\n",
    "        'time_in_force': timeInForce,\n",
    "        'type': orderType\n",
    "    }\n",
    "\n",
    "    url = order_crypto_url()\n",
    "\n",
    "    # This is safe because 'ref_id' guards us from duplicate orders\n",
    "    attempts = 3\n",
    "    while attempts > 0:\n",
    "        data = request_post(url, payload, json=True, jsonify_data=jsonify)\n",
    "        if data is not None:\n",
    "            break\n",
    "\n",
    "        attempts -= 1\n",
    "\n",
    "    return(data)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('> Robin-Stocks imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be362aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "Analysis Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bd871d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Supervised learning\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, Lasso\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lazypredict.Supervised import LazyClassifier  # import regression if needed \n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler, PolynomialFeatures\n",
    "from sklearn.metrics import make_scorer, accuracy_score, fbeta_score, recall_score, f1_score, fbeta_score, mean_squared_error, r2_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.layers import Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# check missing tickers to sell\n",
    "import sean\n",
    "import robin_stocks.robinhood as rs\n",
    "\n",
    "import pandas_ta as ta\n",
    "from pandas_datareader import data as pdr\n",
    "import FinanceDataReader as fdr\n",
    "from forex_python.converter import CurrencyRates\n",
    "import yfinance as yf\n",
    "yf.pdr_override()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")  # ignore runtime warning\n",
    "\n",
    "# NLTK VADER for sentiment analysis\n",
    "import nltk\n",
    "# nltk.downloader.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from pyfinviz.news import News \n",
    "# https://pypi.org/project/pyfinviz/\n",
    "\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "# plt.style.use('classic')\n",
    "\n",
    "# article_ml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# email with attachments\n",
    "import email, smtplib, ssl\n",
    "port = 465  # For SSL\n",
    "smtp_server = \"smtp.gmail.com\"\n",
    "sender_email = \"aicpasean@gmail.com\"  # Enter your address\n",
    "receiver_email = \"aicpasean@gmail.com\"  # Enter receiver address\n",
    "password = sean.gmail_api()\n",
    "\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import pytz\n",
    "# rsi\n",
    "import datetime as dt\n",
    "\n",
    "# other required libraries\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import io\n",
    "import os\n",
    "import webbrowser\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from urllib.request import urlopen, Request\n",
    "from os.path import exists\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "from time import sleep\n",
    "from gtts import gTTS \n",
    "import jupyter_beeper\n",
    "from datetime import date, datetime, timedelta\n",
    "currentdate_verify = date.today()\n",
    "currentdate_verify = int(currentdate_verify.strftime(\"%Y%m%d\"))\n",
    "\n",
    "import pandas_datareader as data\n",
    "data_source = 'yahoo'\n",
    "start_date = '1970-01-01'\n",
    "\n",
    "day_plus = datetime.today() + timedelta(days = 1)\n",
    "day_plus = str(day_plus)[:10]\n",
    "\n",
    "index_ticker = ['XLE', 'XLF', 'XLRE', 'XLB', 'XLK', 'XLP', 'XLV', 'XLI', 'XLY', 'XLU', 'XLC',\n",
    "                '^GSPC', 'BTC-USD', 'GLD', 'USO', '^TNX', '^VIX', 'GC=F', 'CL=F', 'DX-Y.NYB']\n",
    "\n",
    "index_candle_tickers = ['^GSPC']\n",
    "\n",
    "etf_index_dic = pd.read_csv('etf_index_df.csv')\n",
    "etf_index_dic = pd.melt(etf_index_dic, var_name = 'sector', value_name = 'ticker')\n",
    "etf_index_dic = etf_index_dic.sort_values('ticker').reset_index(drop = True)\n",
    "etf_ticker = etf_index_dic.ticker.tolist()\n",
    "\n",
    "review_plot_lookbackdays = 121\n",
    "\n",
    "# reset the counter checking the number of request to yfinance\n",
    "yf_counter = 0\n",
    "\n",
    "def yfinance_df_setting(ticker):\n",
    "    global yf_counter\n",
    "    \n",
    "    try:\n",
    "        # start date is since 1970, which is default value\n",
    "        start_date = '1970-01-01'\n",
    "        df = yf.download(ticker, start = start_date, end = day_plus, prepost = True, progress=False).reset_index()\n",
    "        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "        df['Date'] = df.Date.astype('datetime64[ns]')\n",
    "        return df\n",
    "    except:\n",
    "        yf_counter += 1\n",
    "        if yf_counter <= 50:\n",
    "            return yfinance_df_setting(ticker)\n",
    "        else:\n",
    "            yf_counter = 0\n",
    "            message = f'Ticker [{ticker}] may be delisted. Return an empty DataFrame.'\n",
    "            print('Except >>> ' + message)\n",
    "            alarm()\n",
    "            voice_message(message)\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        \n",
    "def yfinance_df_rsi(ticker):\n",
    "    global yf_counter\n",
    "    \n",
    "    try:\n",
    "        start_date = str(datetime.today() - timedelta(days = 300))[:10]   \n",
    "        df = yf.download(ticker, start = start_date, end = day_plus, prepost = True, progress=False).reset_index()\n",
    "        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "        df['Date'] = df.Date.astype('datetime64[ns]')        \n",
    "        return df\n",
    "    except:\n",
    "        yf_counter += 1\n",
    "        if yf_counter <= 50:\n",
    "            return yfinance_df_rsi(ticker)\n",
    "        else:\n",
    "            yf_counter = 0\n",
    "            message = f'Ticker [{ticker}] may be delisted. Return an empty DataFrame.'\n",
    "            print('Except >>> ' + message)\n",
    "            alarm()\n",
    "            voice_message(message)\n",
    "            return pd.DataFrame()\n",
    "  \n",
    "\n",
    "def yfinance_df(ticker):\n",
    "    global yf_counter\n",
    "  \n",
    "    # start date is since 1970, which is default value\n",
    "    start_date = '1970-01-01'\n",
    "    non_timeout_tickers = ['XLE', 'XLF', 'XLRE', 'XLB', 'XLK', 'XLP', 'XLV', 'XLI', 'XLY', 'XLU', 'XLC',\n",
    "                       '^GSPC', 'BTC-USD', 'GLD', 'USO', '^TNX', '^VIX', 'GC=F', 'CL=F', 'DX-Y.NYB']\n",
    "    if ticker in non_timeout_tickers:\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            df = yf.download(ticker, start = start_date, end = day_plus, prepost = True, progress=False).reset_index()\n",
    "            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "            df['Date'] = df.Date.astype('datetime64[ns]') \n",
    "            return df\n",
    "        except:   \n",
    "            yf_counter += 1\n",
    "            if yf_counter <= 50:\n",
    "                return yfinance_df(ticker)\n",
    "            else:\n",
    "                yf_counter = 0\n",
    "                message = f'Ticker [{ticker}] may be delisted. Return an empty DataFrame.'\n",
    "                print('Except >>> ' + message)\n",
    "                alarm()\n",
    "                voice_message(message)\n",
    "                return pd.DataFrame()\n",
    "          \n",
    "    else:\n",
    "        try:\n",
    "            df = yf.download(ticker, start = start_date, end = day_plus, prepost = True, progress=False).reset_index()\n",
    "            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "            df['Date'] = df.Date.astype('datetime64[ns]') \n",
    "            return df\n",
    "        except:       \n",
    "            yf_counter += 1\n",
    "            if yf_counter <= 50:\n",
    "                return yfinance_df(ticker)\n",
    "            else:\n",
    "                yf_counter = 0\n",
    "                message = f'Ticker [{ticker}] may be delisted. Return an empty DataFrame.'\n",
    "                print('Except >>> ' + message)\n",
    "                alarm()\n",
    "                voice_message(message)\n",
    "                return pd.DataFrame()\n",
    "   \n",
    "\n",
    "def yfinance_df_min(ticker): # only for valuation\n",
    "    global yf_counter\n",
    "    \n",
    "    start_date = str(datetime.today() - timedelta(days = 10))[:10]\n",
    "    non_timeout_tickers = ['XLE', 'XLF', 'XLRE', 'XLB', 'XLK', 'XLP', 'XLV', 'XLI', 'XLY', 'XLU', 'XLC',\n",
    "                       '^GSPC', 'BTC-USD', 'GLD', 'USO', '^TNX', '^VIX', 'GC=F', 'CL=F', 'DX-Y.NYB']\n",
    "        \n",
    "    if ticker in non_timeout_tickers:\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            df = yf.download(ticker, start = start_date, end = day_plus, prepost = True, progress=False).reset_index().tail(1)\n",
    "            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "            df['Date'] = df.Date.astype('datetime64[ns]') \n",
    "            return df\n",
    "        except:    \n",
    "            yf_counter += 1\n",
    "            if yf_counter <= 50:\n",
    "                return yfinance_df_min(ticker)\n",
    "            else:\n",
    "                yf_counter = 0\n",
    "                message = f'Ticker [{ticker}] may be delisted. Return an empty DataFrame.'\n",
    "                print('Except >>> ' + message)\n",
    "                alarm()\n",
    "                voice_message(message)\n",
    "                return pd.DataFrame()\n",
    "          \n",
    "    else:\n",
    "        try:\n",
    "            df = yf.download(ticker, start = start_date, end = day_plus, prepost = True, progress=False).reset_index().tail(1)\n",
    "            df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "            df['Date'] = df.Date.astype('datetime64[ns]') \n",
    "            return df\n",
    "        except: \n",
    "            yf_counter += 1\n",
    "            if yf_counter <= 50:\n",
    "                return yfinance_df_min(ticker)\n",
    "            else:\n",
    "                yf_counter = 0\n",
    "                message = f'Ticker [{ticker}] may be delisted. Return an empty DataFrame.'\n",
    "                print('Except >>> ' + message)\n",
    "                alarm()\n",
    "                voice_message(message)\n",
    "                return pd.DataFrame()\n",
    "        \n",
    "############################################################################################################################################\n",
    "def current_time():\n",
    "    time = f'Time Check | {datetime.now().strftime(\"%H:%M\")}'\n",
    "    return time    \n",
    "    \n",
    "\n",
    "def timechecknow():\n",
    "    # time\n",
    "    ttoday = date.today()\n",
    "    tz_NY = pytz.timezone('America/New_York') \n",
    "    datetime_NY = datetime.now(tz_NY) \n",
    "    speed_hour = int(datetime_NY.strftime(\"%H\"))\n",
    "    speed_minute = int(datetime_NY.strftime(\"%M\"))\n",
    "    speed_second = int(datetime_NY.strftime(\"%S\"))\n",
    "    minutepassed = np.round((speed_hour-9)*60 + (speed_minute-30) + speed_second/60,30)\n",
    "    return minutepassed\n",
    "\n",
    "\n",
    "def day_check():\n",
    "    checkday = datetime.today().strftime('%A')\n",
    "    return checkday\n",
    "\n",
    "\n",
    "def exchange_check():\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    try:\n",
    "        # exchange rate\n",
    "        krw = requests.get('https://www.xe.com/currencyconverter/convert/?Amount=1&From=USD&To=KRW')\n",
    "        soup = BeautifulSoup(krw.content, 'html.parser')\n",
    "        ext = soup.body.get_text()   #get String\n",
    "        ext = pd.Series(ext)         #change to Series because 'str.extract' does not work with String\n",
    "        krwdollar = ext.str.extract(r'((US Dollar =)\\d{1}[,]?\\d{3}[.]?\\d{2})')[0].values[0]  #'US Dollar' after parsing\n",
    "        krw = float(krwdollar[-8:].replace(',',''))\n",
    "        return krw\n",
    "    except:\n",
    "        print(\"\\nExcept >>> exchange_check() Error\\n\")\n",
    "        alarm()\n",
    "        pass\n",
    "\n",
    "    \n",
    "def alarm():\n",
    "    b = jupyter_beeper.Beeper()\n",
    "    b.beep(frequency=150, secs=0.5, blocking=True)\n",
    "    b.beep(frequency=200, secs=0.7, blocking=True)\n",
    "    \n",
    "    \n",
    "def alarm_open(times):\n",
    "    b = jupyter_beeper.Beeper()\n",
    "    for c in range(1,times+1):\n",
    "        time.sleep(0.3)\n",
    "        for i, j in zip([150,200,250], [0.5, 0.5, 0.7]):\n",
    "            b.beep(frequency=i, secs=j, blocking=True)\n",
    "    \n",
    "    \n",
    "def emailsend_to_server(message):\n",
    "    context = ssl.create_default_context()\n",
    "    with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "        server.login(sender_email, password)\n",
    "        server.sendmail(sender_email, receiver_email, message)\n",
    "\n",
    "    # screen message\n",
    "    print('\\n')\n",
    "    print(f\">>> Email sent: {message}\")\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    # text to voice \n",
    "    voice_message(message)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    \n",
    "def voice_message(message):\n",
    "    text = message\n",
    "    speech = gTTS(text = message, lang = 'en', slow = False)\n",
    "    speech.save(\"text.mp3\")   \n",
    "    webbrowser.open(\"text.mp3\")        \n",
    "\n",
    "    \n",
    "def voice_text():\n",
    "    r = sr.Recognizer()\n",
    "    \n",
    "    with sr.Microphone() as source:\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        \n",
    "        print('Please say...')        \n",
    "        audio = r.listen(source)       \n",
    "        print('Recognizing...')\n",
    "#         try:\n",
    "#             print('Audio Recognized: \\n ' + r.recognize_google(audio))\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print('Error: ' + str(e))\n",
    "        \n",
    "        with open('recoded_audio.wav', 'wb') as f:\n",
    "            f.write(audio.get_wav_data())\n",
    "            \n",
    "        return r.recognize_google(audio)\n",
    "\n",
    "    \n",
    "def voice_text_num():\n",
    "    voice_message('Please tell me the threshold')\n",
    "    voice_text_converted = voice_text()\n",
    "    voice_message(voice_text_converted)\n",
    "    return int(voice_text_converted)\n",
    "\n",
    "# threshold_value = voice_text_num()\n",
    "# threshold_value\n",
    "    \n",
    "    \n",
    "def exrate():\n",
    "    print('\\n\\n' + '\\033[1m' + format(int(sum(krwex))) + '\\033[0m' + ' Won Monitoring', '\\n')\n",
    "\n",
    "    krw = exchange_check()\n",
    "    print(np.round(krw,2), '\\n')\n",
    "    \n",
    "    import pytz\n",
    "    ttoday = date.today()\n",
    "    tz_NY = pytz.timezone('America/New_York') \n",
    "    datetime_NY = datetime.now(tz_NY) \n",
    "    print(datetime_NY.strftime(\"%H:%M:%S\"), ttoday, '\\n\\n\\n') #to put the number, date, hour\n",
    "    #####################\n",
    "    if krw < sum(krwex):\n",
    "    #####################  \n",
    "        krwex.append(-10)    \n",
    "\n",
    "        # alarm\n",
    "#         webbrowser.open(\"Nuclear Missile Ready.mp3\")\n",
    "        \n",
    "        # email\n",
    "        message = \"\"\"\\\n",
    "Subject: [{}] {}\"\"\".format('Korean Won', '%.f' %krw)\n",
    "        emailsend_to_server(message) \n",
    "        \n",
    "    else:\n",
    "        pass  \n",
    "        \n",
    "    \n",
    "def slistemail(portfolio):\n",
    "    print('\\n'+current_time()+'\\n')\n",
    "    \n",
    "    date = currentdate \n",
    "    toplist = portfolio   \n",
    "    rank = pd.DataFrame()\n",
    "\n",
    "    cal1 = []\n",
    "    cal2 = []\n",
    "    for i in range(len(toplist)):\n",
    "        cal1.append(i+1)\n",
    "        cal2.append(toplist[i])\n",
    "        \n",
    "    rank[date] = cal1\n",
    "    rank[''] = cal2\n",
    "    \n",
    "    toplist = rank.set_index(date)\n",
    "   \n",
    "    df = pd.read_csv('sean_emailgroup.csv')\n",
    "    sender_email = \"aicpasean@gmail.com\"\n",
    "    password = sean.gmail_api()\n",
    "\n",
    "    sentlist = []\n",
    "\n",
    "    for i in range(len(df.name)):\n",
    "\n",
    "        try:\n",
    "            c=i+1\n",
    "            print(df.name[i],\": email sent : #%d\" %c)\n",
    "\n",
    "            subject = \"[{}] Blue-chip\".format(date)\n",
    "            body = \"\"\"\\\n",
    "Hi {}, \n",
    "\n",
    "Here are top stocks.\n",
    "{}\n",
    "\n",
    "\n",
    "Refer to attachment for more details. \n",
    "\n",
    "Yours Sincerely, \n",
    "Sean Yeon, CPA, MBA\"\"\".format(df.name[i], toplist)   \n",
    "\n",
    "            # Create a multipart message and set headers\n",
    "            message = MIMEMultipart()\n",
    "            message[\"From\"] = sender_email\n",
    "            message[\"To\"] = df.name[i]\n",
    "            message[\"Subject\"] = subject\n",
    "            message[\"Bcc\"] = df.name[i]  # Recommended for mass emails\n",
    "\n",
    "            # Add body to email\n",
    "            message.attach(MIMEText(body, \"plain\"))\n",
    "\n",
    "            filename = \"srank_portfolio.csv\"  # In same directory as script\n",
    "\n",
    "            # Open PDF file in binary mode\n",
    "            with open(filename, \"rb\") as attachment:\n",
    "                # Add file as application/octet-stream\n",
    "                # Email client can usually download this automatically as attachment\n",
    "                part = MIMEBase(\"application\", \"octet-stream\")\n",
    "                part.set_payload(attachment.read())\n",
    "\n",
    "            # Encode file in ASCII characters to send by email    \n",
    "            encoders.encode_base64(part)\n",
    "\n",
    "            # Add header as key/value pair to attachment part\n",
    "            part.add_header(\n",
    "                \"Content-Disposition\",\n",
    "                f\"attachment; filename= {filename}\",\n",
    "            )\n",
    "\n",
    "            # Add attachment to message and convert message to string\n",
    "            message.attach(part)\n",
    "            text = message.as_string()\n",
    "\n",
    "            # Log in to server using secure context and send email\n",
    "            context = ssl.create_default_context()\n",
    "            with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n",
    "                server.login(sender_email, password)\n",
    "                server.sendmail(sender_email, df.email[i], text)\n",
    "\n",
    "    #         # record time sent\n",
    "    #         today = date.today()\n",
    "    #         tz_NY = pytz.timezone('America/New_York') \n",
    "    #         datetime_NY = datetime.now(tz_NY)\n",
    "\n",
    "    #         # append records\n",
    "    #         sentlist.append([df.name[i], df.email[i], datetime_NY.strftime(\"%H:%M:%S\")])\n",
    "\n",
    "        except:\n",
    "            ('\\n%s email error' %df.name[i])\n",
    "\n",
    "    # sentlist = pd.DataFrame(sentlist, columns = [['sent', 'email', 'time']])\n",
    "\n",
    "    # record date sent\n",
    "    # now = today.strftime(\"%Y,%m,%d\")\n",
    "    # y = now[0:4]\n",
    "    # m = now[5:7]\n",
    "    # d = now[8:10]\n",
    "    # date = y+m+d    \n",
    "    # sentlist['date'] = date\n",
    "\n",
    "    # save file as csv\n",
    "\n",
    "    # sentlist.to_csv('email_sent.csv', index = False)\n",
    "    # # sentlist.to_csv('email_sent %s.csv' %date, index = False)\n",
    "    # sentlist = pd.read_csv('email_sent %s.csv'%date)\n",
    "     \n",
    "        \n",
    "def tarot():\n",
    "    f = open('tarot.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    df = pd.DataFrame(data['cards'])\n",
    "    ramdom_pick = random.choice(df.loc[random.choice(np.arange(len(df['fortune_telling'])))]['meanings'][random.choice(['light', 'shadow'])])\n",
    "    ramdom_pick = random.choice(df.loc[random.choice(np.arange(len(df['fortune_telling'])))]['meanings'][random.choice(['light', 'shadow'])])\n",
    "    return ramdom_pick\n",
    "\n",
    "\n",
    "def robin_login():   \n",
    "    login = rs.login('aicpasean@gmail.com', sean.robin_api())\n",
    "    my_stocks = rs.build_holdings()\n",
    "    rs.logout()\n",
    "    robin = pd.DataFrame(my_stocks)\n",
    "    robin = robin.T.reset_index()\n",
    "    return robin\n",
    "\n",
    "\n",
    "def perf_cal(robin):\n",
    "    stock_value = sum(robin.price.astype(float) * robin.quantity.astype(float))\n",
    "    stock_cost = sum(robin.average_buy_price.astype(float) * robin.quantity.astype(float))\n",
    "    stock_return = (stock_value-stock_cost)/stock_cost\n",
    "    return int(stock_return*100)\n",
    "\n",
    "\n",
    "def real_performance_selma():\n",
    "    \n",
    "    sss_perf = pd.read_csv('sss_perf.csv')\n",
    "\n",
    "    # get realized gain/loss and holding period\n",
    "    realized_equity_change_sum = int(sss_perf.equity_change.sum())\n",
    "    realized_percent_change_avg = np.round(sss_perf.percent_change.mean(), 2)\n",
    "    realized_holding_period = int(sss_perf.holding_period.mean())\n",
    "\n",
    "    # get unrealized gain/loss and holding period\n",
    "    df = robin_login()\n",
    "    df = df.loc[-df['index'].isin(dead_stock)]\n",
    "    unrealized_equity_change_sum = int(np.array([float(i) for i in df.equity_change.tolist()]).sum())\n",
    "    unrealized_percent_change_avg = np.round(np.array([float(i) for i in df.percent_change.tolist()]).mean(), 2)\n",
    "    df = pd.read_csv('holding_period.csv')\n",
    "    df = df.loc[-df['index'].isin(dead_stock)]\n",
    "    if math.isnan(df.days.mean()) == False:\n",
    "        unrealized_holding_period = int(df.days.mean())\n",
    "    else:\n",
    "        unrealized_holding_period = 0\n",
    "\n",
    "    # combine\n",
    "    agg_equity_change_sum = realized_equity_change_sum + unrealized_equity_change_sum\n",
    "    agg_percent_change_avg = np.round((realized_percent_change_avg + unrealized_percent_change_avg)*0.5, 2)\n",
    "    agg_holding_period = int((realized_holding_period + unrealized_holding_period)/2)\n",
    "\n",
    "    # group by date\n",
    "    df = sss_perf.groupby('date').mean().reset_index()\n",
    "    df['equity_change'] = sss_perf.groupby('date').sum().reset_index()['equity_change']\n",
    "\n",
    "    # plot   \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "    df.date = df.date.astype(str)\n",
    "    ax.plot(df.date, df['percent_change'], color = 'blue')\n",
    "    ax.set_xlabel('\\nDate')\n",
    "    ax.set_ylabel('\\nRealized Gain/Loss (%)', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "    ax.grid(axis = 'x')\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.bar(df.date, df['equity_change'], color = 'green', alpha=0.3)\n",
    "    ax1.set_ylabel('Realized Value ($)', color = 'green')\n",
    "    ax1.grid(axis = 'y')\n",
    "    ax.set_xticklabels(df.date, rotation = 90)\n",
    "    ax.plot(df.date, [agg_percent_change_avg]*df.shape[0], color = 'blue', linestyle = '-', alpha=0.3, linewidth = 10, label ='Avg Gain (Unrealized + Realized)')\n",
    "    ax.legend()\n",
    "\n",
    "    # ###############################################################\n",
    "    # # annotate - version update details \n",
    "    # annotate_dict = {}\n",
    "    # annotate_dict['2023-01-06'] = 'Ver.3.7 - New ML Model' \n",
    "    # annotate_dict['2023-03-01'] = 'Loss Cut by DMA' \n",
    "    # annotate_dict['2023-04-03'] = 'Ver.3.8 - Strategy def'  \n",
    "    # annotate_dict['2023-04-10'] = 'pandas_ta - RSI, ADX' \n",
    "    # ###############################################################\n",
    "\n",
    "    # for i, v in annotate_dict.items():\n",
    "    #     i = df[df.date <= i].tail(1).date.values[0]\n",
    "    #     plt.annotate(xy = [i, df[df.date == i]['percent_change'].values], text = v)\n",
    "\n",
    "    plt.title(f'{current_time()} - Realtime Performance')\n",
    "    plt.show()\n",
    "\n",
    "    print('\\n\\n\\n', '\\033[1m' + \"_____________________________________________________\" + '\\033[0m')\n",
    "    print('\\n', f\" • Realized Equity Change (aggregrated):    \" + '\\033[1m' + f\"{realized_equity_change_sum} USD\" + '\\033[0m')\n",
    "    print('\\n', f\" • Realized Percentage Change (avg):        \" + '\\033[1m' + f\"{realized_percent_change_avg} %\" + '\\033[0m')\n",
    "    print('\\n', f\" • Realized Holding Period (avg):           \" + '\\033[1m' + f\"{realized_holding_period} days\" + '\\033[0m')\n",
    "    print('\\n', '\\033[1m' + \"-----------------[Excl. dead stocks]-----------------\" + '\\033[0m')\n",
    "    print('\\n', f\" • Unrealized Equity Change (aggregrated):  \" + '\\033[1m' + f\"{unrealized_equity_change_sum} USD\" + '\\033[0m')\n",
    "    print('\\n', f\" • Unrealized Percentage Change (avg):      \" + '\\033[1m' + f\"{unrealized_percent_change_avg} %\" + '\\033[0m')\n",
    "    print('\\n', f\" • Unrealized Holding Period (avg):         \" + '\\033[1m' + f\"{unrealized_holding_period} days\" + '\\033[0m')\n",
    "    print('\\n', '\\033[1m' + \"-----------------[Excl. dead stocks]-----------------\" + '\\033[0m')\n",
    "    print('\\n', f\" • Aggregrated Equity Change (aggregrated): \" + '\\033[1m' + f\"{agg_equity_change_sum} USD\" + '\\033[0m')\n",
    "    print('\\n', f\" • Aggregrated Percentage Change (avg):     \" + '\\033[1m' + f\"{agg_percent_change_avg} %\" + '\\033[0m')\n",
    "    print('\\n', f\" • Aggregrated Holding Period (avg):        \" + '\\033[1m' + f\"{agg_holding_period} days\" + '\\033[0m')\n",
    "    print('\\033[1m' + \"______________________________________________________\" + '\\033[0m')\n",
    "    print('\\033[1m' + '\\n\\n\\n• Recent' + '\\033[0m' +  ' - Top 5')\n",
    "    display(sss_perf.sort_values('date', ascending = False).reset_index(drop = True).head())\n",
    "    print('\\033[1m' + '\\n• Gain' + '\\033[0m' +  ' - Top 5')\n",
    "    display(sss_perf.nlargest(5,'percent_change').reset_index(drop = True))\n",
    "    print('\\033[1m' + '\\n• Loss' + '\\033[0m' +  ' - Top 5')\n",
    "    display(sss_perf.nsmallest(5,'percent_change').reset_index(drop = True))\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "    voice_message(f\"\"\"\\\n",
    "    Here is our real performance update\n",
    "    Aggregrated Equity Change is {agg_equity_change_sum} dollars\n",
    "    Aggregrated Percentage Change is {agg_percent_change_avg} %\n",
    "    Aggregrated Holding Period is {agg_holding_period} days\n",
    "    \"\"\")   \n",
    "\n",
    "    time.sleep(7)  \n",
    "    \n",
    "    \n",
    "def dmv_plot(ticker):\n",
    "    # generate a moving average plot per each day\n",
    "    \n",
    "    df = yfinance_df_rsi(ticker)[['Date', 'Adj Close']]\n",
    "    df = df.rename(columns = {'Adj Close':'Current Price'})\n",
    "    focus_days = [5, 10, 20, 30, 60, 120]\n",
    "\n",
    "    for i in focus_days:\n",
    "        df[f\"{i}_EMA\"] = df['Current Price'].ewm(span= i, adjust=False).mean()\n",
    "\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.title(f'Exponential Moving Average - {ticker}')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    for column in df.columns[1:]:\n",
    "        plt.errorbar(data = df, x = 'Date', y = column, linewidth = 1.5, label = column)\n",
    "        plt.annotate(xy = [df[-1:].Date.values, df[-1:][column].values], text = column)\n",
    "\n",
    "    # plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # buy-condition test\n",
    "    analysis = analysis_price(ticker)\n",
    "    print('• Loss-Cut Condition: ' + '\\033[1m'+ f'{analysis[8]}\\n' + '\\033[0m')\n",
    "    \n",
    "    \n",
    "def analysis_price(ticker):\n",
    "    with open(\"averageline.txt\", 'r') as f:\n",
    "        averageline = [line.rstrip('\\n') for line in f]\n",
    "        averageline = int(averageline[0])\n",
    "\n",
    "    df = yfinance_df(ticker)\n",
    "    \n",
    "    # fill ZERO value with the previous value\n",
    "    for i in df.all().reset_index().values:\n",
    "        if i[1] == False:\n",
    "            df[i[0]] = df[i[0]].replace(to_replace=0, method='ffill')\n",
    "    \n",
    "    df['VC'] = df['Volume'].diff()\n",
    "    df['PC'] = df['Adj Close'].diff()\n",
    "    df['PriceC_by_VolumeC'] = df['PC']/df['VC']\n",
    "  \n",
    "    # price check\n",
    "    ticker_price_mean = df.tail(averageline)['Adj Close'].mean()\n",
    "\n",
    "    ticker_price_yesterday = df.tail(2).head(1)['Adj Close'].mean()\n",
    "    ticker_price_current = df.tail(1)['Adj Close'].mean()\n",
    "    ticker_price_drop = ((ticker_price_yesterday - ticker_price_current) / ticker_price_yesterday)*100\n",
    "    ticker_price_surge = ((ticker_price_current - ticker_price_yesterday) / ticker_price_yesterday)*100\n",
    "    \n",
    "    # ema\n",
    "    ticker_price_longest_mean = df['Adj Close'].ewm(span= 120, adjust=False).mean().values[-1]\n",
    "    ticker_price_longer_mean = df['Adj Close'].ewm(span= 60, adjust=False).mean().values[-1]\n",
    "    ticker_price_midest_mean = df['Adj Close'].ewm(span= 30, adjust=False).mean().values[-1]\n",
    "    ticker_price_mider_mean = df['Adj Close'].ewm(span= 20, adjust=False).mean().values[-1]\n",
    "    ticker_price_mid_mean = df['Adj Close'].ewm(span= 10, adjust=False).mean().values[-1]\n",
    "    ticker_price_short_mean = df['Adj Close'].ewm(span= 5, adjust=False).mean().values[-1]\n",
    "    \n",
    "    ticker_price_trend_surge_1 = ticker_price_longest_mean < ticker_price_longer_mean < ticker_price_current\n",
    "    ticker_price_trend_surge_2 = ticker_price_longest_mean < ticker_price_longer_mean < ticker_price_midest_mean < ticker_price_current\n",
    "    ticker_price_trend_surge_3 = ticker_price_longest_mean < ticker_price_longer_mean < ticker_price_mider_mean < ticker_price_current\n",
    "    ticker_price_trend_surge_4 = ticker_price_longest_mean < ticker_price_longer_mean < ticker_price_mid_mean < ticker_price_current\n",
    "    ticker_price_trend_surge_5 = ticker_price_longest_mean < ticker_price_longer_mean < ticker_price_short_mean < ticker_price_current\n",
    "\n",
    "    ticker_price_trend_drop_1 = ticker_price_longest_mean < ticker_price_current < ticker_price_longer_mean\n",
    "    ticker_price_trend_drop_2 = ticker_price_longer_mean < ticker_price_current < ticker_price_longest_mean\n",
    "    ticker_price_trend_drop_3 = ticker_price_current < ticker_price_longer_mean < ticker_price_longest_mean\n",
    "    ticker_price_trend_drop_4 = ticker_price_current < ticker_price_longest_mean < ticker_price_longer_mean \n",
    "    ticker_price_trend_drop = ticker_price_trend_drop_1 + ticker_price_trend_drop_2 + ticker_price_trend_drop_3 + ticker_price_trend_drop_4\n",
    "    ticker_price_trend_drop = (ticker_price_trend_drop > 0)    \n",
    "\n",
    "    ticker_price_trend = (df[-1:]['Adj Close'].mean()-df[-20:-19]['Adj Close'].mean())/df[-20:-19]['Adj Close'].mean()*100\n",
    "    \n",
    "    ticker_price_long_hill = df[-60:-20]['Adj Close'].max()\n",
    "    ticker_price_short_hill = df[-20:]['Adj Close'].max()\n",
    "    ticker_price_hill = ticker_price_long_hill < ticker_price_short_hill\n",
    "    \n",
    "    df = df[-30:] # not fixed\n",
    "    Price_Volume_botton_day = df.nsmallest(1, 'PriceC_by_VolumeC').Date.astype(str).values[0]\n",
    "    day_minus = datetime.today() - timedelta(days = 5) # not fixed\n",
    "    day_minus = str(day_minus)[:10] # fixed\n",
    "    ticker_vol_price = day_minus > Price_Volume_botton_day\n",
    "\n",
    "    return(ticker_price_drop, \n",
    "           ticker_price_trend_surge_1, \n",
    "           ticker_price_surge, \n",
    "           ticker_price_mean, \n",
    "           ticker_price_current, \n",
    "           ticker_price_trend, \n",
    "           ticker_price_hill, \n",
    "           ticker_price_trend_surge_2, \n",
    "           ticker_price_trend_drop, \n",
    "           ticker_vol_price, \n",
    "           ticker_price_trend_surge_3,\n",
    "           ticker_price_trend_surge_4,\n",
    "           ticker_price_trend_surge_5)\n",
    "\n",
    "    ## For moving average tuning\n",
    "    # for i in range(1,30):\n",
    "    #     mostrecentdate = 20220101 + i\n",
    "    #     analysis_price('^GSPC', mostrecentdate)[1]\n",
    "    #     print('\\n\\n')\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # start declearing def() \n",
    "    currentdate_event = currentdate\n",
    "\n",
    "    if currentdate != currentdate_verify:\n",
    "        print(f\"Exception >>> Input: {currentdate} vs Actual: {currentdate_verify}\")\n",
    "        mostrecentdate = str(mostrecentdate)[0:4]+'-'+str(mostrecentdate)[4:6]+'-'+str(mostrecentdate)[6:8]\n",
    "        currentdate = str(currentdate)[0:4]+'-'+str(currentdate)[4:6]+'-'+str(currentdate)[6:8]  \n",
    "        \n",
    "#         # code cell break ###############\n",
    "#         class StopExecution(Exception):\n",
    "#             def _render_traceback_(self):\n",
    "#                 pass\n",
    "\n",
    "#         raise StopExecution\n",
    "#         # code cell break ###############\n",
    "\n",
    "    else:\n",
    "        print(\"> Date Verified\")\n",
    "        with open(\"mostrecentdate.txt\", 'w') as f:\n",
    "            f.write(str(mostrecentdate)) \n",
    "        with open(\"currentdate.txt\", 'w') as f:\n",
    "            f.write(str(currentdate)) \n",
    "\n",
    "        mostrecentdate = str(mostrecentdate)[0:4]+'-'+str(mostrecentdate)[4:6]+'-'+str(mostrecentdate)[6:8]\n",
    "        currentdate = str(currentdate)[0:4]+'-'+str(currentdate)[4:6]+'-'+str(currentdate)[6:8]\n",
    "        \n",
    "        # remove the previous files\n",
    "        monitoring_previous_exist = exists(f'qed_monitoring_done_{mostrecentdate}.txt')\n",
    "        if monitoring_previous_exist == True:\n",
    "            os.remove(f'qed_monitoring_done_{mostrecentdate}.txt')\n",
    "\n",
    "        focus_previous_exist = exists(f'focused_{mostrecentdate}.txt')\n",
    "        if focus_previous_exist == True:\n",
    "            os.remove(f'focused_{mostrecentdate}.txt')\n",
    "        \n",
    "    ydate = str(yfinance_df_setting('^GSPC')[-1:].Date.values[0])[:10]\n",
    "\n",
    "    if ydate == currentdate:\n",
    "        print(\"> yFinance Verified (Direct Call: yf.download(ticker))\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nException >>> yFinance\\n\")\n",
    "        print(f'yFinance: {ydate} vs Currentdate: {currentdate}')\n",
    "    #     print(yfinance_df_setting('^GSPC')[-1:])\n",
    "        alarm()\n",
    "\n",
    "voice_message(\"\"\"\\\n",
    "      Hi Sean. I'm Selma.\"\"\")\n",
    "time.sleep(3)\n",
    "\n",
    "\n",
    "print('\\n> Modules #1 imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19d1cedb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for sentiment analysis\n",
    "min_market_sentiment = 0\n",
    "mega_tickers = [\n",
    "    'GOOGL', 'META', 'TCEHY', 'SNAP', 'BIDU', 'MTCH',\n",
    "    'QCOM', 'INTC', 'AMD', 'TXN', 'AVGO', 'MU',\n",
    "    'AAPL', 'DELL', 'HPQ', 'FUJIY', 'CAJPY', 'STX',\n",
    "    'MSFT', 'ORCL', 'NOW', 'FTNT', 'PANW', 'VMW',\n",
    "    'NVDA', 'TSM', 'AVGO', 'INTC', 'AMD', 'QCOM',\n",
    "    'CRM', 'ADBE', 'SAP', 'INTU', 'TEAM', 'DASTY',\n",
    "    'TSLA', 'TM', 'GM', 'MBGAF', 'F', 'LCID',\n",
    "    'AMZN', 'BABA', 'JD', 'MELI', 'PDD', 'DASH',\n",
    "    'V', 'MA', 'PYPL', 'ADP', 'SQ', 'FISV',\n",
    "    'JPM', 'BAC', 'WFC', 'RY', 'TD', 'HSBC',\n",
    "    'JNJ', 'PFE', 'LLY', 'NVO', 'MRK', 'AZN',\n",
    "    'WMT', 'COST', 'WMMVY', 'CRRFY', 'BJ', 'ASAI',\n",
    "    'XOM', 'CVX', 'SHEL', 'BP', 'TTE', 'EQNR',\n",
    "    'AMT', 'CCI', 'EQIX', 'PSA', 'DLR', 'SBAC',\n",
    "    'DIS', 'NFLX', 'SPOT', 'LYV', 'WMG', 'ROKU']\n",
    "\n",
    "\n",
    "def treemap(df, column):\n",
    "    print('\\033[1m' + f'{column}' + '\\033[0m')\n",
    "    df = df.drop(index = df[df['Symbol'] == 'GOOG'].index.values[0]).reset_index(drop = True)\n",
    "    df = df[df[column] > 0]\n",
    "    fig = px.treemap(df, path=[px.Constant(\"S&P 500\"), 'GICS Sector', 'Symbol'], \n",
    "                     values = column,\n",
    "                     color = 'Security', \n",
    "                     hover_data = ['5D Perf'],\n",
    "                     color_continuous_scale='RdBu',\n",
    "                     color_continuous_midpoint=np.average(df['P/E TTM'], weights=df['P/E FWD']))\n",
    "    fig.update_traces(root_color=\"white\")\n",
    "    fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "def focus_record(ticker):\n",
    "    # create & update focused file if true\n",
    "    from os.path import exists\n",
    "    focus_today_exist = exists(f'focused_{currentdate}.txt')\n",
    "    if focus_today_exist == False:\n",
    "        with open(f'focused_{currentdate}.txt', 'w') as f:\n",
    "            f.write(str(ticker))\n",
    "        voice_message(f\"\"\"\\\n",
    "                      The initial focus file has been created with {ticker}\"\"\")\n",
    "    else:\n",
    "        with open(f'focused_{currentdate}.txt', 'r') as f:\n",
    "            focused = [line.rstrip('\\n') for line in f]\n",
    "        focused.append(ticker)\n",
    "        with open(f'focused_{currentdate}.txt', 'w') as f:\n",
    "            for s in focused:\n",
    "                f.write(str(s) + '\\n')\n",
    "\n",
    "\n",
    "def volatility_check(ticker):\n",
    "    df = yfinance_df_rsi(ticker).tail(averageline).reset_index()\n",
    "    price_change = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if i == 0:\n",
    "            price_change.append(0)\n",
    "        else:\n",
    "            price_change.append((df['Adj Close'].values[i] - df['Adj Close'].values[i-1]) / df['Adj Close'].values[i-1])\n",
    "    df['price_change'] = price_change   \n",
    "    return np.std(df['price_change']) * np.sqrt(252)\n",
    "\n",
    "\n",
    "def mdd_cal(tickers):\n",
    "    '''\n",
    "    input: ticker in list\n",
    "    output: average value\n",
    "    '''\n",
    "    mdd_avg = []\n",
    "    for ticker in tickers:\n",
    "        print(f'{mdd_lookback_days} days MDD calculating - Ticker: {ticker}', end = '                                                  \\r')\n",
    "        df = yfinance_df_rsi(ticker)[-mdd_lookback_days:]\n",
    "        mdd = (df['Adj Close'].max() - df['Adj Close'].min()) / df['Adj Close'].max()\n",
    "        mdd_avg.append(mdd)\n",
    "        print(end = '                                                              \\r')\n",
    "    return np.average(mdd_avg)\n",
    "\n",
    "\n",
    "def candle_review(ticker):\n",
    "    # get the price change(%)\n",
    "    percent_change_plot = np.round(analysis_price(ticker)[2], 2)\n",
    "    print('\\n• Ticker: ' + '\\033[1m' + f'{ticker} ==> ' + f'{percent_change_plot} %\\n' + '\\033[0m') \n",
    "    \n",
    "    df = pd.read_csv(f'candle_analysis_{ticker}_{currentdate}.csv', index_col = 'Unnamed: 0')\n",
    "    df.Minutepassed = df.Minutepassed.astype(int)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "\n",
    "    ax.plot(df.Minutepassed, df['Adj Close'], color = 'blue')\n",
    "    ax.set_xlabel('Minutepassed')\n",
    "    ax.set_ylabel('Price', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "    ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.bar(df.Minutepassed, df['VolumeC_per_PriceC'], color = 'red')\n",
    "    ax1.set_ylabel('Volume Change per Price Change', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    # ax.set_xticklabels(df.Date, rotation = 45)\n",
    "    plt.title(f'Volume Change per Price Change - {VC_per_PC_limit_V} ({ticker})')\n",
    "    plt.show()\n",
    "    print('\\n\\n')\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "\n",
    "    ax.plot(df.Minutepassed, df['Adj Close'], color = 'blue')\n",
    "    ax.set_xlabel('Minutepassed')\n",
    "    ax.set_ylabel('Price', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "    ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.bar(df.Minutepassed, df['VolumeSpeed_vs_mean'], color = 'red')\n",
    "    ax1.set_ylabel('Volume Speed vs Mean', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    # ax.set_xticklabels(df.Date, rotation = 45)\n",
    "    plt.title(f'Volume Speed vs Mean - {VS_vs_MEAN_limit} ({ticker})')\n",
    "    plt.show()\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "\n",
    "    ax.plot(df.Minutepassed, df['Adj Close'], color = 'blue')\n",
    "    ax.set_xlabel('Minutepassed')\n",
    "    ax.set_ylabel('Price', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "    ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.bar(df.Minutepassed, df['Volume_speed_acceleration'], color = 'red')\n",
    "    ax1.set_ylabel('Volume Speed Acceleration', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    # ax.set_xticklabels(df.Date, rotation = 45)\n",
    "    plt.title(f'Volume Speed Acceleration [abs] - {VS_Accel_limit} ({ticker})')\n",
    "    plt.show()\n",
    "    \n",
    "    # display df\n",
    "    df_1 = df[df['VolumeC_per_PriceC'] >= VC_per_PC_limit_V]\n",
    "    df_2 = df_1[df_1['VolumeSpeed_vs_mean'] >= VS_vs_MEAN_limit]\n",
    "    df_3 = df_2[np.absolute(df_2['Volume_speed_acceleration']) >= VS_Accel_limit]\n",
    "    df_3 = df_3.reset_index(drop = True)\n",
    "    display(df_3[['Minutepassed', 'VolumeC_per_PriceC', 'VolumeSpeed_vs_mean', 'Volume_speed_acceleration']])\n",
    "    print('\\n')\n",
    "    if len(df_3) != 0:        \n",
    "        sentiment_decision = (sentiment_analysis(ticker) > 0)\n",
    "        print('Sentiment Analysis Result: ' + '\\033[1m' + f'{sentiment_decision}' + '\\033[0m')\n",
    "        if sentiment_decision == True:\n",
    "            strategy_analysis_main(ticker, 'on', 'off')\n",
    "        else:\n",
    "            pass\n",
    "    else:\n",
    "        print('\\033[1m' + 'No Candle Sign' + '\\033[0m')\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "\n",
    "def candle_gathering():\n",
    "    # candle gahtering starts after the sector valuation has been executed\n",
    "    print(f'\\n\\n\\n\\n\\n{current_time()}  Candle Gathering')\n",
    "    \n",
    "    if cyclecount%100 == 0:\n",
    "        with open(\"portfolio_original.txt\", 'r') as f:\n",
    "            portfolio = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"buymonitor_blacklist_original.txt\", 'r') as f:\n",
    "            buymonitor_blacklist_original = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"buymonitor_blacklist.txt\", 'w') as f:\n",
    "            for s in buymonitor_blacklist_original:  \n",
    "                f.write(str(s) + '\\n')\n",
    "\n",
    "    else:\n",
    "        with open(\"portfolio_original.txt\", 'r') as f:\n",
    "            portfolio = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"portfolio_superpass.txt\", 'r') as f:\n",
    "            portfolio_superpass = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"etf_portfolio.txt\", 'r') as f:\n",
    "            etf_portfolio = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "            buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"inv_tickers.txt\", 'r') as f:\n",
    "            inv_tickers = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"portfolio_sector_true.txt\", 'r') as f:\n",
    "            portfolio_sector_true = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"portfolio_sector_valuation_true.txt\", 'r') as f:\n",
    "            portfolio_sector_valuation_true = [line.rstrip('\\n') for line in f]\n",
    "            \n",
    "        # exclude Sector valuation - False tickers. buymonitor() will only use candle-gathering for these tickers\n",
    "        sector_valuation_ticker = pd.read_csv('sector_valuation.csv')\n",
    "        sector_valuation_ticker = sector_valuation_ticker[sector_valuation_ticker['Date'] == currentdate]\n",
    "        sector_valuation_ticker = sector_valuation_ticker.reset_index(drop = True)\n",
    "        \n",
    "        if len(sector_valuation_ticker) != 0:\n",
    "            # Candle tickers    \n",
    "            purchased = pd.read_csv('robin_all.csv')['index'].values.tolist()\n",
    "            stock = portfolio_sector_true + portfolio_sector_valuation_true + portfolio_superpass + etf_portfolio + inv_tickers + purchased + index_candle_tickers\n",
    "\n",
    "            stock = [i for i in stock if i not in buymonitor_blacklist]\n",
    "            stock = [i for n, i in enumerate(stock) if i not in stock[:n]] #duplicated                \n",
    "\n",
    "            # shuffle the order considering the loop has a break\n",
    "            random.shuffle(stock)\n",
    "            \n",
    "            from tqdm.notebook import tqdm\n",
    "            for i in tqdm(range(len(stock))):\n",
    "                time.sleep(0.3) \n",
    "                try:\n",
    "                    print('Candle gathering: ', stock[i], end = '          \\r')\n",
    "                    candle_sign = volume_vs_sign(stock[i])\n",
    "                    if candle_sign == 3:\n",
    "                        print('\\nBuy-Sign detected. Break Candle gathering.\\n')\n",
    "                        \n",
    "                        try:\n",
    "                            # create bypass token to read in 'buymonitor()'\n",
    "                            with open(f\"candle_token_{stock[i]}.txt\", 'w') as f:\n",
    "                                f.write(str(timechecknow()))\n",
    "                            print(\"\\n\\nFile Created >>> \" + '\\033[1m' + f\"candle_token_{stock[i]}.txt\\n\\n\" + '\\033[0m')\n",
    "                        except:\n",
    "                            print(\"\\n\\nExcept >>> File Creation Failure: \" + '\\033[1m' + f\"candle_token_{stock[i]}.txt\\n\\n\" + '\\033[0m')\n",
    "                            alarm()\n",
    "                            \n",
    "                        break\n",
    "\n",
    "                except KeyError:\n",
    "                    alarm()\n",
    "                    print('Except >>> Candle gathering:', stock[i])\n",
    "\n",
    "                    # append to the purchased ticker to monitoring blacklist; only purchase once a day\n",
    "                    with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "                        buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "                    buymonitor_blacklist.append(stock[i])\n",
    "                    with open(\"buymonitor_blacklist.txt\", 'w') as f:\n",
    "                        for s in buymonitor_blacklist:  \n",
    "                            f.write(str(s) + '\\n') \n",
    "                    pass\n",
    "            with open(\"_Candle Time Checker.txt\", 'w') as f:\n",
    "                f.write(str(current_time()))  \n",
    "\n",
    "        else:\n",
    "            print(f'\\n\\n{current_time()}  The initial sector valuation has not been executed.\\n\\n')\n",
    "            time.sleep(180)\n",
    "\n",
    "\n",
    "def rsi(ticker):\n",
    "    df = yfinance_df_rsi(ticker)\n",
    "\n",
    "    ## 14_Day RSI\n",
    "    df['RSI'] = ta.rsi(df['Adj Close'], length = 14)\n",
    "\n",
    "    # moving average - 70 days\n",
    "    date_list = df['Date'][69:].values\n",
    "    mvp = 70\n",
    "    mavg = pd.DataFrame()\n",
    "    for column in df.columns[1:]:\n",
    "        mv_change = np.array(df[column])\n",
    "        mv = []\n",
    "        for i in range(len(mv_change)-mvp+1):\n",
    "            mv.append(np.average(mv_change[i:mvp+i]))\n",
    "            i+=1\n",
    "        mavg[column] = pd.DataFrame(mv)\n",
    "    df_70 = mavg\n",
    "    df_70['Date'] = date_list\n",
    "    df_70 = df_70.rename(columns = {'Adj Close' : 'Adj Close_70D', 'RSI' : 'RSI_70D'})[['Date', 'Adj Close_70D', 'RSI_70D']]\n",
    "\n",
    "    # moving average - 40 days\n",
    "    date_list = df['Date'][39:].values\n",
    "    mvp = 40\n",
    "    mavg = pd.DataFrame()\n",
    "    for column in df.columns[1:]:\n",
    "        mv_change = np.array(df[column])\n",
    "        mv = []\n",
    "        for i in range(len(mv_change)-mvp+1):\n",
    "            mv.append(np.average(mv_change[i:mvp+i]))\n",
    "            i+=1\n",
    "        mavg[column] = pd.DataFrame(mv)\n",
    "    df_40 = mavg\n",
    "    df_40['Date'] = date_list\n",
    "    df_40 = df_40.rename(columns = {'Adj Close' : 'Adj Close_40D', 'RSI' : 'RSI_40D'})[['Date', 'Adj Close_40D', 'RSI_40D']]\n",
    "    \n",
    "    # moving average - 20 days\n",
    "    date_list = df['Date'][19:].values\n",
    "    mvp = 20\n",
    "    mavg = pd.DataFrame()\n",
    "    for column in df.columns[1:]:\n",
    "        mv_change = np.array(df[column])\n",
    "        mv = []\n",
    "        for i in range(len(mv_change)-mvp+1):\n",
    "            mv.append(np.average(mv_change[i:mvp+i]))\n",
    "            i+=1\n",
    "        mavg[column] = pd.DataFrame(mv)\n",
    "    df_20 = mavg\n",
    "    df_20['Date'] = date_list\n",
    "    df_20 = df_20.rename(columns = {'Adj Close' : 'Adj Close_20D', 'RSI' : 'RSI_20D'})[['Date', 'Adj Close_20D', 'RSI_20D']]\n",
    "    \n",
    "    # merge\n",
    "    df['Date'] = df['Date'].astype('datetime64[ns]')\n",
    "    df = pd.merge(df, df_20, how = 'inner', on = 'Date')\n",
    "    df = pd.merge(df, df_40, how = 'inner', on = 'Date')\n",
    "    df = pd.merge(df, df_70, how = 'inner', on = 'Date')\n",
    "    \n",
    "    # accel Creation for 70D and 40D\n",
    "    RSI_70D_change = []\n",
    "    for i in range(len(df['RSI_70D'])):\n",
    "        if i == 0:\n",
    "            RSI_70D_change.append(0)\n",
    "        else:\n",
    "            change = (df['RSI_70D'][i] - df['RSI_70D'][i-1]) / df['RSI_70D'][i-1] *100\n",
    "            RSI_70D_change.append(change)\n",
    "    df['RSI_70D_change'] = RSI_70D_change\n",
    "    df = df.drop(index = 0).reset_index(drop = True)\n",
    "\n",
    "    RSI_70D_change_accel = []\n",
    "    for i in range(len(df['RSI_70D_change'])):\n",
    "        if i == 0:\n",
    "            RSI_70D_change_accel.append(0)\n",
    "        else:\n",
    "            change = np.abs((df['RSI_70D_change'][i] - df['RSI_70D_change'][i-1]) / df['RSI_70D_change'][i-1] *100)\n",
    "            RSI_70D_change_accel.append(change)\n",
    "    df['RSI_70D_change_accel'] = RSI_70D_change_accel\n",
    "    df = df.drop(index = 0).reset_index(drop = True)\n",
    "        \n",
    "    RSI_40D_change = []\n",
    "    for i in range(len(df['RSI_40D'])):\n",
    "        if i == 0:\n",
    "            RSI_40D_change.append(0)\n",
    "        else:\n",
    "            change = (df['RSI_40D'][i] - df['RSI_40D'][i-1]) / df['RSI_40D'][i-1] *100\n",
    "            RSI_40D_change.append(change)\n",
    "    df['RSI_40D_change'] = RSI_40D_change\n",
    "    df = df.drop(index = 0).reset_index(drop = True)\n",
    "\n",
    "    RSI_40D_change_accel = []\n",
    "    for i in range(len(df['RSI_40D_change'])):\n",
    "        if i == 0:\n",
    "            RSI_40D_change_accel.append(0)\n",
    "        else:\n",
    "            change = np.abs((df['RSI_40D_change'][i] - df['RSI_40D_change'][i-1]) / df['RSI_40D_change'][i-1] *100)\n",
    "            RSI_40D_change_accel.append(change)\n",
    "    df['RSI_40D_change_accel'] = RSI_40D_change_accel\n",
    "    df = df.drop(index = 0).reset_index(drop = True)\n",
    "    \n",
    "    # save\n",
    "#     df.to_csv(f\"./Backup_rsi/rsi_{ticker}_{currentdate}.csv\", index = False)\n",
    "\n",
    "#     if ticker in index_ticker or ticker in etf_ticker:\n",
    "#         df.to_csv(f\"rsi_{ticker}.csv\", index = False)\n",
    "    df.dropna(inplace = True)\n",
    "    df.to_csv('rsi_report.csv', index = False)\n",
    "\n",
    "    # buy condition \n",
    "    rsi_max_basic = df[-1:]['RSI'].mean() < rsi_max_cut\n",
    "    rsi_70_min_plus = df.tail(3)['RSI_70D'].mean() >= 50\n",
    "    rsi_40_min_plus = df.tail(3)['RSI_40D'].mean() >= 40\n",
    "    price_avg_20_70 = df.tail(3)['Adj Close_20D'].mean() > df.tail(3)['Adj Close_70D'].mean()\n",
    "    rsi_surge = df.tail(20)['RSI_40D_change'].mean() >= 0 \n",
    "    rsi_accel = df.tail(20)['RSI_40D_change_accel'].mean() > np.percentile(df['RSI_40D_change_accel'], 75)\n",
    "    \n",
    "    # sell condition\n",
    "    rsi_70_min_minus = df.tail(3)['RSI_70D'].mean() < 50 \n",
    "    rsi_40_min_minus = df.tail(3)['RSI_40D'].mean() < 40 \n",
    "    price_avg_70_20 = df.tail(3)['Adj Close_20D'].mean() < df.tail(3)['Adj Close_70D'].mean()\n",
    "    \n",
    "    # sell-alarm condition\n",
    "    rsi_70_min_minus = df.tail(3)['RSI_70D'].mean() < 50\n",
    "    rsi_40_min_minus = df.tail(3)['RSI_40D'].mean() < 40\n",
    "    rsi_drop = df.tail(20)['RSI_40D_change'].mean() < 0 \n",
    "    \n",
    "    buy = rsi_max_basic and rsi_70_min_plus and rsi_40_min_plus and price_avg_20_70 \n",
    "    sell = rsi_70_min_minus or rsi_40_min_minus or price_avg_70_20 \n",
    "    sell_alarm = rsi_70_min_minus and rsi_40_min_minus and price_avg_70_20 and rsi_drop and rsi_accel\n",
    "    loss_cut = rsi_70_min_minus and rsi_40_min_minus and price_avg_70_20 \n",
    "    portfolio = rsi_70_min_plus and rsi_40_min_plus\n",
    "\n",
    "    return [buy, sell, sell_alarm, loss_cut, portfolio]\n",
    "\n",
    "\n",
    "def rsi_plot(ticker):\n",
    "    ticker_name = ticker\n",
    "    ticker_rsi = rsi(ticker)\n",
    "    ticker_rsi = pd.read_csv('rsi_report.csv')[-review_plot_lookbackdays:]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "\n",
    "    ax.plot(ticker_rsi.Date, ticker_rsi['Adj Close'], color = 'blue')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "#     ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(ticker_rsi.Date, ticker_rsi['RSI'], color = 'red')\n",
    "    ax1.set_ylabel('RSI', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    ax.set_xticklabels(ticker_rsi.Date, rotation = 90)\n",
    "    ax1.plot(ticker_rsi.Date, [30]*ticker_rsi.shape[0] ,color = 'black', linestyle = '--')\n",
    "    ax1.plot(ticker_rsi.Date, [70]*ticker_rsi.shape[0] ,color = 'black', linestyle = '--')\n",
    "\n",
    "    plt.title(f'Price vs RSI ({ticker_name})')\n",
    "    plt.show()\n",
    "\n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    ticker_rsi['intercept'] = 1\n",
    "    lm = sm.OLS(ticker_rsi['RSI'], ticker_rsi[['intercept', 'Adj Close']])\n",
    "    results = lm.fit()\n",
    "\n",
    "    rsi_coef = np.round(results.params.loc['Adj Close'], 2)\n",
    "    rsi_rsq = np.round(results.rsquared*100, 2)\n",
    "\n",
    "    print(\"• R-Squared: \" + '\\033[1m' + f\"{rsi_rsq} %\" + '\\033[0m')\n",
    "    print(\"• RSI per Price Change: \" + '\\033[1m' + f\"{rsi_coef}\" + '\\033[0m')\n",
    "    \n",
    "\n",
    "def rsi_portfolio():\n",
    "    ##### RSI Portfolio searching for S&P 500 tickers #####\n",
    "    print('\\n', '\\033[1m' + '>>> RSI Portfolio Searching .....' + '\\033[0m', '\\n')\n",
    "    voice_message(\"\"\"\\\n",
    "        RSI Portfolio Searching starts\"\"\")        \n",
    "    snp = pd.read_csv('snp500\\snp_sector_index.csv').sort_values('Symbol')['Symbol'].to_list()\n",
    "    rsi_portfolio = []\n",
    "    c = 0\n",
    "    for i in snp:\n",
    "        c+=1\n",
    "        print(f\"({c}) RSI Checking - Ticker: {i}\")\n",
    "        try:\n",
    "            if rsi(i)[0] == True:\n",
    "                print(f\"                                      >>> RSI into Portfolio - Ticker: {i}\")\n",
    "                rsi_portfolio.append(i)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print(f\"{i} is delisted.\")\n",
    "            pass\n",
    "\n",
    "    with open('rsi_portfolio.txt', 'w') as f:\n",
    "        for s in rsi_portfolio:\n",
    "            f.write(str(s) + '\\n')\n",
    "\n",
    "    print(f\"\\n\\nPortfolio created: {rsi_portfolio}\")\n",
    "    \n",
    "    \n",
    "def sector_stock_plot(stock_ticker):\n",
    "    title_name = stock_ticker\n",
    "    sector_ticker = pd.read_csv('./snp500/snp_sector_index.csv')\n",
    "    sector_ticker = sector_ticker[sector_ticker['Symbol'] == stock_ticker]['Ticker'].values[0]\n",
    "    \n",
    "    stock_ticker = yfinance_df(stock_ticker)[-review_plot_lookbackdays:]\n",
    "    sector_ticker = yfinance_df(sector_ticker)[-review_plot_lookbackdays:]\n",
    "    stock_ticker.Date = stock_ticker.Date.astype(str)\n",
    "    sector_ticker.Date = sector_ticker.Date.astype(str)\n",
    "\n",
    "    # twin plot for sector vs stock\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "\n",
    "    ax.plot(stock_ticker.Date, stock_ticker['Adj Close'], color = 'blue')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Stock Price', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "#     ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(sector_ticker.Date, sector_ticker['Adj Close'], color = 'red')\n",
    "    ax1.set_ylabel('Sector Price', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    ax.set_xticklabels(stock_ticker.Date, rotation = 90)\n",
    "\n",
    "    plt.title(f'Ticker vs Sector ({title_name})')\n",
    "    plt.show()\n",
    "    \n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    stock_ticker['intercept'] = 1\n",
    "    lm = sm.OLS(sector_ticker['Adj Close'].values, stock_ticker[['intercept', 'Adj Close']])\n",
    "    results = lm.fit()\n",
    "\n",
    "    stock_coef = np.round(results.params.loc['Adj Close'], 2)\n",
    "    stock_rsq = np.round(results.rsquared*100, 2)\n",
    "\n",
    "    print(\"• R-Squared: \" + '\\033[1m' + f\"{stock_rsq} %\" + '\\033[0m')\n",
    "    print(\"• Stock Coef. vs Sector: \" + '\\033[1m' + f\"$ {stock_coef}\\n\\n\\n\" + '\\033[0m')\n",
    "    \n",
    "    \n",
    "def volume_price_period_plot(ticker):\n",
    "    # candle chart for period\n",
    "    \n",
    "    title_name = ticker\n",
    "    df = yfinance_df_rsi(ticker)[-review_plot_lookbackdays:]\n",
    "    df.Date = df.Date.astype(str)\n",
    "\n",
    "#     sc = MinMaxScaler()\n",
    "#     vol_scaled = sc.fit_transform(df['Volume'].values.reshape(-1, 1))\n",
    "#     price_scaled = sc.transform(df['Adj Close'].values.reshape(-1, 1))\n",
    "#     df['Volume_scaled'] = np.absolute(vol_scaled)\n",
    "#     df['Price_scaled'] = np.absolute(price_scaled)\n",
    "#     df['PriceC_by_VolumeC'] = vol_scaled/price_scaled\n",
    "    df['VC'] = df['Volume'].diff()\n",
    "    df['PC'] = df['Adj Close'].diff()\n",
    "    df['PriceC_by_VolumeC'] = df['PC']/df['VC']\n",
    "\n",
    "    Up_Down = []\n",
    "    for i in df['Adj Close'].diff():\n",
    "        if i > 0:\n",
    "            Up_Down.append(1)\n",
    "        else:\n",
    "            Up_Down.append(0)\n",
    "    df['Up_Down'] = Up_Down\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(20, 9)\n",
    "\n",
    "    ax.plot(df.Date, df['Adj Close'], color = 'blue')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Price', color = 'blue')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "#     ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    \n",
    "#     ax1.bar(df.Date, df['PriceC_by_VolumeC'], color = 'red')\n",
    "#     ax1.set_ylabel('Volume per Price', color = 'red')\n",
    "    ax1.bar(df.Date, df['PriceC_by_VolumeC'], color = 'red')\n",
    "    ax1.set_ylabel('Price Change by Volume Change', color = 'red')\n",
    "    \n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    ax.set_xticklabels(df.Date, rotation = 90)\n",
    "    plt.title(f'Price Change by Volume Change ({title_name}) - Super Pass Lookback days: {superpass_buysign_lookbackdays}')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def value_snp_pick():\n",
    "    # need to adjust thresholds loosely\n",
    "    snp = pd.read_csv('./snp500/snp500.csv').sort_values('Symbol')\n",
    "    slist = snp.Symbol.values \n",
    "    \n",
    "    # filter out Blacklist\n",
    "    with open('buymonitor_blacklist.txt', 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "    slist_before_ratioAnalysis = [i for i in slist if i not in buymonitor_blacklist]\n",
    "    \n",
    "    snp_value_list_1 = slist_before_ratioAnalysis[:int(len(slist_before_ratioAnalysis)/2)]\n",
    "    snp_value_list_2 = slist_before_ratioAnalysis[int(len(slist_before_ratioAnalysis)/2):]\n",
    "    \n",
    "    # ROE setting\n",
    "    set_roe = pd.read_csv('snp500 %s.csv' %currentdate).describe().reset_index()['Return on Equity']\n",
    "    set_roe_median = float(set_roe[5])\n",
    "    set_roe_quartile = float(set_roe[4])\n",
    "    \n",
    "    list1 = ratio_analysis(snp_value_list_1, set_roe_median, set_roe_quartile)\n",
    "    list2 = ratio_analysis(snp_value_list_2, set_roe_median, set_roe_quartile)\n",
    "    \n",
    "    snp_value_tickers = list1 + list2\n",
    "    \n",
    "    with open('snp_value_tickers.txt', 'w') as f:\n",
    "        for s in snp_value_tickers:\n",
    "            f.write(str(s) + '\\n')\n",
    "            \n",
    "    \n",
    "def ratio_analysis(slist_before_ratioAnalysis, set_roe_median, set_roe_quartile):\n",
    "\n",
    "    # filter out Ratio Analysis\n",
    "    slist_before_backTest = []\n",
    "    import time\n",
    "    print('Ratio Analysis:\\n', slist_before_ratioAnalysis)\n",
    "    print('\\n')\n",
    "    c = 1\n",
    "    for i in slist_before_ratioAnalysis:\n",
    "        try:\n",
    "            print('Analysing: ', c, '/', len(slist_before_ratioAnalysis),'- Ticker: ', i, end = '                     \\r')\n",
    "            c+=1        \n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            df = pd.DataFrame([yf.Ticker(i).info]).fillna(0)\n",
    "            df_fin = yf.Ticker(i).financials.reset_index()\n",
    "\n",
    "            eps_growth_rate = (np.absolute((df['forwardEps'].values[0] - df['trailingEps'].values[0])) / df['trailingEps'].values[0]) * 100\n",
    "            ebit_4yr = df_fin[df_fin['index'] == 'Ebit'].iloc[:, 1:].values\n",
    "            rev_4yr = df_fin[df_fin['index'] == 'Total Revenue'].iloc[:, 1:].values\n",
    "            try:\n",
    "                ebit_ratio_4yr = ebit_4yr / rev_4yr\n",
    "            except ZeroDivisionError:\n",
    "                pass\n",
    "            if type(ebit_ratio_4yr) == float:\n",
    "                ebit_ratio_4yr = np.flip(ebit_ratio_4yr).tolist()\n",
    "            else:\n",
    "                ebit_ratio_4yr = np.flip(ebit_ratio_4yr).tolist()[0]\n",
    "            avg_ebit_ratio_4yr = np.average(ebit_ratio_4yr) # >10%\n",
    "            if type(ebit_ratio_4yr) == float:\n",
    "                ebit_ratio_4yr_diff = np.absolute(ebit_ratio_4yr)\n",
    "            else:\n",
    "                ebit_ratio_4yr_diff = np.absolute(np.diff(ebit_ratio_4yr))\n",
    "            ebit_growth_4yr = []\n",
    "            for eb in range(len(ebit_ratio_4yr_diff)):\n",
    "                ebit_growth_4yr.append(ebit_ratio_4yr_diff[eb] / np.absolute(ebit_ratio_4yr[eb]))\n",
    "            avg_ebit_growth_ratio = np.average(ebit_growth_4yr) # 4yr avg ebit growth% > 10%    \n",
    "\n",
    "            ni_4yr = df_fin[df_fin['index'] == 'Net Income'].iloc[:, 1:].values\n",
    "            ni_4yr = np.flip(ni_4yr).tolist()[0]\n",
    "            ni_4yr_diff = np.absolute(np.diff(ni_4yr))\n",
    "            ni_4yr_growth = []\n",
    "            for ni in range(len(ni_4yr_diff)):\n",
    "                ni_4yr_growth.append(ni_4yr_diff[ni] / np.absolute(ni_4yr[ni]))\n",
    "            avg_ni_4yr_growth = np.average(ni_4yr_growth) # 20% < 4 yr avg net income growth < 50%\n",
    "\n",
    "            if df['sector'].values[0] != 'Financial Services':\n",
    "\n",
    "                #yf.Ticker(\"googl\").recommendations.reset_index()\n",
    "                try:    \n",
    "                    if (eps_growth_rate + df['trailingAnnualDividendRate'].values[0]) / df['trailingPE'].values[0] > 1.02 and avg_ebit_ratio_4yr > 0.1 and avg_ebit_growth_ratio > 0.1 and 0.3 <= avg_ni_4yr_growth <= 3 and df['recommendationKey'].values[0] == 'buy' and df['operatingCashflow'].values[0] > 0 and df['freeCashflow'].values[0] > 0 and df['ebitdaMargins'].values[0] > 0.3 and df['currentRatio'].values[0] > 1.2 and df['debtToEquity'].values[0] < 175 and df['returnOnEquity'].values[0] > set_roe_median and df['beta'].values[0] < 1.5 and df['targetMeanPrice'].values[0] > df['currentPrice'].values[0] and df['forwardPE'].values[0] < df['trailingPE'].values[0] and df['pegRatio'].values[0] < 1:\n",
    "                        slist_before_backTest.append(i)\n",
    "                        print(\"______________________________\\n\")                                                    \n",
    "                        print(f\"Ticker all passed: {i}\")\n",
    "                        print(\"______________________________\\n\\n\") \n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    print('Net Income Minus: ', i, end = '                 \\r')\n",
    "                    pass\n",
    "            else:\n",
    "                # for bank stocks\n",
    "                try:\n",
    "                    if (eps_growth_rate + df['trailingAnnualDividendRate'].values[0]) / df['trailingPE'].values[0] > 1.02 and 0.3 <= avg_ni_4yr_growth <= 3 and df['recommendationKey'].values[0] == 'buy' and df['operatingCashflow'].values[0] > 0 and df['currentRatio'].values[0] > 1 and df['debtToEquity'].values[0] < 175 and df['returnOnEquity'].values[0] > set_roe_median and df['beta'].values[0] < 1.5 and df['targetMeanPrice'].values[0] > df['currentPrice'].values[0] and df['forwardPE'].values[0] < df['trailingPE'].values[0] and df['pegRatio'].values[0] < 1:\n",
    "                        slist_before_backTest.append(i)\n",
    "                        print(\"______________________________\\n\")                                                    \n",
    "                        print(f\"Ticker all passed: {i}\")\n",
    "                        print(\"______________________________\\n\\n\") \n",
    "                    else:\n",
    "                        pass\n",
    "                except:\n",
    "                    print('Net Income Minus: ', i, end = '                \\r')\n",
    "                    pass\n",
    "        except:\n",
    "            print('Except from ratio analysis out of Sector: ', i, end = '                  \\r')\n",
    "            pass\n",
    "\n",
    "    # Back test: average price change within 10 years \n",
    "    slist_after_backtest = []\n",
    "    print('\\n\\nBack test:\\n', slist_before_backTest)\n",
    "    for ticker in slist_before_backTest:\n",
    "        try:\n",
    "            df = yfinance_df(ticker)\n",
    "            # check all time record\n",
    "            price_odd = []\n",
    "            price_even = []\n",
    "            for i in range(len(df['Adj Close'])):\n",
    "                if i%2 == 0:\n",
    "                    price_odd.append(df['Adj Close'][i])\n",
    "                else:\n",
    "                    price_even.append(df['Adj Close'][i])\n",
    "            price_change = []\n",
    "            for i in range(len(price_even)):\n",
    "                price_change.append((price_even[i] - price_odd[i]) / price_odd[i]*100)\n",
    "            backtest_yr_change = pd.DataFrame(np.absolute(price_change)).rename(columns = {0:'PriceChange'})['PriceChange'].mean()\n",
    "###############################################################################################\n",
    "            if backtest_yr_change < 2.5:  # filter out per average price change more than < xx\n",
    "###############################################################################################\n",
    "                slist_after_backtest.append(ticker)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            alarm()\n",
    "            \n",
    "            print('except from back test: ', ticker)\n",
    "    print('\\nFinal tickers:\\n', slist_after_backtest)\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    return slist_after_backtest    \n",
    "\n",
    "\n",
    "def sentiment_analysis(ticker):\n",
    "    df = pd.read_csv('./snp500/snp_sector_index.csv')\n",
    "    sector_ticker = df[df['GICS Sector'] == df[df['Symbol'] == ticker]['GICS Sector'].values[0]].Ticker.values[0]\n",
    "    mega_tickers = df[df['GICS Sector'] == df[df['Symbol'] == ticker]['GICS Sector'].values[0]].Symbol.tolist()\n",
    "    print(f'\\n================[ Sentiment Analysis: {ticker} / {sector_ticker} ]================')\n",
    "    \n",
    "    return sentiment_analysis_cal(mega_tickers)\n",
    "\n",
    "\n",
    "def sentiment_analysis_cal(mega_tickers):\n",
    "    analysis_result = []\n",
    "    for i in tqdm(range(len(mega_tickers))):\n",
    "        try:\n",
    "            sentiment_analysis = news_sentiment_analysis(mega_tickers[i])\n",
    "            print(f'Ticker: {mega_tickers[i]} | {np.round(sentiment_analysis, 3)}', end = '                            \\r')\n",
    "            if sentiment_analysis != 0:\n",
    "                analysis_result.append(sentiment_analysis)\n",
    "            time.sleep(0.1)\n",
    "        except:\n",
    "            pass\n",
    "    print(end = '                                                                                   \\r')\n",
    "    avg_sentiment = np.average(analysis_result)  \n",
    "    if avg_sentiment < 0:\n",
    "        avg_sentiment = 0        \n",
    "    return avg_sentiment\n",
    "\n",
    "\n",
    "def sentiment_portfolio(tickers, fold=7):\n",
    "    \n",
    "    print('\\033[1m' + f'\\n\\n\\n================[ Sentiment Portfolio Analysis ]=====================' + '\\033[0m')\n",
    "\n",
    "    # set the target threshold to filter out\n",
    "    comparison_value = sentiment_analysis_cal(mega_tickers)*fold\n",
    "\n",
    "    sentiment_portfolio_list = []\n",
    "    for i in tqdm(range(len(tickers))):\n",
    "        try:\n",
    "            analysis_result = news_sentiment_analysis(tickers[i])\n",
    "            print(f'Ticker: {tickers[i]} | {np.round(analysis_result, 3)}', end='                            \\r')\n",
    "            if analysis_result > comparison_value:\n",
    "                sentiment_portfolio_list.append(tickers[i])\n",
    "            time.sleep(0.1)\n",
    "        except:\n",
    "            pass\n",
    "    print(end='                                                                                   \\r')\n",
    "    print('\\033[1m' + f'=====================================================================' + '\\033[0m')\n",
    "    with open('sentiment_portfolio.txt', 'w') as f:\n",
    "        for s in sentiment_portfolio_list:\n",
    "            f.write(str(s) + '\\n')\n",
    "    return sentiment_portfolio_list\n",
    "\n",
    "\n",
    "def news_sentiment_analysis(ticker, display = 'off'):\n",
    "    \n",
    "    finwiz_url = 'https://finviz.com/quote.ashx?t='\n",
    "    url = finwiz_url + ticker\n",
    "    req = Request(url=url,headers={'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:20.0) Gecko/20100101 Firefox/20.0'}) \n",
    "    response = urlopen(req)    \n",
    "    # Read the contents of the file into 'html'\n",
    "    html = BeautifulSoup(response)\n",
    "    # Find 'news-table' in the Soup and load it into 'news_table'\n",
    "    news_table = html.find(id='news-table')\n",
    "\n",
    "    parsed_news = []\n",
    "    # Iterate through the news\n",
    "    for x in news_table.findAll('tr'):\n",
    "        # read the text from each tr tag into text\n",
    "        # get text from a only\n",
    "        headline = x.get_text() \n",
    "        # get url\n",
    "        a_tag = x.find('a')\n",
    "        url = a_tag['href'] if a_tag else ''\n",
    "        # split text in the td tag into a list \n",
    "        date_scrape = x.td.text.split()\n",
    "        # if the length of 'date_scrape' is 1, load 'time' as the only element\n",
    "        if len(date_scrape) == 1:\n",
    "            time = date_scrape[0]\n",
    "            date = np.nan\n",
    "        # else load 'date' as the 1st element and 'time' as the second    \n",
    "        else:\n",
    "            date = date_scrape[0]\n",
    "            time = date_scrape[1]\n",
    "\n",
    "        # Append ticker, date, time and headline as a list to the 'parsed_news' list\n",
    "        parsed_news.append([ticker, date, time, headline, url])\n",
    "\n",
    "    # Instantiate the sentiment intensity analyzer\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    # Set column names\n",
    "    columns = ['ticker', 'date', 'time', 'headline', 'url']\n",
    "    # Convert the parsed_news list into a DataFrame called 'df'\n",
    "    df = pd.DataFrame(parsed_news, columns=columns)\n",
    "\n",
    "    # Iterate through the headlines and get the polarity scores using vader\n",
    "    scores = df['headline'].apply(vader.polarity_scores).tolist()\n",
    "\n",
    "    # Convert the 'scores' list of dicts into a DataFrame\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    \n",
    "    # Join the DataFrames of the news and the list of dicts\n",
    "    df = df.join(scores_df, rsuffix='_right')\n",
    "    \n",
    "    # Convert the date column from string to datetime\n",
    "    df['date'] = pd.to_datetime(df.date).dt.date\n",
    "    df.date = df.date.fillna(method = 'ffill')\n",
    "    df_accu_check = df.groupby(['ticker','date']).mean().reset_index()\n",
    "    \n",
    "    # reverse roder\n",
    "    df = df[::-1].reset_index(drop = True)\n",
    "    \n",
    "    # get tema for compound\n",
    "    df[\"compound_TEMA\"] = ta.tema(df['compound'], 3)\n",
    "\n",
    "    if display == 'on':\n",
    "        # display\n",
    "        plt.figure(figsize = (16, 9))\n",
    "        plt.title(f'{current_time()} Sentiment Analysis - Ticker: {ticker}')\n",
    "        plt.errorbar(df.index, df.compound, color = 'black', alpha = 0.3, linewidth = 3, label = 'Compound')\n",
    "        plt.errorbar(df.index, df.compound_TEMA, color = 'black', alpha = 0.8, linewidth = 3, label = 'Compound TEMA')\n",
    "        plt.bar(df.index, df.pos, color = 'blue', alpha = 0.3, label = 'Positive')\n",
    "        plt.bar(df.index, -df.neg, color = 'red', alpha = 0.3, label = 'Negative')\n",
    "        plt.legend()\n",
    "        plt.xlabel('\\nNumber of Articles')\n",
    "        plt.ylabel('\\nSentiment')\n",
    "        plt.grid()\n",
    "        plt.show()        \n",
    "    \n",
    "    # adjust the number of the most recent articles to refer\n",
    "    num_of_articles = int(len(df) * 0.2)\n",
    "    if num_of_articles < 10:\n",
    "        num_of_articles = 10\n",
    "    return_value = df[-num_of_articles:].compound_TEMA.mean() #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # date check\n",
    "    article_first_date = str(df[:1].date.values[0])+ ' ' + str(df[:1].time.values[0])\n",
    "    article_effect_date = str(df[-num_of_articles:][:1].date.values[0])+ ' ' + str(df[:1].time.values[0])\n",
    "    article_last_date = str(df[-1:].date.values[0])+ ' ' + str(df[-1:].time.values[0])\n",
    "\n",
    "    if display == 'on':\n",
    "        print('\\n\\n=================[Result]==================')\n",
    "        print(' • Sentiment Compound:  ' + '\\033[1m' + f'{np.round(return_value, 2)}' + '\\033[0m')\n",
    "        print(' • Positive Sign (avg): ' + f'{np.round(df[-num_of_articles:].pos.mean(), 2)}')\n",
    "        print(' • Negative Sign (avg): ' + f'{np.round(df[-num_of_articles:].neg.mean(), 2)}')\n",
    "        print(' • Total Articles:      ' + f'{len(df)}')\n",
    "        print(' • Target Articles:     ' + f'{num_of_articles}')\n",
    "        print(' • Article First Date:  ' + f'{article_first_date}')\n",
    "        print(' • Article Effect Date: ' + f'{article_effect_date}')\n",
    "        print(' • Article Last Date:   ' + f'{article_last_date}')\n",
    "        print('===========================================')\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "\n",
    "def daily_news_sentiment_all(display = 'off'):\n",
    "\n",
    "    news = News()\n",
    "    # available variables:\n",
    "    # print(news.main_url)  # scraped URL\n",
    "    # print(news.soup)  # beautiful soup object\n",
    "    # print(news.news_df)  # NEWS table information in a pd.DataFrame object\n",
    "    # print(news.blogs_df)  # BLOGS table information in a pd.DataFrame object\n",
    "\n",
    "    df = news.news_df\n",
    "    vader = SentimentIntensityAnalyzer()\n",
    "    scores = df['Headline'].apply(vader.polarity_scores).tolist()\n",
    "    scores_df = pd.DataFrame(scores)\n",
    "    df = df.join(scores_df, rsuffix='_right')\n",
    "    df = df.iloc[::-1].reset_index(drop = True)\n",
    "    df[\"compound_TEMA\"] = ta.tema(df['compound'], 10)\n",
    "\n",
    "    if display == 'on':\n",
    "        plt.figure(figsize = (16, 9))\n",
    "        plt.title(f'{current_time()} Market Sentiment Analysis')\n",
    "        plt.errorbar(df.Time, df.compound, color = 'black', alpha = 0.3, linewidth = 3, label = 'Compound')\n",
    "        plt.errorbar(df.Time, df.compound_TEMA, color = 'black', alpha = 0.8, linewidth = 3, label = 'Compound TEMA')\n",
    "        plt.bar(df.Time, df.pos, color = 'blue', alpha = 0.3, label = 'Positive')\n",
    "        plt.bar(df.Time, -df.neg, color = 'red', alpha = 0.3, label = 'Negative')\n",
    "        plt.xticks(rotation = 90)\n",
    "        plt.xlabel('\\nNumber of Articles')\n",
    "        plt.ylabel('\\nSentiment')\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    # adjust the number of the most recent articles to refer\n",
    "    num_of_articles = int(len(df) * 0.5)\n",
    "    if num_of_articles < 10:\n",
    "        num_of_articles = 10\n",
    "    return_value = df[-num_of_articles:].compound_TEMA.mean() #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    if display == 'on':\n",
    "        print('\\n===========[Result]===========')\n",
    "        print(' • Sentiment Compound:  ' + '\\033[1m' + f'{np.round(return_value, 2)}' + '\\033[0m')\n",
    "        print(' • Positive Sign (avg): ' + f'{np.round(df[-num_of_articles:].pos.mean(), 2)}')\n",
    "        print(' • Negative Sign (avg): ' + f'{np.round(df[-num_of_articles:].neg.mean(), 2)}')\n",
    "        print(' • Total Articles:      ' + f'{len(df)}')\n",
    "        print(' • Target Articles:     ' + f'{num_of_articles}')    \n",
    "        print('==============================')\n",
    "        \n",
    "        if return_value < min_market_sentiment:\n",
    "            print('\\033[1m' + f'\\n\\n\\nMarket Sentiment is less than {min_market_sentiment}: {np.round(return_value, 2)}\\n\\n\\n' + '\\033[0m')\n",
    "        else:\n",
    "            print('\\033[1m' + f'\\n\\n\\nMarket Sentiment is higher than {min_market_sentiment}: {np.round(return_value, 2)}\\n\\n\\n' + '\\033[0m')\n",
    "        \n",
    "    return return_value\n",
    "\n",
    "\n",
    "def top_volume_portfolio(tickers):\n",
    "    print('\\033[1m' + f'\\n\\n\\n===============[ Top Volume Portfolio Analysis ]=====================' + '\\033[0m')\n",
    "\n",
    "    volume_portfolio = {}\n",
    "    ticker_list = []\n",
    "    volume_diff_list = []\n",
    "    for i in tqdm(range(len(tickers))):\n",
    "        try:\n",
    "            volume_diff = yfinance_df_rsi(tickers[i]).Volume.diff().values[-1]\n",
    "            print(f'Ticker: {tickers[i]} | {volume_diff}', end = '                            \\r')            \n",
    "            ticker_list.append(tickers[i])\n",
    "            volume_diff_list.append(volume_diff)\n",
    "\n",
    "        except:\n",
    "            print(f'Except >>> Volume Check Error - Ticker: {tickers[i]}')        \n",
    "\n",
    "    volume_portfolio['ticker'] = ticker_list\n",
    "    volume_portfolio['volume_diff'] = volume_diff_list        \n",
    "\n",
    "    print(end = '                                                                                   \\r')\n",
    "    print('\\033[1m' + f'=====================================================================' + '\\033[0m')\n",
    "\n",
    "    top_tickers = pd.DataFrame(volume_portfolio).sort_values('volume_diff', ascending = False).reset_index(drop = True).ticker[:10]\n",
    "    return top_tickers\n",
    "\n",
    "\n",
    "def buy_sign_day(ticker, lookback_days):\n",
    "    '''\n",
    "    input: ticker name\n",
    "    Volume - 80 percentile\n",
    "    Volume change by Price change - 95 percentile\n",
    "    lookback - 252 days (default)\n",
    "    output: True if sign-date exist within look back    \n",
    "    '''\n",
    "    try:\n",
    "        df = yfinance_df(ticker)[-252:]\n",
    "        df['Volume_Change'] = df['Volume'].diff()\n",
    "        df['Price_Change'] = df['Adj Close'].diff()\n",
    "        df['VC_per_PC'] = df['Volume_Change'] / df['Price_Change']\n",
    "        Up_Down = []\n",
    "        for i in df['Adj Close'].diff():\n",
    "            if i > 0:\n",
    "                Up_Down.append(1)\n",
    "            else:\n",
    "                Up_Down.append(0)\n",
    "        df['Up_Down'] = Up_Down\n",
    "        df.dropna(inplace = True)\n",
    "\n",
    "        filter_mask = (df.Volume > np.percentile(df.Volume, 80)) & (df.VC_per_PC > np.percentile(df.VC_per_PC, 95)) & (df.Up_Down == 0)\n",
    "        sign_days = df[filter_mask].sort_values('Date', ascending = False)\n",
    "        sign_date_volume = sign_days[:1].Date.values\n",
    "        sign_date_volume = str(sign_date_volume)[2:12]\n",
    "\n",
    "        day_plus = datetime.today() - timedelta(days = lookback_days) #<<< lookback\n",
    "        day_plus = str(day_plus)[:10]\n",
    "\n",
    "        return day_plus < sign_date_volume\n",
    "    \n",
    "    except:\n",
    "        print('Except >>> DataFrame is empty.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def trade_list(trade_type, stock):    \n",
    "    # remove previous file\n",
    "    \n",
    "    previous_exist = exists(f'{trade_type}_{mostrecentdate}.txt')\n",
    "    if previous_exist == True:\n",
    "        os.remove(f'{trade_type}_{mostrecentdate}.txt')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # create or update file\n",
    "    today_exist = exists(f'{trade_type}_{currentdate}.txt')\n",
    "    if today_exist == False:\n",
    "        with open(f'{trade_type}_{currentdate}.txt', 'w') as f:\n",
    "            f.write(str(stock))\n",
    "        voice_message(f\"\"\"\\\n",
    "                      The initial {trade_type} file has been created with {stock}\"\"\")\n",
    "        time.sleep(10)\n",
    "        \n",
    "    else:\n",
    "        with open(f'{trade_type}_{currentdate}.txt', 'r') as f:\n",
    "            trade_list = [line.rstrip('\\n') for line in f]\n",
    "        trade_list.append(stock)\n",
    "        trade_list = [i for n, i in enumerate(trade_list) if i not in trade_list[:n]] \n",
    "        with open(f'{trade_type}_{currentdate}.txt', 'w') as f:\n",
    "            for s in trade_list:\n",
    "                f.write(str(s) + '\\n')\n",
    "\n",
    "    # read and return\n",
    "    with open(f'{trade_type}_{currentdate}.txt', 'r') as f:\n",
    "        updated_trade_list = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    return updated_trade_list\n",
    "\n",
    "\n",
    "def portfolio_backtest():\n",
    "    # back test for the performance between 20 days and 5 days on average \n",
    "\n",
    "    with open(\"portfolio_original.txt\", 'r') as f:\n",
    "        portfolio = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    if len(portfolio) < 5 and timechecknow() < 100:\n",
    "        print('\\n\\nExcept >>> The number of portfolio is less than the minimum condition. Re-generating Portfolio.\\n\\n')\n",
    "        voice_message(\"\"\"\\\n",
    "            The number of portfolio is less than the minimum condition. Re-generating Portfolio.\"\"\")  \n",
    "        alarm()\n",
    "        main_creation()    \n",
    "        sectorcheck_main(1)\n",
    "        \n",
    "    back_test_perf = []\n",
    "    for ticker in portfolio:\n",
    "        back_test_perf.append(analysis_price(ticker)[5])\n",
    "\n",
    "    portfolio_backtest_result = np.round(np.average(back_test_perf), 2)\n",
    "    snp_backtest_result = np.round(analysis_price('^GSPC')[5], 2)\n",
    "    print(f'\\n\\n{current_time()}')\n",
    "    print('_____________________________________________\\n')\n",
    "    print('Back Test Result: (Avg.5 days vs Avg.20 days)\\n\\n• S&P500: ' + f'   {snp_backtest_result}%\\n')\n",
    "    print('• Portfolio: ' + '\\033[1m' + f'{portfolio_backtest_result}%' + '\\033[0m')\n",
    "    print('_____________________________________________\\n')\n",
    "    \n",
    "    if portfolio_backtest_result - snp_backtest_result < 0:\n",
    "        print('\\n\\nExcept >>> Back Test Failed: Portfolio performance is less than S&P500. Re-generating Portfolio.\\n\\n')\n",
    "        voice_message(\"\"\"\\\n",
    "            Back Test Failed. Re-generating Portfolio.\"\"\")  \n",
    "        alarm()\n",
    "        main_creation()   \n",
    "        sectorcheck_main(1)\n",
    "        \n",
    "        \n",
    "def proba_value_reset():   \n",
    "    global predict_proba_value\n",
    "    global modelchoice_minimum_fbeta\n",
    "    \n",
    "    # proba value adjusting by market trend\n",
    "    predict_proba_value = proba_initial_value \n",
    "    modelchoice_minimum_fbeta = initial_modelchoice_minimum_fbeta\n",
    "    market_trend_index = (sentiment_analysis_cal(mega_tickers)*0.1) + (sum(valuecheck('^GSPC'))*0.01)\n",
    "\n",
    "    if market_trend_index > 0.02:\n",
    "        market_trend_index = 0.02\n",
    "    if market_trend_index < -0.02:\n",
    "        market_trend_index = -0.02\n",
    "    \n",
    "    predict_proba_value = predict_proba_value - market_trend_index\n",
    "    \n",
    "    # proba value adjusting by real performance\n",
    "    snp_price_analysis = analysis_price('^GSPC')\n",
    "    snp_price_analysis_trend = snp_price_analysis[1] + snp_price_analysis[7] + snp_price_analysis[10] + snp_price_analysis[11] + snp_price_analysis[12] \n",
    "    dmv_plot('^GSPC')\n",
    "    \n",
    "    if snp_price_analysis_trend == 0:\n",
    "        predict_proba_value = predict_proba_value + 0.1\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta + 0.1\n",
    "        \n",
    "    elif snp_price_analysis_trend == 1:\n",
    "        predict_proba_value = predict_proba_value + 0.05\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta + 0.05\n",
    "        \n",
    "    elif snp_price_analysis_trend == 2:\n",
    "        predict_proba_value = predict_proba_value + 0.01\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta + 0.01\n",
    "        \n",
    "    elif snp_price_analysis_trend == 3:\n",
    "        predict_proba_value = predict_proba_value + 0.005\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta + 0.005\n",
    "        \n",
    "    elif snp_price_analysis_trend == 4:\n",
    "        predict_proba_value = predict_proba_value + 0.0025\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta + 0.0025\n",
    "        \n",
    "    elif snp_price_analysis_trend == 5:\n",
    "        predict_proba_value = predict_proba_value\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta\n",
    "        \n",
    "    else:\n",
    "        alarm()\n",
    "        print('\\n\\n\\nExcept >>> snp_price_analysis_trend has incorrect value.\\n\\n\\n')\n",
    "        \n",
    "    print('\\n\\n\\n===> Min. number of positive sets at ' + '\\033[1m' + f'{min_num_positive_models} <===' + '\\033[0m')\n",
    "    \n",
    "    # apply min/max thresholds: predict_proba_value\n",
    "    print(f'\\n\\n\\n• predict_proba_value before adjusting: {np.round(predict_proba_value, 2)}')\n",
    "    if predict_proba_value > predict_proba_value_max:\n",
    "        predict_proba_value = predict_proba_value_max\n",
    "        print(f'• predict_proba_value exceeds the max limit. Reset at {predict_proba_value_max}.')\n",
    "        \n",
    "    if predict_proba_value < predict_proba_value_min:\n",
    "        predict_proba_value = predict_proba_value_min\n",
    "        print(f'• predict_proba_value less than the min limit. Reset at {predict_proba_value_min}.')\n",
    "        \n",
    "    print(f'• predict_proba_value after adjusting: {np.round(predict_proba_value, 2)}\\n\\n\\n')   \n",
    "    \n",
    "    \n",
    "    # apply min/max thresholds: modelchoice_minimum_fbeta\n",
    "    print(f'• modelchoice_minimum_fbeta before adjusting: {np.round(modelchoice_minimum_fbeta, 2)}')\n",
    "    if modelchoice_minimum_fbeta > modelchoice_minimum_fbeta_max:\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta_max\n",
    "        print(f'• modelchoice_minimum_fbeta exceeds the max limit. Reset at {modelchoice_minimum_fbeta_max}.')\n",
    "        \n",
    "    if modelchoice_minimum_fbeta < modelchoice_minimum_fbeta_min:\n",
    "        modelchoice_minimum_fbeta = modelchoice_minimum_fbeta_min\n",
    "        print(f'• modelchoice_minimum_fbeta less than the min limit. Reset at {modelchoice_minimum_fbeta_min}.')\n",
    "        \n",
    "    print(f'• modelchoice_minimum_fbeta after adjusting: {np.round(modelchoice_minimum_fbeta, 2)}\\n\\n\\n')   \n",
    "\n",
    "  \n",
    "    # announce the final change  \n",
    "    message_proba = f'Proba value sets at {int(predict_proba_value*100)} %'\n",
    "    message_fbeta = f'Minimum Model Choice fbeta sets at {int(modelchoice_minimum_fbeta*100)} %'\n",
    "    if timechecknow() < 60:\n",
    "        voice_message(message_proba)\n",
    "        time.sleep(10)\n",
    "        voice_message(message_fbeta)\n",
    "        time.sleep(10)\n",
    "    else:\n",
    "        print(\"===> Proba value sets at \" + '\\033[1m' + f\"{np.round(predict_proba_value, 2)} <===\\n\\n\\n\" + '\\033[0m')\n",
    "        print(\"===> Minimum Model Choice fbeta sets at \" + '\\033[1m' + f\"{np.round(modelchoice_minimum_fbeta, 2)} <===\\n\\n\\n\" + '\\033[0m')\n",
    "        \n",
    "\n",
    "print('> Modules #2 imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2f5da783",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################################################\n",
    "# min. holding period before loss-cut executed\n",
    "min_holding_period = 20  # days\n",
    "\n",
    "# Strategy settings\n",
    "strategy_days = 1200\n",
    "days_plot = 200\n",
    "\n",
    "min_RealizedCount = 10\n",
    "\n",
    "parameter_search_times = 50    \n",
    "\n",
    "# Strategy verification\n",
    "min_AvgRealizedGain = 0                # more than\n",
    "min_Worstperf = -10                    # more than\n",
    "max_HoldingDays = min_holding_period   # less than or equal\n",
    "max_PerfRange = 30                     # less than \n",
    "min_SuccessRate = 70                   # higher than\n",
    "min_SuccessRate_original = min_SuccessRate\n",
    "min_SuccessRate_fixed = 55             # higher than\n",
    "##########################################################\n",
    "\n",
    "# parameter random search range\n",
    "##########################################################\n",
    "freqLine_list = np.arange(8, 11, 1) \n",
    "freqLine_gap_list = np.arange(1.00, 1.06, 0.01) \n",
    "evma_sum_list = np.arange(3, 8, 1) \n",
    "macd_gap_list = np.arange(0.02, 0.07, 0.01)\n",
    "realize_gain_percent_list = np.arange(0.01, 0.04, 0.005)     \n",
    "realize_loss_percent_list = np.arange(-0.06, -0.03, 0.01) \n",
    "##########################################################\n",
    "\n",
    "\n",
    "def stg_volatility_check(df):\n",
    "    price_change = []\n",
    "    for i in range(df.shape[0]):\n",
    "        if i == 0:\n",
    "            price_change.append(0)\n",
    "        else:\n",
    "            price_change.append((df['Current Price'].values[i] - df['Current Price'].values[i-1]) / df['Current Price'].values[i-1])\n",
    "    df['price_change'] = price_change   \n",
    "    return np.round(np.std(df['price_change']) * np.sqrt(252), 2)\n",
    "\n",
    "\n",
    "def twin_plot(x1_axis, y1_axis, x1_color, x1_label, y1_label, y1_label_color,\n",
    "              x2_axis, y2_axis, x2_color, y2_label, y2_label_color,\n",
    "              title_name, \n",
    "              line1_x, line1_y, line1_color, style1,\n",
    "              line2_x, line2_y, line2_color, style2):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 9)\n",
    "    ax.plot(x1_axis, y1_axis, color = x1_color)\n",
    "    ax.set_xlabel(x1_label)\n",
    "    ax.set_ylabel(y1_label, color = y1_label_color)\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(x2_axis, y2_axis, color = x2_color)\n",
    "    ax1.set_ylabel(y2_label, color = y2_label_color)\n",
    "    ax.grid(axis = 'x')\n",
    "    ax1.grid(axis = 'y')\n",
    "    plt.title(title_name)\n",
    "    ax1.plot(line1_x, line1_y, color = line1_color, linestyle = style1)\n",
    "    ax1.plot(line2_x, line2_y, color = line2_color, linestyle = style2)\n",
    "\n",
    "    \n",
    "def twin_bar(x1_axis, y1_axis, x1_color, x1_label, y1_label, y1_label_color,\n",
    "              x2_axis, y2_axis, x2_color, y2_label, y2_label_color,\n",
    "              title_name, \n",
    "              line1_x, line1_y, line1_color, style1,\n",
    "              line2_x, line2_y, line2_color, style2):\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 9)\n",
    "    ax.plot(x1_axis, y1_axis, color = x1_color)\n",
    "    ax.set_xlabel(x1_label)\n",
    "    ax.set_ylabel(y1_label, color = y1_label_color)\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.bar(x2_axis, y2_axis, color = x2_color, width = 20, alpha = 0.2)\n",
    "    ax1.set_ylabel(y2_label, color = y2_label_color)\n",
    "    ax.grid(axis = 'x')\n",
    "    ax1.grid(axis = 'y')\n",
    "    plt.title(title_name)\n",
    "    ax.plot(line1_x, line1_y, color = line1_color, linestyle = style1)\n",
    "    ax.plot(line2_x, line2_y, color = line2_color, linestyle = style2)\n",
    "    \n",
    "\n",
    "def strategy_main_plot(ticker, df_plot, freq_mid_line_1):\n",
    "    # Main plot\n",
    "    for i in range(1,4):\n",
    "        df_plot = df_plot[-int(len(df_plot)/i):]\n",
    "        plt.figure(figsize = (16, 9))\n",
    "        plt.title(f'Exponential Moving Average {(i)} | {ticker}')\n",
    "        plt.grid()\n",
    "        errorbar_columns = ['Current Price', freq_mid_line_1, '60_EMA', '120_EMA']\n",
    "        for column in errorbar_columns:\n",
    "            plt.errorbar(data = df_plot, x = 'Date', y = column, linewidth = 1.5, label = column)\n",
    "            plt.annotate(xy = [df_plot[-1:].Date.values, df_plot[-1:][column].values], text = column)\n",
    "        \n",
    "        annotate = df_plot[df_plot['Long_Sign'] >= 1][['Date', 'Current Price']].reset_index(drop = True)\n",
    "        for i in range(len(annotate)):\n",
    "            plt.annotate(xy = [annotate.Date.values[i], annotate['Current Price'].values[i]], text = 'LT', color = 'Blue', alpha=0.8)\n",
    "        \n",
    "        annotate = df_plot[df_plot['Short_Sign'] >= 1][['Date', 'Current Price']].reset_index(drop = True)\n",
    "        for i in range(len(annotate)):\n",
    "            plt.annotate(xy = [annotate.Date.values[i], annotate['Current Price'].values[i]], text = 'ST', color = 'Red', alpha=0.8)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def ichimoku_chart(ticker, df_plot):\n",
    "    for i in range(1,4):\n",
    "        # Ichimoku\n",
    "        df_plot = df_plot[-int(len(df_plot)/i):]\n",
    "        plt.figure(figsize = (16,9))\n",
    "        plt.title(f'Ichimoku Chart {(i)} | {ticker}')\n",
    "        plt.errorbar(df_plot.Date, df_plot['Current Price'], label = 'Current Price', linewidth = 18, color = 'black', alpha = 0.05)\n",
    "        plt.errorbar(df_plot.Date, df_plot.AdvSpan_1, label = 'AdvSpan #1', linewidth = 5, color = 'red', alpha = 0.6)\n",
    "        plt.errorbar(df_plot.Date, df_plot.AdvSpan_2, label = 'AdvSpan #2', linewidth = 5, color = 'green', alpha = 0.4)\n",
    "        plt.errorbar(df_plot.Date, df_plot.Conversion, label = 'Conversion', linewidth = 2, color = 'black', alpha = 1)\n",
    "        plt.errorbar(df_plot.Date, df_plot.Standard, label = 'Standard', linewidth = 2, color = 'blue', alpha = 1)\n",
    "        plt.errorbar(df_plot.Date, df_plot.FlwSpan, label = 'FlwSpan', linewidth = 2, color = 'black', alpha = 0.4)\n",
    "        ichimoku_columns = ['Current Price', 'AdvSpan_1', 'AdvSpan_2','Conversion','Standard','FlwSpan']\n",
    "        for column in ichimoku_columns:\n",
    "            plt.annotate(xy = [df_plot[-1:].Date.values, df_plot[-1:][column].values], text = column)\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "def bollinger_plot(ticker, df_plot, squeeze_ceiling_buy, squeeze_floor_sell, loose_ceiling_sell, loose_floor_buy):\n",
    "    # bollinger band\n",
    "    for i in range(1,4):\n",
    "        # bollinger band\n",
    "        df_plot = df_plot[-int(len(df_plot)/i):]\n",
    "        plt.figure(figsize = (16, 9))\n",
    "        plt.title(f'Bollinger Band {(i)} | {ticker}')\n",
    "        plt.errorbar(df_plot.Date, df_plot['High'], label = 'High', color = 'green', alpha = 0.3)\n",
    "        plt.errorbar(df_plot.Date, df_plot['Current Price'], label = 'Current Price', color = 'blue', alpha = 0.3, linewidth = 5)\n",
    "        plt.errorbar(df_plot.Date, df_plot['Low'], label = 'Low', color = 'green', alpha = 0.3)\n",
    "        plt.errorbar(df_plot.Date, df_plot['BBU_20_2.0'], label = 'BB_Upper', color = 'black', alpha = 0.5)\n",
    "        # plt.errorbar(df_plot.Date, df_plot['BBM_20_2.0'], label = '5_SMA', alpha = 0.3)\n",
    "        plt.errorbar(df_plot.Date, df_plot['BBL_20_2.0'], label = 'BB_Low', color = 'black', alpha = 0.5)\n",
    "        plt.errorbar(df_plot.Date, df_plot['200_EMA'], label = '200 EMA', color = 'blue', alpha = 0.6)\n",
    "        plt.xticks(rotation = 90)\n",
    "        squeeze_ceiling_buy.reset_index(drop = True, inplace = True)\n",
    "        for i in range(len(squeeze_ceiling_buy)):\n",
    "            try:\n",
    "                plt.annotate(xy = [squeeze_ceiling_buy.Date.values[i], squeeze_ceiling_buy[squeeze_ceiling_buy['Date'] == squeeze_ceiling_buy.Date.values[i]]['BBU_20_2.0'].values[0]], text = 'Sqz Buy', color = 'black', fontsize = 12)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        squeeze_floor_sell.reset_index(drop = True, inplace = True)\n",
    "        for i in range(len(squeeze_floor_sell)):    \n",
    "            try:\n",
    "                plt.annotate(xy = [squeeze_floor_sell.Date.values[i], squeeze_floor_sell[squeeze_floor_sell['Date'] == squeeze_floor_sell.Date.values[i]]['BBL_20_2.0'].values[0]], text = 'Sqz Sell', color = 'red', fontsize = 12)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        loose_ceiling_sell.reset_index(drop = True, inplace = True)\n",
    "        for i in range(len(loose_ceiling_sell)):\n",
    "            try:\n",
    "                plt.annotate(xy = [loose_ceiling_sell.Date.values[i], loose_ceiling_sell[loose_ceiling_sell['Date'] == loose_ceiling_sell.Date.values[i]]['BBU_20_2.0'].values[0]], text = 'Loo Sell', color = 'red', fontsize = 12)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        loose_floor_buy.reset_index(drop = True, inplace = True)\n",
    "        for i in range(len(loose_floor_buy)):    \n",
    "            try:\n",
    "                plt.annotate(xy = [loose_floor_buy.Date.values[i], loose_floor_buy[loose_floor_buy['Date'] == loose_floor_buy.Date.values[i]]['BBL_20_2.0'].values[0]], text = 'Loo Buy', color = 'black', fontsize = 12)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        print(f'\\n• Squeeze Ceiling Buy: {len(squeeze_ceiling_buy)}')\n",
    "        print(f'• Squeeze Floor Sell:  {len(squeeze_floor_sell)}\\n\\n')\n",
    "    \n",
    "    \n",
    "def bollinger_sign_gen(ticker):\n",
    "    '''\n",
    "    input: ticker name\n",
    "    output: bollinger sign - 20 days moving average with std. at 5\n",
    "    '''\n",
    "    print('\\n\\n\\n===============[ Bollinger Sign Analysis: ' + '\\033[1m' + f'{ticker}' + '\\033[0m' + ' ]=====================')\n",
    "    global min_SuccessRate\n",
    "    min_SuccessRate_original = min_SuccessRate\n",
    "    min_SuccessRate = int(min_SuccessRate * 0.5)\n",
    "        \n",
    "    if strategy_analysis_main(ticker) != -10:\n",
    "        df = pd.read_csv(f'strategy_anaysis_{ticker }_{currentdate}.csv')\n",
    "        df['bollinger_sum'] = df['squeeze_ceiling_buy'] + df['loose_floor_buy'] + df['squeeze_floor_sell'] + df['loose_ceiling_sell']\n",
    "        df = df[df['bollinger_sum'] != 0][-1:]\n",
    "\n",
    "        squeeze_ceiling_buy = df.squeeze_ceiling_buy.values[0]\n",
    "        loose_floor_buy = df.loose_floor_buy.values[0]\n",
    "        buy_sign = squeeze_ceiling_buy + loose_floor_buy\n",
    "\n",
    "        squeeze_floor_sell = df.squeeze_floor_sell.values[0]\n",
    "        loose_ceiling_sell = df.loose_ceiling_sell.values[0]\n",
    "        sell_sign = squeeze_floor_sell + loose_ceiling_sell\n",
    "\n",
    "        if sell_sign > 0:\n",
    "            print('Bollinger - ' + '\\033[1m' + 'Sell' + '\\033[0m')\n",
    "        elif buy_sign > 0:\n",
    "            print('Bollinger - ' + '\\033[1m' +'Buy' + '\\033[0m')\n",
    "        else:\n",
    "            print('Except >>> Bollinger Sum Error')\n",
    "        min_SuccessRate = min_SuccessRate_original\n",
    "        print('\\n\\n====================================================================')\n",
    "        return [buy_sign, sell_sign]\n",
    "    else:\n",
    "        print('Except >>> Strategy Analysis Failed')\n",
    "        return [-10, -10]\n",
    "\n",
    "    \n",
    "def BBand_Buy_Sign_Count(tickers):\n",
    "    passed = []\n",
    "    c = 1\n",
    "    print('\\n')\n",
    "    for ticker in tickers:\n",
    "        print(f'Bollinger Sign Checking: {c} / {len(tickers)}', end = '                      \\r')\n",
    "        if bollinger_sign_gen(ticker)[0] == 1:\n",
    "            passed.append(ticker)\n",
    "        c+=1\n",
    "    success_rate = len(passed) / len(tickers)\n",
    "    print('\\n\\nSuccess Rate: ' + '\\033[1m' + f'{np.round(success_rate*100, 2)} %' + '\\033[0m')\n",
    "    return success_rate   \n",
    "\n",
    "\n",
    "def macro_strategy(ticker, \n",
    "                   period, \n",
    "                   opt_freqLine, \n",
    "                   opt_freqLine_gap,\n",
    "                   opt_evma_sum,\n",
    "                   opt_macd_gap,\n",
    "                   realize_gain_percent, \n",
    "                   realize_loss_percent,\n",
    "                   plot_display_1, \n",
    "                   plot_display_2):\n",
    "    \n",
    "    global SuccessRate\n",
    "    global timesRealized\n",
    "    # https://github.com/twopirllc/pandas-ta\n",
    "    \n",
    "    df = yfinance_df(ticker)[-period:]\n",
    "    df = df.rename(columns = {'Adj Close':'Current Price'})\n",
    "    df['Current Price'] = (df['Current Price'] + df['High'] + df['Low'])/3\n",
    "    \n",
    "    ## 14_Day RSI\n",
    "    df['RSI'] = ta.rsi(df['Current Price'], length = 14)\n",
    "    focus_days = [40, 70]\n",
    "    for i in focus_days:\n",
    "        df[f\"{i}_RSI\"] = df['RSI'].ewm(span= i, adjust=False).mean()\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    ## 14_Day ADX\n",
    "    df = df.join(ta.adx(df.High, df.Low, df.Close, length = 14))\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    # ema\n",
    "    focus_days = [opt_freqLine, opt_freqLine+2, opt_freqLine+4, opt_freqLine+6, opt_freqLine+8, 12, 27, 60, 120, 200]\n",
    "    for i in focus_days:\n",
    "        df[f\"{i}_EMA\"] = df['Current Price'].ewm(span= i, adjust=False).mean()\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    # find the s/t ema line having most crossing with the current price\n",
    "    sc = StandardScaler()\n",
    "    freq_mid_line = {}\n",
    "    for column_num in range(13, 18):\n",
    "        close_sc = sc.fit_transform(np.array(df['Current Price']).reshape(-1, 1))\n",
    "        compare_sc = sc.transform(np.array(df[df.columns[column_num]]).reshape(-1, 1))\n",
    "        df[f'vs_{df.columns[column_num]}'] = np.absolute(close_sc - compare_sc)\n",
    "        freq_mid_line[f'vs_{df.columns[column_num]}'] = len(df[df[f'vs_{df.columns[column_num]}'] < 0.1])     \n",
    "    freq_mid_line_1 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-1][0][3:]\n",
    "    freq_mid_line_2 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-2][0][3:]\n",
    "    freq_mid_line_3 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-3][0][3:]\n",
    "    \n",
    "    # Cross_date: when the current price is located as 'freq_mid_line' > 'Current Price' > '60_EMA'    \n",
    "    df['Current_freq_1'] = df['Current Price'] - df[freq_mid_line_1]\n",
    "    df['Current_freq_2'] = df['Current Price'] - df[freq_mid_line_2]\n",
    "    df['Current_freq_3'] = df['Current Price'] - df[freq_mid_line_3]\n",
    "    df['Current_60_EMA'] = df['Current Price'] - df['60_EMA']\n",
    "    \n",
    "    df.loc[(df['Current_freq_1'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_1'] = 1\n",
    "    df['Cross_zone_1'] = df['Cross_zone_1'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['Current_freq_2'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_2'] = 1\n",
    "    df['Cross_zone_2'] = df['Cross_zone_2'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['Current_freq_3'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_3'] = 1\n",
    "    df['Cross_zone_3'] = df['Cross_zone_3'].fillna(0)\n",
    "    \n",
    "# strategy 1: EMA lines should be arranged as One of Short-Terms > Mid-Term > Long-Term\n",
    "    ###############################################################################    \n",
    "    # EMA Plus Condition  \n",
    "    df.loc[df['Current Price'] >= (df[freq_mid_line_1] * opt_freqLine_gap), 'EMA_Plus_1'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df.loc[df['Current Price'] >= (df[freq_mid_line_2] * opt_freqLine_gap), 'EMA_Plus_2'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df.loc[df['Current Price'] >= (df[freq_mid_line_3] * opt_freqLine_gap), 'EMA_Plus_3'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    ### modify ###\n",
    "    df['EMA_Plus_Sum'] = df['EMA_Plus_1'] + df['EMA_Plus_2'] + df['EMA_Plus_3']  \n",
    "    df.drop(columns = {'EMA_Plus_1', 'EMA_Plus_2', 'EMA_Plus_3'}, inplace = True)\n",
    "    \n",
    "        \n",
    "    # EMA Minus Condition    \n",
    "    df.loc[(df['Current Price'] * opt_freqLine_gap) < df[freq_mid_line_1], 'EMA_Minus'] = -1\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    ### modify ###\n",
    "    df['EMA_Minus_Sum'] = df['EMA_Minus'] \n",
    "    df.drop(columns = {'EMA_Minus'}, inplace = True)    \n",
    "    ###############################################################################\n",
    "    \n",
    "    # tema\n",
    "    focus_days = [opt_freqLine, opt_freqLine+2, opt_freqLine+4, opt_freqLine+6, opt_freqLine+8]\n",
    "    for i in focus_days:\n",
    "        df[f\"{i}_TEMA\"] = ta.tema(df['Current Price'], i)\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    # find the s/t ema line having most crossing with the current price\n",
    "    freq_mid_line = {}\n",
    "    for column_num in range(36, 41):\n",
    "        close_sc = sc.fit_transform(np.array(df['Current Price']).reshape(-1, 1))\n",
    "        compare_sc = sc.transform(np.array(df[df.columns[column_num]]).reshape(-1, 1))\n",
    "        df[f'vs_{df.columns[column_num]}'] = np.absolute(close_sc - compare_sc)\n",
    "        freq_mid_line[f'vs_{df.columns[column_num]}'] = len(df[df[f'vs_{df.columns[column_num]}'] < 0.1])     \n",
    "    freq_mid_line_11 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-1][0][3:]\n",
    "    freq_mid_line_22 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-2][0][3:]\n",
    "    freq_mid_line_33 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-3][0][3:]\n",
    "    \n",
    "    # Cross_date: when the current price is located as 'freq_mid_line' > 'Current Price' > '60_EMA'    \n",
    "    df['Current_freq_11'] = df['Current Price'] - df[freq_mid_line_11]\n",
    "    df['Current_freq_22'] = df['Current Price'] - df[freq_mid_line_22]\n",
    "    df['Current_freq_33'] = df['Current Price'] - df[freq_mid_line_33]\n",
    "    \n",
    "    df.loc[(df['Current_freq_11'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_11'] = 1\n",
    "    df['Cross_zone_11'] = df['Cross_zone_11'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['Current_freq_22'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_22'] = 1\n",
    "    df['Cross_zone_22'] = df['Cross_zone_22'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['Current_freq_33'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_33'] = 1\n",
    "    df['Cross_zone_33'] = df['Cross_zone_33'].fillna(0)\n",
    "    \n",
    "# strategy 2: TEMA lines should be arranged as One of Short-Terms > Mid-Term > Long-Term\n",
    "    ###############################################################################    \n",
    "    # TEMA Plus Condition  \n",
    "    df.loc[df['Current Price'] >= (df[freq_mid_line_11] * opt_freqLine_gap), 'TEMA_Plus_1'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df.loc[df['Current Price'] >= (df[freq_mid_line_22] * opt_freqLine_gap), 'TEMA_Plus_2'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df.loc[df['Current Price'] >= (df[freq_mid_line_33] * opt_freqLine_gap), 'TEMA_Plus_3'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    ### modify ###\n",
    "    df['TEMA_Plus_Sum'] = df['TEMA_Plus_1'] + df['TEMA_Plus_2'] + df['TEMA_Plus_3']  \n",
    "    df.drop(columns = {'TEMA_Plus_1', 'TEMA_Plus_2', 'TEMA_Plus_3'}, inplace = True)\n",
    "    \n",
    "        \n",
    "    # TEMA Minus Condition    \n",
    "    df.loc[(df['Current Price'] * opt_freqLine_gap) < df[freq_mid_line_11], 'TEMA_Minus'] = -1\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    ### modify ###\n",
    "    df['TEMA_Minus_Sum'] = df['TEMA_Minus'] \n",
    "    df.drop(columns = {'TEMA_Minus'}, inplace = True)    \n",
    "    ############################################################################### \n",
    "    \n",
    "    # vwap_ema\n",
    "    df['VWAP'] = (df['Volume'] * df['Current Price']) / df['Volume']\n",
    "    focus_days = [opt_freqLine, opt_freqLine+2, opt_freqLine+4, opt_freqLine+6, opt_freqLine+8]\n",
    "    for i in focus_days:\n",
    "        df[f\"{i}_VWAP_EMA\"] = ta.ema(df['VWAP'], i)\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    # find the s/t ema line having most crossing with the current price\n",
    "    freq_mid_line = {}\n",
    "    for column_num in range(56, 60):\n",
    "        close_sc = sc.fit_transform(np.array(df['VWAP']).reshape(-1, 1))\n",
    "        compare_sc = sc.transform(np.array(df[df.columns[column_num]]).reshape(-1, 1))\n",
    "        df[f'vs_{df.columns[column_num]}'] = np.absolute(close_sc - compare_sc)\n",
    "        freq_mid_line[f'vs_{df.columns[column_num]}'] = len(df[df[f'vs_{df.columns[column_num]}'] < 0.1])     \n",
    "    freq_mid_line_111 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-1][0][3:]\n",
    "    freq_mid_line_222 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-2][0][3:]\n",
    "    freq_mid_line_333 = sorted(freq_mid_line.items(), key=lambda x:x[1])[-3][0][3:]\n",
    "    \n",
    "    # Cross_date: when the current price is located as 'freq_mid_line' > 'Current Price' > '60_EMA'    \n",
    "    df['Current_freq_111'] = df['VWAP'] - df[freq_mid_line_111]\n",
    "    df['Current_freq_222'] = df['VWAP'] - df[freq_mid_line_222]\n",
    "    df['Current_freq_333'] = df['VWAP'] - df[freq_mid_line_333]\n",
    "    \n",
    "    df.loc[(df['Current_freq_111'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_111'] = 1\n",
    "    df['Cross_zone_111'] = df['Cross_zone_111'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['Current_freq_222'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_222'] = 1\n",
    "    df['Cross_zone_222'] = df['Cross_zone_222'].fillna(0)\n",
    "    \n",
    "    df.loc[(df['Current_freq_333'] < 0) & (df['Current_60_EMA'] > 0), 'Cross_zone_333'] = 1\n",
    "    df['Cross_zone_333'] = df['Cross_zone_333'].fillna(0)\n",
    "    \n",
    "# strategy 2: TEMA lines should be arranged as One of Short-Terms > Mid-Term > Long-Term\n",
    "    ###############################################################################    \n",
    "    # TEMA Plus Condition  \n",
    "    df.loc[df['VWAP'] >= (df[freq_mid_line_111] * opt_freqLine_gap), 'VWAP_EMA_Plus_1'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df.loc[df['VWAP'] >= (df[freq_mid_line_222] * opt_freqLine_gap), 'VWAP_EMA_Plus_2'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    df.loc[df['VWAP'] >= (df[freq_mid_line_333] * opt_freqLine_gap), 'VWAP_EMA_Plus_3'] = 1 \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    ### modify ###\n",
    "    df['VWAP_EMA_Plus_Sum'] = df['VWAP_EMA_Plus_1'] + df['VWAP_EMA_Plus_2'] + df['VWAP_EMA_Plus_3']  \n",
    "    df.drop(columns = {'VWAP_EMA_Plus_1', 'VWAP_EMA_Plus_2', 'VWAP_EMA_Plus_3'}, inplace = True)\n",
    "    \n",
    "        \n",
    "    # TEMA Minus Condition    \n",
    "    df.loc[(df['VWAP'] * opt_freqLine_gap) < df[freq_mid_line_111], 'VWAP_EMA_Minus'] = -1\n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    ### modify ###\n",
    "    df['VWAP_EMA_Minus_Sum'] = df['VWAP_EMA_Minus'] \n",
    "    df.drop(columns = {'VWAP_EMA_Minus'}, inplace = True)    \n",
    "    ############################################################################### \n",
    "    \n",
    "## strategy 4: EMA Volume diff. should be positive \n",
    "    ###############################################################################\n",
    "    evma_focus_days = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    for i in evma_focus_days:\n",
    "        df[f\"{i}_EVMA\"] = df['Volume'].ewm(span= i, adjust=False).mean()\n",
    "        df[f\"{i}_EVMA_diff\"] = df[f\"{i}_EVMA\"].diff() > 0\n",
    "        df[f\"{i}_EVMA_diff\"] = df[f\"{i}_EVMA_diff\"].map({True:1, False:0})\n",
    "    df.dropna(inplace = True)    \n",
    "   \n",
    "    ### modify ###     \n",
    "    df['EVMA_SUM'] = df['1_EVMA_diff'] + df['2_EVMA_diff'] + df['3_EVMA_diff'] + df['4_EVMA_diff'] + df['5_EVMA_diff'] + df['6_EVMA_diff'] + df['7_EVMA_diff'] + df['8_EVMA_diff'] + df['9_EVMA_diff'] + df['10_EVMA_diff'] #<<<\n",
    "    df.drop(columns = {'1_EVMA_diff', '2_EVMA_diff', '3_EVMA_diff', '4_EVMA_diff', '5_EVMA_diff', '6_EVMA_diff', '7_EVMA_diff', '8_EVMA_diff', '9_EVMA_diff', '10_EVMA_diff'}, inplace = True) #<<<\n",
    "    \n",
    "    df.fillna(0, inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    ###############################################################################\n",
    "    \n",
    "    # MACD line is calculated by subtracting the 26-period EMA from the 12-period EMA\n",
    "    # A nine-day EMA of the MACD line is called the signal line, which is then plotted on top of the MACD line, \n",
    "    # which can function as a trigger for buy or sell signals. \n",
    "    # Traders may buy the security when the MACD line crosses above the signal line \n",
    "    df['MACD'] = df['12_EMA'] - df['27_EMA']    \n",
    "    df['SignalLine'] = df['MACD'].ewm(span= 9, adjust=False).mean()\n",
    "    \n",
    "## strategy 5 (0~1): MACD line should crosses above the signal line\n",
    "    df['MACD_SignalLine'] = df['MACD'] - df['SignalLine']\n",
    "    df['SignalLine_Gap'] = df['SignalLine'] * opt_macd_gap\n",
    "     \n",
    "    df.loc[df['MACD'] >= df['SignalLine_Gap'], 'MACD_Sign'] = 1\n",
    "    df.loc[df['MACD'] < df['SignalLine_Gap'], 'MACD_Sign'] = 0   \n",
    "    \n",
    "## strategy 6 bollinger band\n",
    "    df = df.join(ta.bbands(df.Close, length=20, std = 2))\n",
    "    df.dropna(inplace=True)\n",
    "    sc = StandardScaler()\n",
    "    \n",
    "    close_sc = sc.fit_transform(np.array(df['Current Price']).reshape(-1, 1))\n",
    "    high_sc = sc.transform(np.array(df['High']).reshape(-1, 1))\n",
    "    low_sc = sc.transform(np.array(df['Low']).reshape(-1, 1))\n",
    "    BBU_sc = sc.transform(np.array(df['BBU_20_2.0']).reshape(-1, 1))\n",
    "    BBL_sc = sc.transform(np.array(df['BBL_20_2.0']).reshape(-1, 1))\n",
    "\n",
    "    df['BB_Gap'] = BBU_sc - BBL_sc\n",
    "    df['price_diff_sc'] = np.absolute(df['Current Price'].diff())\n",
    "    df['bb_gap_diff_sc'] = np.absolute(df['BB_Gap'].diff())\n",
    "    \n",
    "    ###################################################\n",
    "    bb_gap_threshold = df['bb_gap_diff_sc'].max()/2\n",
    "    ###################################################\n",
    "    \n",
    "    df['Ceiling'] = high_sc - BBU_sc  \n",
    "    df['Floor'] = low_sc - BBL_sc     \n",
    "    \n",
    "    df.loc[(df['Ceiling'] > 0) & \n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold), \n",
    "           'squeeze_ceiling_buy'] = 1   \n",
    "    df.loc[(df['Floor'] < 0) & \n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold), \n",
    "           'squeeze_floor_sell'] = 1  \n",
    "    df.loc[(df['Ceiling'] > 0) & \n",
    "           (df['bb_gap_diff_sc'] > bb_gap_threshold), \n",
    "           'loose_ceiling_sell'] = 1  \n",
    "    df.loc[(df['Floor'] < 0) & \n",
    "           (df['bb_gap_diff_sc'] > bb_gap_threshold), \n",
    "           'loose_floor_buy'] = 1  \n",
    "    df.fillna(0, inplace = True)\n",
    "    df.dropna(inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    squeeze_ceiling_buy = df[df['squeeze_ceiling_buy'] == 1]  # buy sign\n",
    "    squeeze_floor_sell = df[df['squeeze_floor_sell'] == 1]    # sell sign\n",
    "    loose_ceiling_sell = df[df['loose_ceiling_sell'] == 1]    # sell sign\n",
    "    loose_floor_buy = df[df['loose_floor_buy'] == 1]          # buy sign\n",
    "\n",
    "## strategy 7 ichimoku  \n",
    "    df = df.join(ta.ichimoku(df.High, df.Low, df.Close)[0])\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    df = df.rename(columns = {'ISA_9':'AdvSpan_1', 'ISB_26':'AdvSpan_2', 'ITS_9':'Conversion', 'IKS_26':'Standard', 'ICS_26':'FlwSpan'})\n",
    "    df = df.fillna(method = 'ffill')\n",
    "    \n",
    "## strategy 8 CCI      \n",
    "    df = df.join(ta.cci(df.High, df.Low, df.Close))\n",
    "    df = df.rename(columns = {'CCI_14_0.015':'CCI'}) \n",
    "\n",
    "    #\n",
    "    # Create new columns from here '^'   \n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop = True)\n",
    " \n",
    "## strategy creation \n",
    "    # Long ##########################################################################################################################################################\n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['EMA_Plus_Sum'] >= 1) &\n",
    "           (df['EVMA_SUM'] >= opt_evma_sum) &\n",
    "           (df['RSI'] < 70),\n",
    "           'Long_Sign_1'] = 1 \n",
    "    \n",
    "# ema crossover\n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Cross_zone_1'] == 1) &\n",
    "           (df['EMA_Plus_Sum'] >= 1) &\n",
    "           (df['DMP_14'] > 20), \n",
    "           'Long_Sign_2'] = 1  \n",
    "    \n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Cross_zone_2'] == 1) &\n",
    "           (df['EMA_Plus_Sum'] >= 1) &           \n",
    "           (df['DMP_14'] > 20), \n",
    "           'Long_Sign_3'] = 1  \n",
    "    \n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Cross_zone_3'] == 1) &\n",
    "           (df['EMA_Plus_Sum'] >= 1) &\n",
    "           (df['DMP_14'] > 20), \n",
    "           'Long_Sign_4'] = 1 \n",
    "    \n",
    "# vwap ema crossover \n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Cross_zone_222'] == 1) &\n",
    "           (df['VWAP_EMA_Plus_Sum'] >= 1) &           \n",
    "           (df['DMP_14'] > 20), \n",
    "           'Long_Sign_5'] = 1  \n",
    "    \n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Cross_zone_333'] == 1) &\n",
    "           (df['VWAP_EMA_Plus_Sum'] >= 1) &\n",
    "           (df['DMP_14'] > 20), \n",
    "           'Long_Sign_6'] = 1  \n",
    "    \n",
    "# vwap ema price\n",
    "    df.loc[(df['MACD_SignalLine'] > opt_macd_gap) &\n",
    "           (df['Conversion'] > df['Standard']) &\n",
    "           (df['VWAP_EMA_Plus_Sum'] >= 1) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['RSI'] < 40) &\n",
    "           (df['DMP_14'] > 20), \n",
    "           'Long_Sign_7'] = 1   \n",
    "\n",
    "# bband long term\n",
    "    df.loc[(df['ADX_14'] > 20) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Ceiling'] > 0) &\n",
    "           (df['EMA_Plus_Sum'] >= 1) &\n",
    "           (df['BBL_20_2.0'] > df['200_EMA']), \n",
    "           'Long_Sign_8'] = 1  \n",
    "    \n",
    "    df.loc[(df['ADX_14'] > 20) &\n",
    "           (df['CCI'] > 0) &\n",
    "           (df['bb_gap_diff_sc'] > bb_gap_threshold) &\n",
    "           (df['Floor'] < 0) &\n",
    "           (df['EMA_Plus_Sum'] >= 1) &\n",
    "           (df['BBL_20_2.0'] > df['200_EMA']), \n",
    "           'Long_Sign_9'] = 1  \n",
    "    \n",
    "    df.fillna(0, inplace = True)\n",
    "\n",
    "    # Short #########################################################################################################################################################\n",
    "    df.loc[(df['MACD_SignalLine'] < opt_macd_gap) &\n",
    "           (df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['DMN_14'] > 25),\n",
    "           'Short_Sign_1'] = 1   \n",
    "    \n",
    "    df.loc[(df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['EMA_Minus_Sum'] == -1) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['DMN_14'] > 25),\n",
    "           'Short_Sign_2'] = 1   \n",
    "              \n",
    "    df.loc[(df['MACD_SignalLine'] < opt_macd_gap) &\n",
    "           (df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['RSI'] > 70),\n",
    "           'Short_Sign_3'] = 1   \n",
    "    \n",
    "    df.loc[(df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Floor'] < 0) &\n",
    "           (df['TEMA_Plus_Sum'] == 0),\n",
    "           'Short_Sign_4'] = 1 \n",
    "    \n",
    "    df.loc[(df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] > bb_gap_threshold) &\n",
    "           (df['Ceiling'] > 0) &\n",
    "           (df['TEMA_Plus_Sum'] == 0),\n",
    "           'Short_Sign_5'] = 1 \n",
    "    \n",
    "    df.loc[(df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['Floor'] < 0) &\n",
    "           (df['VWAP_EMA_Plus_Sum'] == 0),\n",
    "           'Short_Sign_6'] = 1 \n",
    "    \n",
    "    df.loc[(df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] > bb_gap_threshold) &\n",
    "           (df['Ceiling'] > 0) &\n",
    "           (df['VWAP_EMA_Plus_Sum'] == 0),\n",
    "           'Short_Sign_7'] = 1 \n",
    "    \n",
    "    df.loc[(df['Conversion'] < df['Standard']) &\n",
    "           (df['CCI'] < 0) &\n",
    "           (df['TEMA_Minus_Sum'] == -1) &\n",
    "           (df['ADX_14'] > 20) &\n",
    "           (df['bb_gap_diff_sc'] < bb_gap_threshold) &\n",
    "           (df['DMN_14'] > 25),\n",
    "           'Short_Sign_8'] = 1 \n",
    "    \n",
    "    df.fillna(0, inplace = True)\n",
    "    \n",
    "    # select or read the best lottery file\n",
    "    lottery_list_exist = exists(f\"lottery_list_{ticker}_{currentdate}.txt\")\n",
    "    if lottery_list_exist == True:\n",
    "        with open(f\"lottery_list_{ticker}_{currentdate}.txt\", 'r') as f:\n",
    "            lottery_list = [line.rstrip('\\n') for line in f]\n",
    "        long_lottery = eval(lottery_list[0])\n",
    "        short_lottery = eval(lottery_list[1])    \n",
    "        os.remove(f\"lottery_list_{ticker}_{currentdate}.txt\")\n",
    "    else:\n",
    "        long_lottery = random.sample(range(1, 10), 5)\n",
    "        short_lottery = random.sample(range(1, 9), 4)\n",
    "    \n",
    "    # apply the lottery\n",
    "    df['Long_Sign'] = df[f'Long_Sign_{long_lottery[0]}'] + df[f'Long_Sign_{long_lottery[1]}'] + df[f'Long_Sign_{long_lottery[2]}'] + df[f'Long_Sign_{long_lottery[3]}'] + df[f'Long_Sign_{long_lottery[4]}']               \n",
    "    df['Short_Sign'] = df[f'Short_Sign_{short_lottery[0]}'] + df[f'Short_Sign_{short_lottery[1]}'] + df[f'Short_Sign_{short_lottery[2]}'] + df[f'Short_Sign_{short_lottery[3]}']         \n",
    "    \n",
    "    #################################################################################################################################################################\n",
    "\n",
    "    # backtest\n",
    "    df['Shares'] = 0\n",
    "    df['Unit Cost'] = 0\n",
    "    df['Avg Unit Cost'] = 0\n",
    "    \n",
    "    df['Target Percent Margin_Gain'] = realize_gain_percent\n",
    "    df['Target Percent Margin_Loss'] = realize_loss_percent\n",
    "    df['Target Gain Price'] = 0\n",
    "    df['Target Loss Price'] = 0\n",
    "    \n",
    "    df['Unrealized Value'] = 0\n",
    "    df['Unrealized Gain'] = 0\n",
    "    df['Realized Value'] = 0\n",
    "    df['Realized Gain'] = 0\n",
    "    df['LongPosition Days'] = 0\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        \n",
    "        # Target setting\n",
    "        if df['Avg Unit Cost'][i-1] != 0:\n",
    "            df['Target Gain Price'][i] = df['Avg Unit Cost'][i-1] + (df['Avg Unit Cost'][i-1] * df['Target Percent Margin_Gain'][i])\n",
    "            df['Target Loss Price'][i] = df['Avg Unit Cost'][i-1] + (df['Avg Unit Cost'][i-1] * df['Target Percent Margin_Loss'][i])    \n",
    "        else:\n",
    "            df['Target Gain Price'][i] = df['Current Price'][i] + (df['Current Price'][i] * df['Target Percent Margin_Gain'][i])\n",
    "            df['Target Loss Price'][i] = df['Current Price'][i] + (df['Current Price'][i] * df['Target Percent Margin_Loss'][i])    \n",
    "        \n",
    "        # Purchase\n",
    "        if df['Long_Sign'][i] >= 1 and df['Short_Sign'][i] == 0:\n",
    "\n",
    "            if df['Shares'][i-1] != 0:\n",
    "                df['Shares'][i] = df['Shares'][i-1]+1\n",
    "            else:\n",
    "                df['Shares'][i] = 1\n",
    "                \n",
    "            df['Unit Cost'][i] = df['Current Price'][i]\n",
    "            \n",
    "            if df['Avg Unit Cost'][i-1] != 0:\n",
    "                df['Avg Unit Cost'][i] = ((df['Avg Unit Cost'][i-1] * df['Shares'][i-1]) + df['Unit Cost'][i]) / df['Shares'][i]\n",
    "            else:\n",
    "                df['Avg Unit Cost'][i] = df['Unit Cost'][i]            \n",
    "\n",
    "            df['Unrealized Value'][i] = df['Avg Unit Cost'][i] * df['Shares'][i]\n",
    "            df['Unrealized Gain'][i] = (df['Current Price'][i] - df['Avg Unit Cost'][i]) / df['Avg Unit Cost'][i]\n",
    "            \n",
    "            if df['LongPosition Days'][i-1] != 0:\n",
    "                df['LongPosition Days'][i] = df['LongPosition Days'][i-1] + 1\n",
    "            else:\n",
    "                df['LongPosition Days'][i] = 1\n",
    "    \n",
    "        # Sell\n",
    "        elif (df['Long_Sign'][i] == 0 and df['Short_Sign'][i] >= 1) or df['Current Price'][i] >= df['Target Gain Price'][i] or df['Current Price'][i] < df['Target Loss Price'][i]:\n",
    "#         elif df['Current Price'][i] >= df['Target Gain Price'][i] or df['Current Price'][i] < df['Target Loss Price'][i]:\n",
    "                          \n",
    "            if df['Shares'][i-1] != 0:\n",
    "                df['Shares'][i] = 0\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if df['Avg Unit Cost'][i-1] != 0:\n",
    "                df['Avg Unit Cost'][i] = 0\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            df['Unrealized Value'][i] = 0\n",
    "            df['Unrealized Gain'][i] = 0\n",
    "            \n",
    "            if df['Shares'][i-1] != 0:\n",
    "                df['Realized Value'][i] = df['Current Price'][i] * df['Shares'][i-1]\n",
    "                df['Realized Gain'][i] = (df['Current Price'][i] - df['Avg Unit Cost'][i-1]) / df['Avg Unit Cost'][i-1]\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "            if df['Realized Gain'][i] != 0:\n",
    "                df['LongPosition Days'][i] = df['LongPosition Days'][i-1]+1\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # hold \n",
    "        else:\n",
    "            \n",
    "            if df['Shares'][i-1] != 0:\n",
    "                df['Shares'][i] = df['Shares'][i-1]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if df['Avg Unit Cost'][i-1] != 0:\n",
    "                df['Avg Unit Cost'][i] = df['Avg Unit Cost'][i-1]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if df['Avg Unit Cost'][i] != 0:\n",
    "                df['Unrealized Value'][i] = df['Avg Unit Cost'][i] * df['Shares'][i]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if df['Avg Unit Cost'][i] != 0:\n",
    "                df['Unrealized Gain'][i] = (df['Current Price'][i] - df['Avg Unit Cost'][i]) / df['Avg Unit Cost'][i]\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            if df['Avg Unit Cost'][i] == 0:\n",
    "                df['LongPosition Days'][i] = 0\n",
    "            else:\n",
    "                df['LongPosition Days'][i] = df['LongPosition Days'][i-1] + 1\n",
    "        \n",
    "    # save\n",
    "    df.to_csv(f'strategy_anaysis_{ticker}_{currentdate}.csv', index = False)\n",
    "\n",
    "## Display\n",
    "    df_plot = df[-days_plot:]\n",
    "\n",
    "    if plot_display_1 == 'off' and plot_display_2 == 'off':\n",
    "        pass\n",
    "    \n",
    "    elif plot_display_1 == 'on' and plot_display_2 == 'off':\n",
    "        # main plot\n",
    "        strategy_main_plot(ticker, df_plot, freq_mid_line_1)\n",
    "        \n",
    "        # backtest\n",
    "        twin_bar(x1_axis = df.Date, y1_axis = df['Unrealized Gain']*100, x1_color = 'red', x1_label = '\\nDate', y1_label = '\\nUnrealized Gain (%)', y1_label_color = 'red',\n",
    "                 x2_axis = df.Date, y2_axis = df['Realized Gain']*100, x2_color = 'blue', y2_label = '\\nRealized Gain (%)', y2_label_color = 'blue', \n",
    "                 title_name = f'{current_time()} - Performance | {ticker}', \n",
    "                 line1_x = df.Date, line1_y = [df['Unrealized Gain'].min()*100]*df.shape[0], line1_color = 'red', style1 = '--',\n",
    "                 line2_x = df.Date, line2_y = [df['Unrealized Gain'].max()*100]*df.shape[0], line2_color = 'red', style2 = '--')\n",
    "        plt.show()\n",
    "        \n",
    "    elif plot_display_1 == 'on' and plot_display_2 == 'on':\n",
    "        # main plot\n",
    "        strategy_main_plot(ticker, df_plot, freq_mid_line_1)\n",
    "        \n",
    "        # Volume\n",
    "        twin_bar(x1_axis = df_plot.Date, y1_axis = df_plot['6_EVMA'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\n6_EVMA', y1_label_color = 'blue',\n",
    "                 x2_axis = df_plot.Date, y2_axis = df_plot['EVMA_SUM'], x2_color = 'red', y2_label = '\\nEVMA_SUM', y2_label_color = 'red',\n",
    "                 title_name = f'{current_time()} - Volume | {ticker}', \n",
    "                 line1_x = df_plot.Date, line1_y = [7]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                 line2_x = df_plot.Date, line2_y = [7]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "\n",
    "        # ema price\n",
    "        twin_bar(x1_axis = df_plot.Date, y1_axis = df_plot['Current Price'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nCurrent Price', y1_label_color = 'blue',\n",
    "                 x2_axis = df_plot.Date, y2_axis = df_plot['EMA_Plus_Sum'], x2_color = 'red', y2_label = '\\nEMA_Plus_Sum', y2_label_color = 'red',\n",
    "                 title_name = f'{current_time()} - EMA Price | {ticker}', \n",
    "                 line1_x = df_plot.Date, line1_y = [1]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                 line2_x = df_plot.Date, line2_y = [1]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        \n",
    "        # tema price\n",
    "        twin_bar(x1_axis = df_plot.Date, y1_axis = df_plot['Current Price'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nCurrent Price', y1_label_color = 'blue',\n",
    "                 x2_axis = df_plot.Date, y2_axis = df_plot['TEMA_Plus_Sum'], x2_color = 'red', y2_label = '\\nTEMA_Plus_Sum', y2_label_color = 'red',\n",
    "                 title_name = f'{current_time()} - TEMA Price | {ticker}', \n",
    "                 line1_x = df_plot.Date, line1_y = [1]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                 line2_x = df_plot.Date, line2_y = [1]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        \n",
    "        # vwma ema price\n",
    "        twin_bar(x1_axis = df_plot.Date, y1_axis = df_plot['VWAP'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nVWAP', y1_label_color = 'blue',\n",
    "                 x2_axis = df_plot.Date, y2_axis = df_plot['VWAP_EMA_Plus_Sum'], x2_color = 'red', y2_label = '\\nVWAP_EMA_Plus_Sum', y2_label_color = 'red',\n",
    "                 title_name = f'{current_time()} - VWAP EMA Price | {ticker}', \n",
    "                 line1_x = df_plot.Date, line1_y = [1]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                 line2_x = df_plot.Date, line2_y = [1]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "\n",
    "        # MACD_1\n",
    "        twin_plot(x1_axis = df_plot.Date, y1_axis = df_plot['MACD'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nMACD', y1_label_color = 'blue',\n",
    "                  x2_axis = df_plot.Date, y2_axis = df_plot['SignalLine'], x2_color = 'red', y2_label = '\\nSignalLine', y2_label_color = 'red',\n",
    "                  title_name = f'{current_time()} - MACD > SignalLine | {ticker}', \n",
    "                  line1_x = df_plot.Date, line1_y = [opt_macd_gap]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                  line2_x = df_plot.Date, line2_y = [opt_macd_gap]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        \n",
    "        # MACD_2\n",
    "        twin_bar(x1_axis = df_plot.Date, y1_axis = df_plot['MACD_SignalLine'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nMACD ~ SignalLine', y1_label_color = 'blue',\n",
    "                 x2_axis = df_plot.Date, y2_axis = df_plot['MACD_SignalLine'], x2_color = 'green', y2_label = '\\nMACD ~ SignalLine', y2_label_color = 'red',\n",
    "                 title_name = f'{current_time()} - MACD ~ SignalLine | {ticker}', \n",
    "                 line1_x = df_plot.Date, line1_y = [opt_macd_gap]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                 line2_x = df_plot.Date, line2_y = [opt_macd_gap]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        \n",
    "        # RSI\n",
    "        twin_plot(x1_axis = df_plot.Date, y1_axis = df_plot['Current Price'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nCurrent Price', y1_label_color = 'blue',\n",
    "                  x2_axis = df_plot.Date, y2_axis = df_plot['RSI'], x2_color = 'red', y2_label = '\\nRSI', y2_label_color = 'red',\n",
    "                  title_name = f'{current_time()} - RSI (Long: < 70 || Short: > 70) | {ticker}', \n",
    "                  line1_x = df_plot.Date, line1_y = [70]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                  line2_x = df_plot.Date, line2_y = [90]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        # 40 RSI\n",
    "        twin_plot(x1_axis = df_plot.Date, y1_axis = df_plot['Current Price'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nCurrent Price', y1_label_color = 'blue',\n",
    "                  x2_axis = df_plot.Date, y2_axis = df_plot['40_RSI'], x2_color = 'red', y2_label = '\\n40_RSI', y2_label_color = 'red',\n",
    "                  title_name = f'{current_time()} - 40 RSI (Long: > 40 || Short: <= 40) | {ticker}', \n",
    "                  line1_x = df_plot.Date, line1_y = [40]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                  line2_x = df_plot.Date, line2_y = [40]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        # 70 RSI\n",
    "        twin_plot(x1_axis = df_plot.Date, y1_axis = df_plot['Current Price'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nCurrent Price', y1_label_color = 'blue',\n",
    "                  x2_axis = df_plot.Date, y2_axis = df_plot['70_RSI'], x2_color = 'red', y2_label = '\\n70_RSI', y2_label_color = 'red',\n",
    "                  title_name = f'{current_time()} - 70 RSI (Long: > 45 || Short: <= 45) | {ticker}', \n",
    "                  line1_x = df_plot.Date, line1_y = [45]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                  line2_x = df_plot.Date, line2_y = [45]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        \n",
    "        # ADX\n",
    "        plt.figure(figsize = (16,9))\n",
    "        plt.title(f'Average Directional Index | {ticker}')\n",
    "        plt.grid()\n",
    "        errorbar_columns = errorbar_columns = ['ADX_14', 'DMP_14', 'DMN_14']\n",
    "        for column in errorbar_columns:\n",
    "            plt.errorbar(data = df_plot, x = 'Date', y = column, linewidth = 1.5, label = column)\n",
    "            plt.annotate(xy = [df_plot[-1:].Date.values, df_plot[-1:][column].values], text = column)\n",
    "        plt.plot(df_plot.Date, [20]*df_plot.shape[0], color = 'black', linestyle = '--')\n",
    "        plt.plot(df_plot.Date, [25]*df_plot.shape[0], color = 'black', linestyle = '--')\n",
    "        \n",
    "        # CCI\n",
    "        twin_plot(x1_axis = df_plot.Date, y1_axis = df_plot['Close'], x1_color = 'blue', x1_label = '\\nDate', y1_label = '\\nCurrent Price', y1_label_color = 'blue',\n",
    "                 x2_axis = df_plot.Date, y2_axis = df_plot['CCI'], x2_color = 'red', y2_label = '\\nCCI', y2_label_color = 'red',\n",
    "                 title_name = f'{current_time()} - Commodity Channel Index | {ticker}', \n",
    "                 line1_x = df_plot.Date, line1_y = [100]*df_plot.shape[0], line1_color = 'black', style1 = '--',\n",
    "                 line2_x = df_plot.Date, line2_y = [-100]*df_plot.shape[0], line2_color = 'black', style2 = '--')\n",
    "        \n",
    "        # Ichimoku\n",
    "        ichimoku_chart(ticker, df_plot)\n",
    "        \n",
    "        # bollinger band\n",
    "        bollinger_plot(ticker, df_plot, squeeze_ceiling_buy, squeeze_floor_sell, loose_ceiling_sell, loose_floor_buy)\n",
    "    \n",
    "        # backtest\n",
    "        twin_bar(x1_axis = df.Date, y1_axis = df['Unrealized Gain']*100, x1_color = 'red', x1_label = '\\nDate', y1_label = '\\nUnrealized Gain (%)', y1_label_color = 'red',\n",
    "                 x2_axis = df.Date, y2_axis = df['Realized Gain']*100, x2_color = 'blue', y2_label = '\\nRealized Gain (%)', y2_label_color = 'blue', \n",
    "                 title_name = f'{current_time()} - Performance | {ticker}', \n",
    "                 line1_x = df.Date, line1_y = [df['Unrealized Gain'].min()*100]*df.shape[0], line1_color = 'red', style1 = '--',\n",
    "                 line2_x = df.Date, line2_y = [df['Unrealized Gain'].max()*100]*df.shape[0], line2_color = 'red', style2 = '--')\n",
    "        plt.show()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # performance\n",
    "    duration = len(df)\n",
    "    exposure_days = len(df[df['Unrealized Value'] != 0])\n",
    "    exposure_percentage = np.round(exposure_days / duration * 100, 2)\n",
    "    equity_peak = df[df['Unrealized Value'] == df['Unrealized Value'].max()]\n",
    "    equity_peak_value = np.round(equity_peak['Unrealized Value'].values[0], 2)\n",
    "    equity_peak_positionDays = equity_peak['LongPosition Days'].values[0]\n",
    "    AvgUnrealizedGain = np.round(df['Unrealized Gain'].mean() *100, 2)\n",
    "    \n",
    "    pdf = df[df['Realized Gain'] != 0][['Date', 'Realized Gain', 'LongPosition Days']].reset_index(drop = True)\n",
    "    if len(pdf) == 0:\n",
    "        AvgRealizedGain = 0\n",
    "        AvgHoldingDays = int(df[-1:]['LongPosition Days'].mean())\n",
    "        stg_volatility = 0\n",
    "        BestPerf = df['Unrealized Gain'].max() *100\n",
    "        BestPerf_date = str(df[df['Unrealized Gain'] == df['Unrealized Gain'].max()].Date.values[0])[:10]\n",
    "        Worstperf = df['Unrealized Gain'].min() *100        \n",
    "        Worstperf_date = str(df[df['Unrealized Gain'] == df['Unrealized Gain'].min()].Date.values[0])[:10]\n",
    "        PerfRange = BestPerf - Worstperf\n",
    "        SuccessRate = 0\n",
    "        timesRealized = 0\n",
    "    else:\n",
    "        AvgRealizedGain = np.round(pdf['Realized Gain'].mean() *100, 2)\n",
    "        AvgHoldingDays = int(pdf['LongPosition Days'].mean())\n",
    "        stg_volatility = stg_volatility_check(df[df['Unrealized Gain'] != 0])\n",
    "        BestPerf = df['Unrealized Gain'].max() *100\n",
    "        BestPerf_date = str(df[df['Unrealized Gain'] == df['Unrealized Gain'].max()].Date.values[0])[:10]\n",
    "        Worstperf = df['Unrealized Gain'].min() *100        \n",
    "        Worstperf_date = str(df[df['Unrealized Gain'] == df['Unrealized Gain'].min()].Date.values[0])[:10]\n",
    "        PerfRange = BestPerf - Worstperf\n",
    "        pdf['Result'] = (pdf['Realized Gain'] > 0)\n",
    "        SuccessRate = np.round(pdf.Result.mean()*100, 2)    \n",
    "        timesRealized = len(pdf['Realized Gain'])      \n",
    "    if plot_display_1 == 'off':\n",
    "        pass\n",
    "    else:\n",
    "        pd.set_option('display.max_rows', None)\n",
    "        if plot_display_1 == 'off':\n",
    "            pass\n",
    "        else:\n",
    "            pdf['Realized Gain'] = np.round(pdf['Realized Gain']*100, 2)\n",
    "            display(pdf)\n",
    "            \n",
    "        firstDate = str(df['Date'].values[0])\n",
    "        long_sign_sum = df['Long_Sign'].sum()\n",
    "        short_sign_sum = df['Short_Sign'].sum()\n",
    "        long_to_short = long_sign_sum / short_sign_sum\n",
    "        \n",
    "        print('<<< Backtest Result: ' + '\\033[1m' + f'{ticker}' + '\\033[0m' + ' >>>')\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print('• Start Date: ' + '\\033[1m' + f'{firstDate[:10]}' + '\\033[0m')\n",
    "        print('• Duration: ' + '\\033[1m' + f'{duration}' + '\\033[0m' + ' days')\n",
    "        print('• Exposure Days: ' + '\\033[1m' + f'{exposure_days}' + '\\033[0m' + ' days')\n",
    "        print('• Exposure [%]: ' + '\\033[1m' + f'{exposure_percentage}' + '\\033[0m' + ' %')\n",
    "        print('• Equity Peak [$]: ' + '\\033[1m' + f'{equity_peak_value}' + '\\033[0m')\n",
    "        print('• Equity Peak Position Days: ' + '\\033[1m' + f'{equity_peak_positionDays}' + '\\033[0m' + ' days')\n",
    "        \n",
    "        print( '\\033[1m' + f'• Realized(>={min_RealizedCount}): ' + f'{timesRealized}' + ' times' + '\\033[0m')\n",
    "        print('• Long Sign:  ' + '\\033[1m' + f'{int(long_sign_sum)}' + '\\033[0m' + ' signs')\n",
    "        print('• Short Sign: ' + '\\033[1m' + f'{int(short_sign_sum)}' + '\\033[0m' + ' signs')\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print('==> Long to Short: ' + '\\033[1m' + f'{np.round(long_to_short, 2)}' + '\\033[0m')\n",
    "        \n",
    "        print('\\n\\n<<< Strategy Verification: ' + '\\033[1m' + f'{ticker}' + '\\033[0m' + ' >>>')\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        print(f'• Total Search Times: ' + '\\033[1m' + f'{parameter_search_times} cycles' + '\\033[0m')\n",
    "        print('\\033[1m' + f'• Success Rate (>{min_SuccessRate}): ' + f'{SuccessRate} %' + '\\033[0m')\n",
    "        print('\\033[1m' + f'• Average Realized Gain (>{min_AvgRealizedGain}): ' + f'{AvgRealizedGain} %' + '\\033[0m')\n",
    "        print('\\033[1m' + f'• Realized Gain Volatility: ' + f'{stg_volatility}' + '\\033[0m')\n",
    "        print(f'• Average Unrealized Gain : ' + '\\033[1m' + f'{AvgUnrealizedGain} %' + '\\033[0m')\n",
    "        print(f'• Highest Unrealized Gain : ' + '\\033[1m' + f'{np.round(BestPerf, 2)} %' + '\\033[0m')\n",
    "        print(f'• Highest Unrealized Gain Date: ' + '\\033[1m' + f'{BestPerf_date}' + '\\033[0m')\n",
    "        print(f'• Lowest Unrealized Gain (>{min_Worstperf}): ' + '\\033[1m' + f'{np.round(Worstperf, 2)} %' + '\\033[0m')\n",
    "        print(f'• Lowest Unrealized Gain Date: ' + '\\033[1m' + f'{Worstperf_date}' + '\\033[0m')\n",
    "        print(f'• Unrealized Gain Range (<{max_PerfRange}): ' + '\\033[1m' + f'{np.round(PerfRange, 2)} %' + '\\033[0m')\n",
    "        print(f'• Average LongPosition Days (<={max_HoldingDays}): ' + '\\033[1m' + f'{AvgHoldingDays} days' + '\\033[0m')\n",
    "        print('--------------------------------------------------------------------------')\n",
    "        \n",
    "    return [AvgRealizedGain, AvgUnrealizedGain, AvgHoldingDays, BestPerf, Worstperf, PerfRange, SuccessRate, timesRealized, long_lottery, short_lottery]\n",
    "\n",
    "\n",
    "def stg_para_search(ticker, parameter_search_times):\n",
    "    \n",
    "    stg_df = pd.DataFrame(columns = [\"Ticker\",\n",
    "                                     \"Period\",\n",
    "                                     \"Searching Count\",\n",
    "                                     \"Freq Line\",\n",
    "                                     \"Freq Line Gap\",\n",
    "                                     \"EVMA Sum\",\n",
    "                                     \"MACD Gap\",\n",
    "                                     \"Target Gain Percent\",\n",
    "                                     \"Target Loss Percent\",\n",
    "                                     \"Avg Realized Gain\",\n",
    "                                     \"Avg Unrealized Gain\",\n",
    "                                     \"Avg Holding Days\",\n",
    "                                     \"Best Perf\",\n",
    "                                     \"Worst Perf\",\n",
    "                                     \"Perf Range\", \n",
    "                                     \"Success Rate Percent\", \n",
    "                                     \"Realized Count\"])\n",
    "\n",
    "    print(f'\\n\\n\\n{current_time()} - Strategy Parameter Searching '  + '\\033[1m' + f'{min_SuccessRate}'  + '\\033[0m' + ' - Ticker: ' + '\\033[1m' + f'{ticker}' + '\\033[0m')\n",
    "    for i in tqdm(range(parameter_search_times)):\n",
    "        # random research on parameters\n",
    "        freqLine_choice = random.choice(freqLine_list)\n",
    "        freqLine_gap_choice = random.choice(freqLine_gap_list)\n",
    "        evma_sum_choice = random.choice(evma_sum_list)\n",
    "        macd_gap_choice = random.choice(macd_gap_list)\n",
    "        gain_choice = random.choice(realize_gain_percent_list)\n",
    "        loss_choice = random.choice(realize_loss_percent_list)  \n",
    "        \n",
    "        stg = macro_strategy(ticker, \n",
    "                             period = strategy_days, \n",
    "                             opt_freqLine = freqLine_choice,\n",
    "                             opt_freqLine_gap = freqLine_gap_choice,\n",
    "                             opt_evma_sum = evma_sum_choice,\n",
    "                             opt_macd_gap = macd_gap_choice,\n",
    "                             realize_gain_percent = gain_choice,\n",
    "                             realize_loss_percent = loss_choice,\n",
    "                             plot_display_1 = 'off', \n",
    "                             plot_display_2 = 'off')\n",
    "\n",
    "        info_dict = {}\n",
    "        try:\n",
    "            info_dict[\"Ticker\"] = ticker\n",
    "            info_dict[\"Period\"] = strategy_days\n",
    "            info_dict[\"Searching Count\"] = i\n",
    "            info_dict[\"Freq Line\"] = freqLine_choice\n",
    "            info_dict[\"Freq Line Gap\"] = freqLine_gap_choice\n",
    "            info_dict[\"EVMA Sum\"] = evma_sum_choice\n",
    "            info_dict[\"MACD Gap\"] = macd_gap_choice\n",
    "            info_dict[\"Target Gain Percent\"] = gain_choice\n",
    "            info_dict[\"Target Loss Percent\"] = loss_choice\n",
    "            info_dict[\"Avg Realized Gain\"] = stg[0]\n",
    "            info_dict[\"Avg Unrealized Gain\"] = stg[1]\n",
    "            info_dict[\"Avg Holding Days\"] = stg[2]\n",
    "            info_dict[\"Best Perf\"] = stg[3]\n",
    "            info_dict[\"Worst Perf\"] = stg[4]\n",
    "            info_dict[\"Perf Range\"] = stg[5]\n",
    "            info_dict[\"Success Rate Percent\"] = stg[6]\n",
    "            info_dict[\"Realized Count\"] = stg[7]\n",
    "        except:\n",
    "            alarm()\n",
    "            print(f\"unable to read stg: {i} th\")\n",
    "\n",
    "        stg_df = stg_df.append(info_dict, ignore_index=True)\n",
    "        \n",
    "#         print(f'Debug - {i} Realized Counts: {stg[7]} > {min_RealizedCount}')\n",
    "\n",
    "        # break loop \n",
    "        if stg[0] > min_AvgRealizedGain and stg[2] <= max_HoldingDays and stg[4] > min_Worstperf and stg[5] < max_PerfRange and stg[6] > min_SuccessRate and stg[7] > min_RealizedCount:\n",
    "#             print('\\n>>> Break Condition has been met. Save the condition <<<\\n')\n",
    "            \n",
    "            # save lottery list\n",
    "            lottery_list = [stg[8], stg[9]]            \n",
    "            with open(f\"lottery_list_{ticker}_{currentdate}.txt\", 'w') as f:\n",
    "                for s in lottery_list:\n",
    "                    f.write(str(s) + '\\n')\n",
    "            break\n",
    "\n",
    "    # save\n",
    "    stg_df = stg_df[(stg_df['Avg Realized Gain'] > min_AvgRealizedGain) & \n",
    "            (stg_df['Avg Holding Days'] <= max_HoldingDays) &        \n",
    "            (stg_df['Worst Perf'] > min_Worstperf) & \n",
    "            (stg_df['Perf Range'] < max_PerfRange) &        \n",
    "            (stg_df['Success Rate Percent'] > min_SuccessRate) & \n",
    "            (stg_df['Realized Count'] > min_RealizedCount)]\n",
    "    stg_df.to_csv(f'stg_para_search_{ticker}_{currentdate}.csv', index = False)\n",
    "\n",
    "    \n",
    "def strategy_analysis_main(ticker, display_1 = 'off', display_2 = 'off'):\n",
    "    \n",
    "    '''\n",
    "    input: ticker name, display_1, display_2 (e.g. strategy_analysis_main('^GSPC', 'on', 'off'))\n",
    "    output: signs\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # display settings \n",
    "    # on + off: summary | on + on: all\n",
    "\n",
    "    # generate parameter search table\n",
    "    stg_para_search(ticker, parameter_search_times)\n",
    "\n",
    "    # get the highest parameter\n",
    "    df = pd.read_csv(f'stg_para_search_{ticker}_{currentdate}.csv').head(1)\n",
    "    \n",
    "#     display(df)\n",
    "    \n",
    "    if len(df) != 0:\n",
    "        best_searching_count = df['Searching Count'].values[0]\n",
    "        best_freqLine = df['Freq Line'].values[0] \n",
    "        best_freqLine_gap = df['Freq Line Gap'].values[0]\n",
    "        best_evma_sum = df['EVMA Sum'].values[0]\n",
    "        best_macd_gap = df['MACD Gap'].values[0]\n",
    "        best_gain = df['Target Gain Percent'].values[0]\n",
    "        best_loss = df['Target Loss Percent'].values[0]\n",
    "\n",
    "\n",
    "        # make a strategy with the parameter \n",
    "        # analysis_result[AvgRealizedGain, AvgHoldingDays, BestPerf, Worstperf, PerfRange]\n",
    "        analysis_result = macro_strategy(ticker, \n",
    "                                         period = strategy_days, \n",
    "                                         opt_freqLine = best_freqLine,\n",
    "                                         opt_freqLine_gap = best_freqLine_gap,\n",
    "                                         opt_evma_sum = best_evma_sum,\n",
    "                                         opt_macd_gap = best_macd_gap,\n",
    "                                         realize_gain_percent = best_gain,\n",
    "                                         realize_loss_percent = best_loss,\n",
    "                                         plot_display_1 = display_1, \n",
    "                                         plot_display_2 = display_2)\n",
    "\n",
    "        if display_1 == 'off':\n",
    "            pass\n",
    "        else:\n",
    "            print('\\n\\n<<< Optimized Parameters: ' + '\\033[1m' + f'{ticker}' + '\\033[0m' + ' >>>')\n",
    "            print('--------------------------------------------------------------------------')\n",
    "            print('\\033[1m' + f'• Best Searching Count - {int(best_searching_count)}/{parameter_search_times}' + '\\033[0m')\n",
    "            print(f'• Best Freq Line - {freqLine_list}: ' + '\\033[1m' + f'{np.round(best_freqLine, 3)}' + '\\033[0m')\n",
    "            print(f'• Best Freq Line Gap - {freqLine_gap_list}: ' + '\\033[1m' + f'{np.round(best_freqLine_gap, 3)}' + '\\033[0m')\n",
    "            print(f'• Best EVMA Sum - {evma_sum_list}: ' + '\\033[1m' + f'{np.round(best_evma_sum, 3)}' + '\\033[0m')\n",
    "            print(f'• Best MACD Gap - {macd_gap_list}: ' + '\\033[1m' + f'{np.round(best_macd_gap, 3)}' + '\\033[0m')\n",
    "            print(f'• Best Gain [%] - {realize_gain_percent_list}: ' + '\\033[1m' + f'{np.round(best_gain*100, 2)}' + '\\033[0m')\n",
    "            print(f'• Best Loss [%] - {realize_loss_percent_list}: ' + '\\033[1m' + f'{np.round(best_loss*100, 2)}' + '\\033[0m')\n",
    "            print('--------------------------------------------------------------------------')\n",
    "\n",
    "        df = pd.read_csv(f'strategy_anaysis_{ticker}_{currentdate}.csv')\n",
    "\n",
    "        if analysis_result[0] > min_AvgRealizedGain and analysis_result[2] <= max_HoldingDays and analysis_result[4] > min_Worstperf and analysis_result[5] < max_PerfRange and analysis_result[6] >= min_SuccessRate and analysis_result[7] >= min_RealizedCount:\n",
    "            print('==> Strategy Verification Result: ' + '\\033[1m' + 'Passed' + '\\033[0m')\n",
    "\n",
    "            # trading condition\n",
    "            trading_df = df.tail(analysis_result[2])\n",
    "            long_sign_sum = trading_df.Long_Sign.values.sum()\n",
    "            short_sign_sum = trading_df.Short_Sign.values.sum()\n",
    "\n",
    "            # recent Long sign date\n",
    "            if len(trading_df[trading_df.Long_Sign > 0]) > 0:\n",
    "                recent_long_date = trading_df[trading_df.Long_Sign > 0].Date.values[-1]\n",
    "            else:\n",
    "                recent_long_date = 0\n",
    "\n",
    "            # recent Short sign date\n",
    "            if len(trading_df[trading_df.Short_Sign > 0]) > 0:\n",
    "                recent_short_date = trading_df[trading_df.Short_Sign > 0].Date.values[-1]\n",
    "            else:\n",
    "                recent_short_date = 0\n",
    "\n",
    "            # sign decision\n",
    "            if display_1 == 'off':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'\\n\\n<<< Sign Check: {analysis_result[2]} days >>>')\n",
    "                print('--------------------------------------------------------------------------')\n",
    "                print(f'{analysis_result[2]} days total (Buy | Sell): ' + f'{int(long_sign_sum)} | {int(short_sign_sum)}')\n",
    "                print('--------------------------------------------------------------------------')\n",
    "\n",
    "            if df['Current Price'].tail(1).values.mean() >= df['Target Gain Price'].tail(1).values.mean() or df['Current Price'].tail(1).values.mean() < df['Target Loss Price'].tail(1).values.mean():\n",
    "                print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'Realize Sign\\n' + '\\033[0m')\n",
    "                return [-1, best_gain, best_loss]\n",
    "\n",
    "            else:\n",
    "                # Buy condition\n",
    "                if long_sign_sum > 0 and short_sign_sum == 0:\n",
    "                    print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'Long Sign\\n' + '\\033[0m')\n",
    "                    return [1, best_gain, best_loss]\n",
    "\n",
    "                # Sell condition\n",
    "                elif long_sign_sum == 0 and short_sign_sum > 0 and analysis_price(ticker)[8] == True:\n",
    "                    print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'Short Sign\\n' + '\\033[0m')\n",
    "                    return [-1, best_gain, best_loss]\n",
    "\n",
    "                # Dual condition\n",
    "                elif long_sign_sum > 0 and short_sign_sum > 0:\n",
    "                    if recent_long_date > recent_short_date:\n",
    "                        print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'Long Sign\\n' + '\\033[0m')\n",
    "                        return [1, best_gain, best_loss]\n",
    "\n",
    "                    elif recent_long_date + recent_short_date == 0:\n",
    "                        print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'No Sign\\n' + '\\033[0m')\n",
    "                        return [0, best_gain, best_loss]          \n",
    "\n",
    "                    else:\n",
    "                        if analysis_price(ticker)[8] == True:\n",
    "                            print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'Short Sign\\n' + '\\033[0m')\n",
    "                            return [-1, best_gain, best_loss]\n",
    "                        else:\n",
    "                            print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'No Sign\\n' + '\\033[0m')\n",
    "                            return [0, best_gain, best_loss]  \n",
    "                            \n",
    "                # no sign condition\n",
    "                else:\n",
    "                    print(f'==> {current_time()} - Sign Check Result: ' + '\\033[1m' + 'No Sign\\n' + '\\033[0m')\n",
    "                    return [0, best_gain, best_loss]\n",
    "\n",
    "        else:\n",
    "            print(f'==> {current_time()} - Strategy Verification Result: ' + '\\033[1m' + 'Failed\\n' + '\\033[0m')\n",
    "            print('\\033[1m' + '!!! Disregard the random result !!!\\n' + '\\033[0m')\n",
    "            return [-10, best_gain, best_loss]\n",
    "        \n",
    "    else:\n",
    "        print(f'==> {current_time()} - Strategy Verification Result: ' + '\\033[1m' + 'Failed\\n' + '\\033[0m')\n",
    "        return [-10, 0, 0]\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('> Modules #3 imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea6a027b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "def tensorflow_model_processing(days, h, o, l, tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features):\n",
    "    # initiate\n",
    "    tf.keras.backend.clear_session() \n",
    "\n",
    "    # build model\n",
    "    layer_neurons = [256, 128, 64, 32, 16, 8]\n",
    "    input_layers_features = tf_X_train.shape[1]\n",
    "    output_layers_features = 1\n",
    "\n",
    "    # model design\n",
    "    tf_model = tf.keras.Sequential()\n",
    "    tf_model.add(tf.keras.layers.Flatten(input_shape=(input_layers_features, 1)))\n",
    "    for neurons in layer_neurons:\n",
    "        tf_model.add(tf.keras.layers.Dense(neurons, activation = h))\n",
    "        tf_model.add(tf.keras.layers.Dropout(0.2))\n",
    "    tf_model.add(tf.keras.layers.Dense(output_layers_features, activation = o))\n",
    "    \n",
    "    # compile\n",
    "    tf_model.compile(optimizer='adam', \n",
    "                     loss = l, \n",
    "                     metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=0.5, threshold=0.5)])\n",
    "\n",
    "    # Stop training when there is no improvement in the validation loss for n consecutive epochs\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_fbeta_score', patience = 10)\n",
    "\n",
    "    # Save the Model with the lowest validation loss\n",
    "    save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                                   monitor = 'val_fbeta_score',\n",
    "                                                   save_best_only=True)\n",
    "\n",
    "#         # evaluate loss and fbeta before tuning\n",
    "#         loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'\\n\\nTensorFlow Model Evalution before training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "\n",
    "    # train the model\n",
    "    EPOCHS = 500    \n",
    "    history = tf_model.fit(tf_X_train, tf_y_train, \n",
    "                           epochs = EPOCHS, \n",
    "                           validation_data = (tf_X_val, tf_y_val), \n",
    "                           batch_size = 8, \n",
    "                           verbose = 0, \n",
    "                           callbacks = [early_stopping, save_best])\n",
    "    \n",
    "    # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1, callbacks = [early_stopping])\n",
    "    # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1)\n",
    "\n",
    "    # evaluate loss and fbeta after tuning\n",
    "    loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "    print(f'\\nTensorFlow Model Evalution after training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n\\n\\n\\n')\n",
    "\n",
    "    # predict up or down\n",
    "    prediction = tf_model.predict(pred_features)\n",
    "    print(prediction)\n",
    "    print(f'{days} days moving average applied\\n\\n\\n')\n",
    "    return [days, h, o, l, loss, fbeta, prediction[0][0]]\n",
    "\n",
    "\n",
    "def opt_data_processing(df):\n",
    "    # features to predict\n",
    "    pred_features = df[-1:]\n",
    "\n",
    "    # get the outcome from the tomorrow price\n",
    "    df['Tmr_price'] = df['Stock_price'].shift(-1)\n",
    "    df['classifier_result'] = (df['Tmr_price'] > df['Stock_price']).astype(int)\n",
    "    df.drop(columns = 'Tmr_price', inplace = True)\n",
    "\n",
    "    # features to train &test\n",
    "    df = df[:-1]\n",
    "\n",
    "    # outcome \n",
    "    outcomes = df.pop('classifier_result').values\n",
    "    # filter out unwanted columns\n",
    "    features = df.values\n",
    "\n",
    "    # for non-tf\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 0, stratify = outcomes)\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)  \n",
    "    non_tf_pred_features = sc.transform(pred_features)\n",
    "\n",
    "    # for tf\n",
    "    tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 0, stratify = outcomes)\n",
    "\n",
    "    def tf_normalize(set):  \n",
    "        return tf.keras.utils.normalize(set)\n",
    "    tf_X_train = tf_normalize(tf_X_train)\n",
    "    tf_X_test = tf_normalize(X_test[:int(len(X_test)/2)])\n",
    "    tf_X_val = tf_normalize(X_test[int(len(X_test)/2):])\n",
    "    tf_y_test = y_test[:int(len(y_test)/2)]\n",
    "    tf_y_val = y_test[int(len(y_test)/2):] \n",
    "    tf_pred_features = tf_normalize(pred_features)\n",
    "    \n",
    "    return tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, tf_pred_features\n",
    "\n",
    "\n",
    "def param_optimizer(df):\n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0]) \n",
    "    \n",
    "    # for hidden layer model\n",
    "#     hidden_activation_f = ['relu', 'tanh']\n",
    "#     output_activation_f = ['softmax', 'sigmoid']\n",
    "#     loss_f = ['BinaryCrossentropy', 'Hinge', 'MeanSquaredError']\n",
    "    \n",
    "    # for LSTM model\n",
    "    hidden_activation_f = ['relu'] # n/a\n",
    "    output_activation_f = ['sigmoid']    \n",
    "    loss_f = ['BinaryCrossentropy', 'Hinge', 'MeanSquaredError']\n",
    "\n",
    "    param_tune = []\n",
    "    for days in range(moving_avg_value, moving_avg_value+1):\n",
    "        print(f'\\n\\n########## Initial setting: {days} days moving average ##########\\n\\n')\n",
    "        # ML-Classifier\n",
    "        # Get moving average\n",
    "        mvp = days\n",
    "        mavg = pd.DataFrame()\n",
    "        for column in df.columns[1:]:\n",
    "            mv_change = np.array(df[column])\n",
    "            mv = []\n",
    "            for i in range(len(mv_change)-mvp+1):\n",
    "                mv.append(np.average(mv_change[i:mvp+i]))\n",
    "                i+=1\n",
    "            mavg[column] = pd.DataFrame(mv)\n",
    "        tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features = opt_data_processing(mavg)\n",
    "        \n",
    "        for h in hidden_activation_f:\n",
    "            print(f'>>> Hidden Function: {h}')\n",
    "            for o in output_activation_f:\n",
    "                print(f'>> Output Function: {o}')\n",
    "                for l in loss_f:\n",
    "                    print(f'> Loss Function: {l}')\n",
    "                    param_tune.append(tensorflow_model_processing(days, h, o, l, tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features))\n",
    "                    print('----------------------------------------------------')\n",
    "    return param_tune\n",
    "\n",
    "def mvg_optimizer(df, hidden_activation, output_activation, loss):\n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0])    \n",
    "        \n",
    "        if moving_avg_value == 3:\n",
    "            moving_avg_value = 4\n",
    "    \n",
    "    mvg_tune = []\n",
    "    for days in range(moving_avg_value-3, moving_avg_value+5):\n",
    "        if days < 1:\n",
    "            days = 1\n",
    "        try:\n",
    "            print(f'\\n\\n\\n##### {days} days moving average #####')\n",
    "            # ML-Classifier\n",
    "            # Get moving average\n",
    "            mvp = days\n",
    "            mavg = pd.DataFrame()\n",
    "            for column in df.columns[1:]:\n",
    "                mv_change = np.array(df[column])\n",
    "                mv = []\n",
    "                for i in range(len(mv_change)-mvp+1):\n",
    "                    mv.append(np.average(mv_change[i:mvp+i]))\n",
    "                    i+=1\n",
    "                mavg[column] = pd.DataFrame(mv)\n",
    "            tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features = opt_data_processing(mavg)\n",
    "            tensorflow_model_processing_result = tensorflow_model_processing(days, hidden_activation, output_activation, loss, tf_X_train, tf_y_train, tf_X_test, tf_y_test, tf_X_val, tf_y_val, pred_features)\n",
    "            mvg_tune.append(tensorflow_model_processing_result)\n",
    "            print('====================================================')   \n",
    "            print('Loop breaker:', tensorflow_model_processing_result[5])            \n",
    "            if tensorflow_model_processing_result[5] > moving_avg_searchbreak:\n",
    "                print('\\n\\n\\n\\n\\n########## Moving average searching: Break ##########\\n\\n\\n\\n\\n')\n",
    "                break                \n",
    "        except:\n",
    "            print(f'Except >>> {days} moving average')\n",
    "            pass\n",
    "\n",
    "    df = pd.DataFrame(mvg_tune)\n",
    "    df.columns = ['moving_avg', 'hidden_activation', 'output_activation', 'loss_compile', 'loss', 'fbeta', 'result']\n",
    "    \n",
    "    df = df.sort_values('loss').reset_index(drop = True)\n",
    "    df.to_csv('para_mvg_tuned.csv', index = False)\n",
    "    return df\n",
    "\n",
    "def main_param():\n",
    "    voice_message(\"\"\"\\\n",
    "    TensorFlow optimizing starts\"\"\")\n",
    "    from os.path import exists\n",
    "    snp_source_exists = exists('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    if snp_source_exists == True:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    else:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_snp.csv')\n",
    "        \n",
    "    # parameter optimize\n",
    "    df = df_source.copy()\n",
    "    df = pd.DataFrame(param_optimizer(df))\n",
    "    df.columns = ['moving_avg', 'hidden_activation', 'output_activation', 'loss_compile', 'loss', 'fbeta', 'result']\n",
    "    df = df.sort_values('loss').reset_index(drop = True)\n",
    "\n",
    "    param_top_ten = df[df['loss'] == df.head(1).loss.values[0]].sort_values('loss').reset_index(drop = True).head(10)\n",
    "    param_top_ten.to_csv('param_top_ten.csv', index = False)\n",
    "    print(param_top_ten)\n",
    "    \n",
    "    hidden_activation = param_top_ten.hidden_activation[0] # hidden_activation\n",
    "    output_activation = param_top_ten.output_activation[0] # output_activation\n",
    "    loss = param_top_ten.loss_compile[0]      # loss_compile\n",
    "    \n",
    "    print('\\n\\n\\n##################################################')\n",
    "    print(f'• Hidden Layer Activator ==> {hidden_activation}')\n",
    "    print(f'• Output Layer Activator ==> {output_activation}')\n",
    "    print(f'• Loss Compiler          ==> {loss}')\n",
    "    print('##################################################\\n\\n\\n\\n\\n')\n",
    "    \n",
    "    # moving average optimize\n",
    "    final_opt_df = mvg_optimizer(df_source, hidden_activation, output_activation, loss)  \n",
    "    df = final_opt_df.copy()\n",
    "    print(df.sort_values('loss').reset_index(drop = True))\n",
    "    \n",
    "    # plot\n",
    "    df = df.sort_values('moving_avg', ascending = False)\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 9)\n",
    "\n",
    "    ax.plot(df.moving_avg, df.fbeta, color = 'blue', marker = 'X')\n",
    "    ax.set_xlabel('Moving Average (days)')\n",
    "    ax.set_ylabel('fbeta', color = 'blue')\n",
    "    # ax.axvline(x = 5, color = 'black', linestyle = '--')\n",
    "    # ax.grid(axis = 'x')\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(df.moving_avg, df.loss, color = 'red', marker = 'X')\n",
    "    ax1.set_ylabel('Loss', color = 'red')\n",
    "    ax1.grid(axis = 'y')\n",
    "\n",
    "    # ax.set_xticklabels(df.Date, rotation = 90)\n",
    "    plt.show()\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    print('> Modules #4 imported')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35a056d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pred_tensorflow(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, pred_features):\n",
    "    with tf.device('/GPU:0'):\n",
    "        \n",
    "        # parameter input\n",
    "        opt_param_verified_result = opt_param_verified()\n",
    "        hidden_activation = opt_param_verified_result[0]\n",
    "        output_activation = opt_param_verified_result[1]\n",
    "        loss_compile = opt_param_verified_result[2]        \n",
    "#         print(f'• Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Loss Compiler                    ==> {loss_compile}\\n')\n",
    "        \n",
    "        # initiate\n",
    "        tf.keras.backend.clear_session() \n",
    "        \n",
    "        # build model\n",
    "        layer_neurons = [256, 128, 64, 32, 16, 8]\n",
    "        input_layers_features = tf_X_train.shape[1]\n",
    "        output_layers_features = 1\n",
    "        \n",
    "        # model design\n",
    "        tf_model = tf.keras.Sequential()\n",
    "        tf_model.add(tf.keras.layers.Flatten(input_shape=(input_layers_features, 1)))\n",
    "        for neurons in layer_neurons:\n",
    "            tf_model.add(tf.keras.layers.Dense(neurons, activation = hidden_activation))\n",
    "            tf_model.add(tf.keras.layers.Dropout(0.2))\n",
    "        tf_model.add(tf.keras.layers.Dense(output_layers_features, activation = output_activation))\n",
    "        \n",
    "        # compile\n",
    "        tf_model.compile(optimizer='adam', \n",
    "                         loss = loss_compile, \n",
    "                         metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=0.5, threshold=0.5)])\n",
    "\n",
    "        # Stop training when there is no improvement in the validation loss for n consecutive epochs\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_fbeta_score', patience = 10)\n",
    "\n",
    "        # Save the Model with the lowest validation loss\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                                       monitor = 'val_fbeta_score',\n",
    "                                                       save_best_only=True)\n",
    "\n",
    "#         # evaluate loss and fbeta before tuning\n",
    "#         loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'\\n\\nTensorFlow Model Evalution before training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "\n",
    "        # train the model\n",
    "        EPOCHS = 500    \n",
    "        history = tf_model.fit(tf_X_train, tf_y_train, \n",
    "                               epochs = EPOCHS, \n",
    "                               validation_data = (tf_X_val, tf_y_val), \n",
    "                               batch_size = 8, \n",
    "                               verbose = 0, \n",
    "                               callbacks = [early_stopping, save_best])\n",
    "        \n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1, callbacks = [early_stopping])\n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1)\n",
    "\n",
    "        # evaluate loss and fbeta after tuning\n",
    "        loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'TensorFlow Model Evalution after training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "#         print(f'• Applied_Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Applied_Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Applied_Loss Compiler                    ==> {loss_compile}\\n')\n",
    "#         print(f'• Applied_Moving Average Value (optimized) ==> {moving_avg_value} days\\n')\n",
    "\n",
    "        # predict up or down\n",
    "        prediction = tf_model.predict(pred_features)\n",
    "    \n",
    "    return [fbeta[0], prediction[0][0]]\n",
    "\n",
    "\n",
    "def pred_tensorflow_LSTM(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, pred_features):\n",
    "    with tf.device('/GPU:0'):\n",
    "        \n",
    "        # parameter input\n",
    "        opt_param_verified_result = opt_param_verified()\n",
    "        hidden_activation = opt_param_verified_result[0]\n",
    "        output_activation = opt_param_verified_result[1]\n",
    "        loss_compile = opt_param_verified_result[2]        \n",
    "#         print(f'• Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Loss Compiler                    ==> {loss_compile}\\n')\n",
    "        \n",
    "        # initiate\n",
    "        tf.keras.backend.clear_session() \n",
    "        \n",
    "        # build model\n",
    "        layer_neurons = [256, 128, 64, 32, 16, 8]\n",
    "        input_layers_features = tf_X_train.shape[1]\n",
    "        output_layers_features = 1\n",
    "        \n",
    "#         # model design\n",
    "#         tf_model = tf.keras.Sequential()\n",
    "#         tf_model.add(tf.keras.layers.Flatten(input_shape=(input_layers_features, 1)))\n",
    "#         for neurons in layer_neurons:\n",
    "#             tf_model.add(tf.keras.layers.Dense(neurons, activation = hidden_activation))\n",
    "#             tf_model.add(tf.keras.layers.Dropout(0.2))\n",
    "#         tf_model.add(tf.keras.layers.Dense(output_layers_features, activation = output_activation))\n",
    "\n",
    "        # LSTM model design\n",
    "        tf_model = tf.keras.Sequential()    \n",
    "        tf_model.add(LSTM(100, return_sequences = True, input_shape = (input_layers_features, 1)))\n",
    "        tf_model.add(LSTM(100, return_sequences = False))\n",
    "        tf_model.add(Dense(25))\n",
    "        tf_model.add(Dense(output_layers_features, activation = output_activation))\n",
    "        \n",
    "        # compile\n",
    "        tf_model.compile(optimizer='adam', \n",
    "                         loss = loss_compile, \n",
    "                         metrics=[tfa.metrics.FBetaScore(num_classes=1, beta=0.5, threshold=0.5)])\n",
    "\n",
    "        # Stop training when there is no improvement in the validation loss for n consecutive epochs\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_fbeta_score', patience = 10)\n",
    "\n",
    "        # Save the Model with the lowest validation loss\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('./best_model.h5',\n",
    "                                                       monitor = 'val_fbeta_score',\n",
    "                                                       save_best_only=True)\n",
    "\n",
    "#         # evaluate loss and fbeta before tuning\n",
    "#         loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'\\n\\nTensorFlow Model Evalution before training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "\n",
    "        # train the model\n",
    "        EPOCHS = 500    \n",
    "        history = tf_model.fit(tf_X_train, tf_y_train, \n",
    "                               epochs = EPOCHS, \n",
    "                               validation_data = (tf_X_val, tf_y_val), \n",
    "                               batch_size = 8, \n",
    "                               verbose = 0, \n",
    "                               callbacks = [early_stopping, save_best])\n",
    "        \n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1, callbacks = [early_stopping])\n",
    "        # history = tf_model.fit(tf_X_train, tf_y_train, epochs = EPOCHS, validation_data = (tf_X_val, tf_y_val), batch_size = 8, verbose = 1)\n",
    "\n",
    "        # evaluate loss and fbeta after tuning\n",
    "        loss, fbeta = tf_model.evaluate(tf_X_test, tf_y_test, verbose = 0)\n",
    "#         print(f'TensorFlow Model Evalution after training\\n• Loss: {loss}\\n• fbeta: {fbeta}\\n\\n')\n",
    "#         print(f'• Applied_Hidden Layer Activator           ==> {hidden_activation}')\n",
    "#         print(f'• Applied_Output Layer Activator           ==> {output_activation}')\n",
    "#         print(f'• Applied_Loss Compiler                    ==> {loss_compile}\\n')\n",
    "#         print(f'• Applied_Moving Average Value (optimized) ==> {moving_avg_value} days\\n')\n",
    "\n",
    "        # predict up or down\n",
    "        prediction = tf_model.predict(pred_features)\n",
    "    \n",
    "    return [fbeta[0], prediction[0][0]]\n",
    "\n",
    "\n",
    "def pred_logi(X_train, X_test, y_train, y_test, pred_features):\n",
    "    pipe = Pipeline(steps = [('classifier', LogisticRegression(random_state = 0))])\n",
    "\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred) \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = pipe.predict_proba(pred_features)[:,1][0]\n",
    "    \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_svc(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_svc = SVC(random_state = 0, probability = True)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {\"C\": [0.1, 0.5, 1, 3, 5],\n",
    "                  \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                  \"degree\": [1, 4]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "    \n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_svc, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)   \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "     \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_rf(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_rf = RandomForestClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {\"max_depth\": [3, None],\n",
    "                  \"n_estimators\": list(range(10, 200)),\n",
    "                  \"max_features\": list(range(1, X_test.shape[1]+1)),\n",
    "                  \"min_samples_split\": list(range(2, 11)),\n",
    "                  \"min_samples_leaf\": list(range(1, 11)),\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "    \n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_rf, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)   \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "     \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_ada(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_ada = AdaBoostClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    # look at  setting up your search for n_estimators, learning_rate\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "    param_dist = {\"n_estimators\": [10, 100, 200, 400],\n",
    "                  \"learning_rate\": [0.001, 0.005, .01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 1, 2, 10, 20]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "    \n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_ada, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)       \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "    \n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_grd(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_grd = GradientBoostingClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {'learning_rate': [0.01, 0.02, 0.03],\n",
    "                  'subsample'    : [0.9, 0.5, 0.2],\n",
    "                  'n_estimators' : [100, 500, 1000], \n",
    "                  'max_depth'    : [4, 6, 8]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "\n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_grd, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)       \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "\n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_xgb(X_train, X_test, y_train, y_test, pred_features):\n",
    "    clf_xg = xgb.XGBClassifier(random_state = 0)\n",
    "\n",
    "    # Set up the hyperparameter search\n",
    "    param_dist = {'learning_rate'   : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30],\n",
    "                  'max_depth'       : [3, 4, 5, 6, 8, 10, 12, 15], \n",
    "                  'min_child_weight': [1, 3, 5, 7],\n",
    "                  'gamma'           : [0.0, 0.1, 0.2, 0.3, 0.4],\n",
    "                  'colsample_bytree': [0.3, 0.4, 0.5, 0.7]}\n",
    "    \n",
    "    scorer = make_scorer(fbeta_score, beta = 0.5)\n",
    "\n",
    "    # Run a randomized search over the hyperparameters\n",
    "    random_search = RandomizedSearchCV(estimator = clf_xg, \n",
    "                                       param_distributions = param_dist,\n",
    "                                       scoring = scorer,\n",
    "                                       cv = 2, \n",
    "                                       n_iter = 10, \n",
    "                                       n_jobs = -1)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "    # reflect Proba Conversion Rate\n",
    "    y_pred = random_search.best_estimator_.predict_proba(X_test)[:,1]\n",
    "    y_pred[y_pred > predict_proba_value] = 1\n",
    "    y_pred[y_pred <= predict_proba_value] = 0\n",
    "    y_pred = pd.Series(y_pred)       \n",
    "\n",
    "    fbeta = fbeta_score(y_test, y_pred, beta=0.5)\n",
    "    prediction = random_search.best_estimator_.predict_proba(pred_features)[:,1][0]\n",
    "\n",
    "    return [fbeta, prediction]\n",
    "\n",
    "\n",
    "def pred_lr_opt(df):\n",
    "    df = df.drop(columns = ['Date'])\n",
    "\n",
    "    # features to predict\n",
    "    pred_features = df[-1:].drop(columns = ['Stock_price'])\n",
    "\n",
    "    # features to train &test\n",
    "    df = df[:-1]         \n",
    "\n",
    "    # outcome \n",
    "    outcomes = df.pop('Stock_price').values\n",
    "    # filter out unwanted columns\n",
    "    features = df.values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 0)\n",
    "\n",
    "    mse_opt = {}\n",
    "    for degree_value in range(1,4):    \n",
    "        # find the poly degree having the lowest MSE\n",
    "        poly_feat = PolynomialFeatures(degree = degree_value)\n",
    "        X_train_poly = poly_feat.fit_transform(X_train)\n",
    "        X_test_poly = poly_feat.transform(X_test)\n",
    "        poly_model = LinearRegression(fit_intercept = False).fit(X_train_poly, y_train)\n",
    "        y_pred = poly_model.predict(X_test_poly)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        mse_opt[degree_value] = mse\n",
    "\n",
    "    mse = pd.DataFrame([mse_opt]).T\n",
    "    mse.rename(columns = {0:'mse'}, inplace = True)\n",
    "    mse = mse.reset_index()\n",
    "    degree_opt = mse[mse['mse'] == mse.mse.min()]['index'].values[0]\n",
    "\n",
    "    poly_feat = PolynomialFeatures(degree = degree_opt)\n",
    "    X_train_poly = poly_feat.fit_transform(X_train)\n",
    "    X_test_poly = poly_feat.transform(X_test)\n",
    "    poly_model = LinearRegression(fit_intercept = False).fit(X_train_poly, y_train)\n",
    "    y_pred = poly_model.predict(X_test_poly)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    pred_features_poly = poly_feat.fit_transform(pred_features)\n",
    "    prediction = float(poly_model.predict(pred_features_poly)[0])\n",
    "    return [int(features.shape[0]-1), degree_opt, r2, mse, prediction]\n",
    "\n",
    "\n",
    "def pred_lr(df):\n",
    "    opt_tail_value = {}\n",
    "    for i in range(df.shape[0]-30, df.shape[0]):\n",
    "        pred_lr_opt_result = pred_lr_opt(df.tail(i))\n",
    "        opt_tail_value[i] = pred_lr_opt_result[3]\n",
    "#         if pred_lr_opt_result[3] < pred_lr_opt_result[4]*0.03:\n",
    "#             break\n",
    "\n",
    "    tail_value = pd.DataFrame([opt_tail_value]).T\n",
    "    tail_value.rename(columns = {0:'mse'}, inplace = True)\n",
    "    tail_value = tail_value.reset_index()\n",
    "    tail_value = tail_value[tail_value['mse'] == tail_value.mse.min()]['index'].values[0]\n",
    "        \n",
    "    lr_result = pred_lr_opt(df.tail(tail_value))\n",
    "    print('\\n\\n#################################')\n",
    "    print(f'• No. Observations: {lr_result[0]} records')\n",
    "    print(f'• Most recent {tail_value} days')\n",
    "    print(f'• Poly degree: {lr_result[1]}')\n",
    "    print(f'• R squared: {lr_result[2]}')\n",
    "    print(f'• MSE: {lr_result[3]}')\n",
    "    print(f'• Prediction: {lr_result[4]}')    \n",
    "    print('#################################')\n",
    "        \n",
    "    return lr_result[4]\n",
    "\n",
    "\n",
    "def pred_lstm(df):\n",
    "    df = df.drop(columns=['Date'])\n",
    "    df = df[['Stock_price']]  # We'll only use the 'Stock_price' column for simplicity\n",
    "\n",
    "    # Normalize data using Min-Max scaling\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "    # Prepare sequences and labels\n",
    "    sequence_length = 30  # Number of past days to consider for prediction\n",
    "    X, y = [], []\n",
    "    for i in range(len(df_scaled) - sequence_length):\n",
    "        X.append(df_scaled[i:i + sequence_length])\n",
    "        y.append(df_scaled[i + sequence_length])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Build the LSTM model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "    # Evaluate the model\n",
    "    mse = model.evaluate(X_test, y_test, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Make a prediction for the next day\n",
    "    last_sequence = df_scaled[-sequence_length:]\n",
    "    last_sequence = last_sequence.reshape((1, sequence_length, 1))\n",
    "    prediction = model.predict(last_sequence)\n",
    "    prediction = scaler.inverse_transform(prediction)[0][0]\n",
    "\n",
    "    print('\\n\\n#################################')\n",
    "    print(f'• No. Observations: {int(df.shape[0]-1)} records')\n",
    "    print(f'• R squared: {r2}')\n",
    "    print(f'• MSE: {mse}')\n",
    "    print(f'• Prediction: {prediction}')    \n",
    "    print('#################################')\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "def classifer_data_input(df, moving_avg_value):\n",
    "\n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0]) \n",
    "\n",
    "    # ML-Classifier\n",
    "    # Get moving average\n",
    "    mvp = moving_avg_value\n",
    "    mavg = pd.DataFrame()\n",
    "    for column in df.columns[1:]:\n",
    "        mv_change = np.array(df[column])\n",
    "        mv = []\n",
    "        for i in range(len(mv_change)-mvp+1):\n",
    "            mv.append(np.average(mv_change[i:mvp+i]))\n",
    "            i+=1\n",
    "        mavg[column] = pd.DataFrame(mv)\n",
    "    df = mavg\n",
    "\n",
    "    # features to predict\n",
    "    pred_features = df[-1:]\n",
    "\n",
    "    # get the outcome from the tomorrow price\n",
    "    df['Tmr_price'] = df['Stock_price'].shift(-1)\n",
    "    df['classifier_result'] = (df['Tmr_price'] > df['Stock_price']).astype(int)\n",
    "    df.drop(columns = 'Tmr_price', inplace = True)\n",
    "\n",
    "    # features to train &test\n",
    "    df = df[:-1]\n",
    "\n",
    "    # outcome \n",
    "    outcomes = df.pop('classifier_result').values\n",
    "    # filter out unwanted columns\n",
    "    features = df.values\n",
    "\n",
    "    # for non-tf\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size=0.2, random_state = 42, stratify = outcomes)\n",
    "    sc = MinMaxScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)  \n",
    "    non_tf_pred_features = sc.transform(pred_features)\n",
    "\n",
    "    # for tf\n",
    "    tf_X_train, tf_X_test, tf_y_train, tf_y_test = train_test_split(features, outcomes, test_size=0.2, random_state= 42, stratify = outcomes)\n",
    "\n",
    "    tf_X_train = sc.transform(tf_X_train)\n",
    "    tf_X_test = sc.transform(X_test[:int(len(X_test)/2)])\n",
    "    tf_X_val = sc.transform(X_test[int(len(X_test)/2):])\n",
    "    tf_y_test = y_test[:int(len(y_test)/2)]\n",
    "    tf_y_val = y_test[int(len(y_test)/2):] \n",
    "    tf_pred_features = sc.transform(pred_features)\n",
    "\n",
    "    print('\\n')\n",
    "    \n",
    "    print('tf DNN', end = '                               \\r')\n",
    "    tensorflow = pred_tensorflow(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, tf_pred_features)\n",
    "    print(end = '                                     \\r')    \n",
    "    \n",
    "    print('tf LSTM', end = '                               \\r')\n",
    "    tensorflow_LSTM = pred_tensorflow_LSTM(tf_X_train, tf_y_train, tf_X_val, tf_y_val, tf_X_test, tf_y_test, tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('logi', end = '                               \\r')\n",
    "    logi = pred_logi(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('svc', end = '                               \\r')\n",
    "    svc = pred_svc(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('rf', end = '                               \\r')\n",
    "    rf = pred_rf(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('ada', end = '                               \\r')\n",
    "    ada = pred_ada(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')\n",
    "    \n",
    "    print('grd', end = '                               \\r')\n",
    "    grd = pred_grd(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')    \n",
    "\n",
    "    print('xgb', end = '                               \\r')\n",
    "    grd = pred_xgb(X_train, X_test, y_train, y_test, non_tf_pred_features)\n",
    "    print(end = '                                     \\r')        \n",
    "\n",
    "    result = {}\n",
    "    result['tensorflow_DNN'] = tensorflow[1]\n",
    "    result['tensorflow_LSTM'] = tensorflow_LSTM[1]\n",
    "    result['logi'] = logi[1]\n",
    "    result['svc'] = svc[1]\n",
    "    result['rf'] = rf[1]\n",
    "    result['ada'] = ada[1]\n",
    "    result['grd'] = grd[1]\n",
    "    result['xgb'] = grd[1]\n",
    "\n",
    "    fbeta = {}\n",
    "    fbeta['tensorflow_DNN'] = tensorflow[0]\n",
    "    fbeta['tensorflow_LSTM'] = tensorflow_LSTM[0]\n",
    "    fbeta['logi'] = logi[0]\n",
    "    fbeta['svc'] = svc[0]\n",
    "    fbeta['rf'] = rf[0]\n",
    "    fbeta['ada'] = ada[0]\n",
    "    fbeta['grd'] = grd[0]    \n",
    "    fbeta['xgb'] = grd[0]   \n",
    "\n",
    "    rank = pd.DataFrame([fbeta, result]).T.rename(columns = {0:'fbeta' , 1:'result'})\n",
    "    rank.sort_values('fbeta', ascending = False, inplace = True)\n",
    "    print(f'\\n\\n• Moving Average Days: {mvp}')\n",
    "    print('====================================\\n', rank)\n",
    "    print('====================================')\n",
    "    \n",
    "    print('\\n========[Bullish Conditions]========')\n",
    "    print(f'• Min. fbeta value:  {np.round(modelchoice_minimum_fbeta, 2)}')\n",
    "    print(f'• Min. Result value: {np.round(predict_proba_value, 2)}')\n",
    "    print('------------------------------------')\n",
    "    print(f'• Average fbeta: {min_average_fbeta}')\n",
    "    print(f'• Number of (+) Models: {min_num_positive_models}')\n",
    "    print(f'• Pred. (+) Percent: {min_positive_percent}')    \n",
    "    print('====================================')\n",
    "    rank = rank[rank['fbeta'] >= modelchoice_minimum_fbeta]\n",
    "    \n",
    "    # Proba conversion\n",
    "    rank_prediction = []\n",
    "    for result_prediction in rank['result'].values:\n",
    "        if result_prediction >= np.round(predict_proba_value, 2):\n",
    "            rank_prediction.append(1)\n",
    "        else:\n",
    "            rank_prediction.append(0)\n",
    "    rank['sign'] = rank_prediction\n",
    "    \n",
    "    print(rank)\n",
    "    \n",
    "    average_fbeta = np.round(rank.fbeta.mean(), 2)\n",
    "    print('\\n==============[Result]==============')\n",
    "    print(f'• Average fbeta: {average_fbeta}')\n",
    "    \n",
    "    positive_modes = rank['sign'].sum()\n",
    "    print(f'• Number of (+) Models: {positive_modes}')\n",
    "    \n",
    "    average_result = np.round(rank['sign'].mean(), 2)\n",
    "    print(f'• Pred. (+) Percent: {average_result}')\n",
    "    print('====================================')\n",
    "    \n",
    "    if average_result >= min_positive_percent and positive_modes >= min_num_positive_models and average_fbeta > min_average_fbeta:\n",
    "        print(\"• Result: \" + '\\033[1m' + '(+)' + '\\033[0m')\n",
    "        print('====================================\\n\\n\\n')\n",
    "        return [average_fbeta, 1]\n",
    "    else:\n",
    "        print(\"• Result: \" + '\\033[1m' + '(-)' + '\\033[0m')\n",
    "        print('====================================\\n\\n\\n')\n",
    "        return [average_fbeta, 0]   \n",
    "\n",
    "    \n",
    "\n",
    "def opt_param_verified(): # decide parameters and moving average value\n",
    "    global moving_avg_value\n",
    "    \n",
    "    df = pd.read_csv('para_mvg_tuned.csv')\n",
    "    df.sort_values('loss', inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    hidden_activation = df.hidden_activation[0]\n",
    "    output_activation = df.output_activation[0]\n",
    "    loss_compile = df.loss_compile[0]\n",
    "\n",
    "#     # moving average value based on 'loss'\n",
    "#     df.sort_values('loss', inplace = True)\n",
    "#     df.reset_index(drop = True, inplace = True)\n",
    "#     moving_avg_value = df[df['loss'] == df.head(1).loss.values[0]].sort_values('moving_avg').head(1).moving_avg.values[0]\n",
    "#     with open(\"moving_avg_value.txt\", 'w') as f:\n",
    "#         f.write(str(moving_avg_value))\n",
    "        \n",
    "    # moving average value based on 'fbeta'   \n",
    "    df.sort_values('fbeta', ascending = False, inplace = True)\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    moving_avg_value = df[df['fbeta'] == df.head(1).fbeta.values[0]].sort_values('moving_avg').head(1).moving_avg.values[0]\n",
    "    if moving_avg_value < 0:\n",
    "        moving_avg_value = 1\n",
    "        \n",
    "    if moving_avg_value > moving_avg_value_max:\n",
    "        moving_avg_value = moving_avg_value_max \n",
    "    if moving_avg_value < moving_avg_value_min:\n",
    "        moving_avg_value = moving_avg_value_min \n",
    "    else:\n",
    "        pass        \n",
    "    with open(\"moving_avg_value.txt\", 'w') as f:\n",
    "        f.write(str(moving_avg_value))\n",
    "            \n",
    "    return [hidden_activation, output_activation, loss_compile, moving_avg_value]\n",
    "            \n",
    " \n",
    "def main_engine():    \n",
    "    print('\\u2022 TensorFlow version:', tf.__version__)\n",
    "    print('\\u2022 tf.keras version:', tf.keras.__version__)\n",
    "    print('\\u2022 Running on GPU' if tf.test.is_gpu_available() else '\\t\\u2022 GPU device not found. Running on CPU')\n",
    "\n",
    "    print('\\n\\nAlgorithm Test with S&P500 ...')\n",
    "    moving_avg_value = opt_param_verified()[3]\n",
    "\n",
    "    from os.path import exists\n",
    "    snp_source_exists = exists('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    if snp_source_exists == True:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    else:\n",
    "        df_source = pd.read_csv('predict_aim_sourcedata_snp.csv')\n",
    "\n",
    "    al_test_results = classifer_data_input(df_source, moving_avg_value)\n",
    "    if al_test_results[0] < minimum_fbeta:\n",
    "        print(f'\\n\\nExcept >>> The fbeta average is less than the minimum value:\\n{al_test_results[0]} ==(Improve)==> {minimum_fbeta}')\n",
    "        voice_message(\"\"\"\\\n",
    "        The highest prediction fbeta is less than the mininum value. Code cell break \"\"\")\n",
    "        # code cell break ###############\n",
    "        class StopExecution(Exception):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "\n",
    "        raise StopExecution\n",
    "        # code cell break ###############\n",
    "\n",
    "    else:\n",
    "        print('\\n\\nPrediction Result: Passed\\n\\n')\n",
    "        voice_message(\"\"\"\\\n",
    "        TensorFlow has been optimized\"\"\")\n",
    "        pass\n",
    "\n",
    "    param_result = opt_param_verified()\n",
    "    print(f'• Hidden_activation:   {param_result[0]}')\n",
    "    print(f'• Output_activation:   {param_result[1]}')\n",
    "    print(f'• Loss_compile:        {param_result[2]}')\n",
    "    print(f'• Moving Average Days: {param_result[3]}')\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('> Modules #5 imported')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c0473f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f46a2ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "# Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165769bf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def event_check():  \n",
    "   \n",
    "    event_schedule = {}\n",
    "    ############# Update Event #########################################\n",
    "    event_schedule['2022 FED December Projections 1st day'] = 20221213\n",
    "    event_schedule['2022 FED December Projections 2nd day'] = 20221214\n",
    "    event_schedule['2023 FED January'] = 20230131\n",
    "    event_schedule['2023 FED Feburary'] = 20230201\n",
    "    event_schedule['2023 FED March Projections 1st day'] = 20230321\n",
    "    event_schedule['2023 FED March Projections 2nd day'] = 20230322\n",
    "    event_schedule['2023 FED May 1st'] = 20230502\n",
    "    event_schedule['2023 FED May 2nd'] = 20230503\n",
    "    event_schedule['2023 FED June Projections 1st day'] = 20230613\n",
    "    event_schedule['2023 FED June Projections 2nd day'] = 20230614\n",
    "    event_schedule['2023 FED July 1st'] = 20230725\n",
    "    event_schedule['2023 FED July 2nd'] = 20230726\n",
    "    event_schedule['2023 FED September Projections 1st day'] = 20230919\n",
    "    event_schedule['2023 FED September Projections 2nd day'] = 20230920\n",
    "    event_schedule['2023 FED October'] = 20231031\n",
    "    event_schedule['2023 FED November'] = 20231101\n",
    "    event_schedule['2023 FED December Projections 1st day'] = 20231212\n",
    "    event_schedule['2023 FED December Projections 2nd day'] = 20231213\n",
    "    ####################################################################\n",
    "\n",
    "    event_schedule = pd.DataFrame([event_schedule]).T.rename(columns = {0:'Date'})\n",
    "    event_schedule.reset_index(inplace = True)\n",
    "    event_schedule.rename(columns = {'index':'Event'}, inplace = True)\n",
    "    event_schedule = event_schedule[event_schedule['Date'] >= currentdate_event]\n",
    "    event_day = str(event_schedule.Date.values[0])[0:4]+'-'+str(event_schedule.Date.values[0])[4:6]+'-'+str(event_schedule.Date.values[0])[6:8]  \n",
    "    event_day = datetime.strptime(event_day, '%Y-%m-%d').date()\n",
    "    currentdate_event_datetime = str(currentdate_event)[0:4]+'-'+str(currentdate_event)[4:6]+'-'+str(currentdate_event)[6:8]  \n",
    "    currentdate_event_datetime = datetime.strptime(currentdate_event_datetime, '%Y-%m-%d').date()\n",
    "    upcoming_remain = event_day - currentdate_event_datetime\n",
    "    try:\n",
    "        upcoming_remain = int(str(upcoming_remain)[:2])\n",
    "    except:\n",
    "        upcoming_remain = 0\n",
    "        \n",
    "    if timechecknow() < 10:\n",
    "        \n",
    "        voice_message(f\"\"\"\\\n",
    "        Today is {date.today()}\n",
    "\n",
    "        We have {num_of_purchased} stocks on hold.\n",
    "        The total unrealized performance average is at {stock_return_percent}%. \n",
    "\n",
    "        The upcoming event is {event_schedule[:1].Event.values[0]} \n",
    "        scheduled on {event_day} \n",
    "        which is in {upcoming_remain} days.\n",
    "        We are going to narrow down the mid-term gain percentage by {int(event_risk_ratio*100)}% during the day. \n",
    "\n",
    "        Sean, please stay positive and enjoy your progress. Here is a fortune message: {tarot()}\n",
    "        \"\"\")\n",
    "        time.sleep(5)\n",
    "        \n",
    "    else:\n",
    "        voice_message(f\"\"\"\\\n",
    "        We have {num_of_purchased} stocks on hold.\n",
    "        The total unrealized performance average is at {stock_return_percent}%. \n",
    "        \"\"\")\n",
    "        \n",
    "    time.sleep(5)\n",
    "        \n",
    "    return [upcoming_remain, event_schedule[:1].Event.values[0], event_day]\n",
    "\n",
    "\n",
    "def sean_index_plot():\n",
    "    \n",
    "    from os.path import exists\n",
    "    snp_source_exists = exists('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    if snp_source_exists == True:\n",
    "        df = pd.read_csv('predict_aim_sourcedata_monitoring_^GSPC_%s.csv' %currentdate)\n",
    "    else:\n",
    "        df = pd.read_csv('predict_aim_sourcedata_snp.csv')\n",
    "    df_predict = df.copy()\n",
    "    df.drop(columns = ['High', 'Low' , 'Open' , 'Volume'], inplace = True)\n",
    "\n",
    "    df_s = df.drop(['Date'], axis = 1)\n",
    "    col_names = df_s.columns\n",
    "\n",
    "    # Feature Scaling\n",
    "    sc = StandardScaler()\n",
    "    df_s = sc.fit_transform(df_s)\n",
    "    df_s = pd.concat([df['Date'], pd.DataFrame(df_s, columns = col_names)], axis = 1)\n",
    "\n",
    "    df_stats = df_s.copy()\n",
    "    df_stats['intercept'] = 1\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    lm = sm.OLS(df_stats['Stock_price'], df_stats[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "    results = lm.fit()\n",
    "    # print(results.summary())\n",
    "\n",
    "    df_s = df_s.drop(['Stock_price'], axis = 1)\n",
    "    coef = results.params[1:].reset_index().rename(columns = {0:'coef'})\n",
    "    for column in df_s.columns.tolist()[1:]:\n",
    "        df_s[column] = df_s[column] * coef[coef['index'] == column].values.tolist()[0][1]\n",
    "\n",
    "    Average_index = df_s.drop(columns = 'Date').sum(axis = 1) / 10\n",
    "    sean_index = pd.DataFrame()\n",
    "    sean_index['Date'] = df_s.Date\n",
    "    sean_index['Average_index'] = Average_index\n",
    "    sean_index = sean_index.reset_index(drop = True)\n",
    "    sean_index.to_csv('sean_index.csv', index = False)\n",
    "    snp500_today = pred_lstm(df_predict)\n",
    "\n",
    "    sean_index_plot = sean_index.tail(averageline)                 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    price_record = yfinance_df_setting('^GSPC').tail(averageline)  #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16, 9)\n",
    "    #     sean_index_plot['Date'] = sean_index_plot['Date'].astype('datetime64[ns]')\n",
    "\n",
    "    ax.plot(sean_index_plot['Date'], sean_index_plot['Average_index'], color = 'red')\n",
    "    ax.set_xlabel('\\nDate')\n",
    "    ax.set_ylabel('\\nPredicted S&P 500', color = 'red')\n",
    "    # ax.axvline(x = 1, color = 'black', linestyle = '--')\n",
    "    ax.grid()\n",
    "\n",
    "    ax1 = ax.twinx()\n",
    "    ax1.plot(sean_index_plot['Date'], price_record['Adj Close'], color = 'blue')\n",
    "    ax1.set_ylabel('\\nActual S&P 500', color = 'blue')\n",
    "\n",
    "    ax.set_xticklabels(sean_index_plot['Date'], rotation = 90)\n",
    "\n",
    "    plt.title('Sean Index')\n",
    "    plt.show()\n",
    "\n",
    "    # prediction accuracy\n",
    "    sean_index['Date'] = sean_index['Date'].astype('datetime64[ns]')\n",
    "    price_record = yfinance_df_setting('^GSPC')\n",
    "    df = pd.merge(sean_index, price_record, on = 'Date', how = 'left')\n",
    "    df.loc[df['Average_index'].shift(-1).diff() > 0, 'sean_diff'] = 1\n",
    "    df.loc[df['Average_index'].shift(-1).diff() <= 0, 'sean_diff'] = 0\n",
    "    df.loc[df['Adj Close'].shift(-1).diff() > 0, 'snp_diff'] = 1\n",
    "    df.loc[df['Adj Close'].shift(-1).diff() <= 0, 'snp_diff'] = 0\n",
    "    df.loc[df['sean_diff'] == df['snp_diff'], 'Accuracy'] = 1\n",
    "    df.loc[df['sean_diff'] != df['snp_diff'], 'Accuracy'] = 0\n",
    "    prediction_accuracy = df.Accuracy.mean()*100\n",
    "\n",
    "    if df[-2:].Average_index.diff().values[-1] > 0:\n",
    "        prediction_result = 'Up'\n",
    "    else:\n",
    "        prediction_result = 'Down'\n",
    "\n",
    "    print('==============[Result]==============')\n",
    "    # print(' • S&P 500 Actual:          ' + '%.2f' %analysis_price('^GSPC')[4])\n",
    "    # print(' • S&P 500 Prediction:      ' + '%.2f' %snp500_today)\n",
    "    print(' • Prediction Accuracy[%]:  ' + '%.2f' %prediction_accuracy)\n",
    "    print(' • Prediction Result:       ' + '\\033[1m' + prediction_result + '\\033[0m')\n",
    "    print('====================================')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # adjust below when selling\n",
    "    dead_stock = ['QCOM']\n",
    "\n",
    "    robin = robin_login() \n",
    "    num_of_purchased = len(robin['index'])\n",
    "    sellmonitor_freq_limit = 10     ###### the max number of stocks on hold\n",
    "    sellmonitor_freq = int((sellmonitor_freq_limit) - (num_of_purchased - len(dead_stock))) ### adjust \n",
    "\n",
    "    stock_return_percent = perf_cal(robin)\n",
    "    performance_updated_df = robin[['index', 'equity', 'equity_change', 'percent_change']]\n",
    "\n",
    "    Upcoming_Announce_Date = pd.read_csv('srank.csv')[['Symbol', 'Upcoming Announce Date']]\n",
    "    Upcoming_Announce_Date.rename(columns = {'Symbol':'index'}, inplace = True)\n",
    "    performance_updated_df = pd.merge(performance_updated_df, Upcoming_Announce_Date, on = 'index', how = 'left')\n",
    "    \n",
    "    performance_updated_df.sort_values('percent_change', ascending = False, inplace = True)\n",
    "    performance_updated_df = performance_updated_df.reset_index(drop = True)\n",
    "    equity_change = np.round(performance_updated_df['equity_change'].astype(float).sum(), 2)\n",
    "    performance_updated_df.to_csv('performance_updated_df.csv', index = False)\n",
    "    \n",
    "    vars = ['price', 'quantity', 'average_buy_price', 'equity', 'percent_change', \n",
    "            'intraday_percent_change', 'equity_change', 'pe_ratio', 'percentage']\n",
    "    for var in vars:\n",
    "        robin[var] = pd.to_numeric(robin[var])\n",
    "    robin['quantity'] = robin['quantity'].astype(int)\n",
    "    robin.to_csv('robin_all.csv', index = False)\n",
    "    portfolio_list = robin['index'].values.tolist()\n",
    "\n",
    "\n",
    "    \n",
    "#___< Buy condition >________________________________________________________________________________________\n",
    "\n",
    "    # order limit$\n",
    "    #########################################################################################################\n",
    "    dollar_limit_order_dollar = 100 # must be less than to equal 500 per day trading\n",
    "    #########################################################################################################\n",
    "    \n",
    "    # split condition dollar amount\n",
    "    #########################################################################################################\n",
    "    split_condition_dollar = 100\n",
    "    #########################################################################################################\n",
    "    \n",
    "    # Current Exchange\n",
    "    #########################################################################################################\n",
    "    krwex = [exchange_check()*0.995] # Currency exchange alarm\n",
    "    #########################################################################################################\n",
    "    \n",
    "\n",
    "    # averageline for volatility, candle, and etc.\n",
    "    #########################################################################################################\n",
    "    averageline = 70 #average price to guess trend  \n",
    "    with open(\"averageline.txt\", 'w') as f:\n",
    "        f.write(str(averageline))\n",
    "    #########################################################################################################\n",
    "\n",
    "    \n",
    "    # event\n",
    "    #########################################################################################################\n",
    "    # fed event gain cut\n",
    "    event_risk_ratio = 0.7\n",
    "    \n",
    "    event_check_result = event_check()\n",
    "    upcoming_remain = event_check_result[0]\n",
    "    upcoming_event = event_check_result[1]\n",
    "    eventdate = event_check_result[2]\n",
    "    #########################################################################################################\n",
    "    \n",
    "    \n",
    "    # Financial Statement Filtering \n",
    "    #########################################################################################################\n",
    "    # 'snp500_xxxx-xx-xx.csv' comes from SeekingAlpha\n",
    "    # 'srank' after regression analysis; SeanScore\n",
    "    # 'slist_xxxx-xx-xx.csv' is filtered by YahooFinance\n",
    "    \n",
    "    bluechips = 400 # initial setting\n",
    "    \n",
    "    buy_sign_lookback_days = 252\n",
    "    \n",
    "    superpass_buysign_lookbackdays = 3\n",
    "    \n",
    "    mdd_lookback_days = 120\n",
    "    \n",
    "    corr = 0 # slist corr. (default: 1st)\n",
    "        \n",
    "    fs_cut = 93 # Financial Statement Filtering\n",
    "    fs_cut_plus = 3 # weight on focal indexes\n",
    "    \n",
    "    vola_cut_rate_value = 2\n",
    "    \n",
    "    portfolio_sector_ticker = 1.02\n",
    "    \n",
    "    rsi_max_cut = 65\n",
    "    \n",
    "    volume_ticker_percent = 0.1\n",
    "    #########################################################################################################\n",
    "    \n",
    "    # Buy condition\n",
    "    #########################################################################################################\n",
    "    buymonitor_blacklist = ['GOOG']# not for perf_reveiw** srank.csv\n",
    "                                    \n",
    "    inv_tickers = ['GLD', 'USO', 'BTC-USD']\n",
    "    with open(\"inv_tickers.txt\", 'w') as f:\n",
    "        for s in inv_tickers:  \n",
    "            f.write(str(s) + '\\n')  \n",
    "\n",
    "    with open(\"buymonitor_blacklist.txt\", 'w') as f:\n",
    "        for s in buymonitor_blacklist:  \n",
    "            f.write(str(s) + '\\n')  \n",
    "\n",
    "    with open(\"buymonitor_blacklist_original.txt\", 'w') as f:\n",
    "        for s in buymonitor_blacklist:  \n",
    "            f.write(str(s) + '\\n')   \n",
    "\n",
    "    df = pd.read_csv('srank.csv') # Top 5 market cap tickers\n",
    "    buymonitor_whitelist = df.sort_values('1M Perf', ascending = False\n",
    "                                         ).head()['Symbol'].tolist() \n",
    "    #########################################################################################################\n",
    "\n",
    "    \n",
    "\n",
    "#___< Sell condition >_______________________________________________________________________________________\n",
    "\n",
    "    # holding period sets in Strategy Def()\n",
    "    #########################################################################################################\n",
    "    # filter out stocks within the min. holding period\n",
    "    current_index = sorted(robin['index'].tolist())\n",
    "    holding_period = pd.read_csv('holding_period.csv')\n",
    "    holding_period['days_left'] = holding_period['days'] - min_holding_period\n",
    "    holding_period_index = sorted(holding_period[holding_period['days'] >= min_holding_period]['index'].tolist())\n",
    "    holding_period_index = [i for i in holding_period_index if i in current_index]\n",
    "    #########################################################################################################\n",
    "    \n",
    "  \n",
    "    # realize \n",
    "    ######################################################################################################### \n",
    "    # limit% for gain range and buying range\n",
    "    snp_yfinance = yfinance_df_setting('^GSPC')\n",
    "    snp_20 = snp_yfinance.tail(20)['Adj Close'].mean()\n",
    "    snp_5 = snp_yfinance.tail(5)['Adj Close'].mean()\n",
    "    snptrend = np.round(((snp_5 - snp_20)/snp_20),2)\n",
    "\n",
    "    mid_gain_adjust_percent = 0.2\n",
    "\n",
    "    mid_gain_market_reflect_percent_by_etf = 0.1\n",
    "\n",
    "    realize_gain_min = 1\n",
    "    \n",
    "    realize_gain_max = 3\n",
    "  \n",
    "    buy_limit_rate = 5+snptrend*10\n",
    "\n",
    "    buy_snp_t_y = [np.round(buy_limit_rate/2, 2)] \n",
    "\n",
    "    # Gold Buy price change limit\n",
    "    buy_gold_t_y = [np.round(buy_limit_rate/2.2, 2)] \n",
    "\n",
    "    # Oil Buy price change limit\n",
    "    buy_oil_t_y = [np.round(buy_limit_rate/2.2, 2)] \n",
    "\n",
    "    # Bitcoin Buy price change limit\n",
    "    buy_bit_t_y = [np.round(buy_limit_rate/2.2, 2)] \n",
    "    #########################################################################################################\n",
    "\n",
    "\n",
    "    # loss-cut split sales \n",
    "    #########################################################################################################\n",
    "    sale_split = 3\n",
    "    #########################################################################################################\n",
    "   \n",
    "\n",
    "    \n",
    "#___< ML Model >_____________________________________________________________________________________________\n",
    "\n",
    "    ######################################################################################################### \n",
    "    # TF optimizing \n",
    "    with open(\"moving_avg_value.txt\", 'r') as f:\n",
    "        moving_avg_value = [line.rstrip('\\n') for line in f]\n",
    "        moving_avg_value = int(moving_avg_value[0]) \n",
    "    \n",
    "    # fbeta - loop break threshold \n",
    "    moving_avg_searchbreak = 0.62\n",
    "    \n",
    "    moving_avg_value_min = 3\n",
    "    \n",
    "    moving_avg_value_max = 5\n",
    "    \n",
    "    if moving_avg_value > moving_avg_value_max:\n",
    "        print(f'Except >>> Moving Average Value exceeds the limit of {moving_avg_value_max} days')\n",
    "        voice_message(f\"\"\"\\\n",
    "                      Moving Average Value exceeds the limit of {moving_avg_value_max} days\"\"\")  \n",
    "    if moving_avg_value < moving_avg_value_min:\n",
    "        print(f'Except >>> Moving Average Value less then the limit of {moving_avg_value_min} days')\n",
    "        voice_message(f\"\"\"\\\n",
    "                      Moving Average Value less then the limit of {moving_avg_value_min} days\"\"\")  \n",
    "    \n",
    "    # min. fbeta score when optimizing combined model score only (fixed var.)\n",
    "    minimum_fbeta = 0.60\n",
    "\n",
    "    if upcoming_remain >= 0 and upcoming_remain <= 5:\n",
    "        \n",
    "        # candle\n",
    "        VC_per_PC_limit_V = 300000\n",
    "        PC_limit_V = 0.000001\n",
    "        VS_vs_MEAN_limit = 0.8\n",
    "        VS_Accel_limit = 25\n",
    "\n",
    "        # proba (+) conversion rate for y_pred\n",
    "        proba_initial_value = 0.57 #<=== Important\n",
    "        \n",
    "        # min. fbeta score when choosing model to predict\n",
    "        initial_modelchoice_minimum_fbeta = 0.57 #<=== Important\n",
    "        \n",
    "        # aggregrated min. percentage to convert (+) sign    \n",
    "        min_positive_percent = 0.57 #<=== Important \n",
    "        \n",
    "        # min. aggregated average fbeta\n",
    "        min_average_fbeta = 0.62 #<=== Important (*if the actual fbeta is low, increase)      \n",
    "        \n",
    "    else:\n",
    "        # candle\n",
    "        VC_per_PC_limit_V = 1000\n",
    "        PC_limit_V = 0.000001\n",
    "        VS_vs_MEAN_limit = 0.5\n",
    "        VS_Accel_limit = 25\n",
    "        \n",
    "        proba_initial_value = 0.55 #<=== Important    \n",
    "        initial_modelchoice_minimum_fbeta = 0.55 #<=== Important\n",
    "        min_positive_percent = 0.55 #<=== Important \n",
    "        min_average_fbeta = 0.60 #<=== Important (*if the actual fbeta is low, increase)   \n",
    "        \n",
    "    # fixed variable: 1\n",
    "    min_num_positive_models = 3\n",
    "    \n",
    "    # not fixed variables: 4\n",
    "    modelchoice_minimum_fbeta = initial_modelchoice_minimum_fbeta\n",
    "    proba_initial_value_original = proba_initial_value\n",
    "    predict_proba_value = proba_initial_value \n",
    "    # Min / Max thresholds\n",
    "    modelchoice_minimum_fbeta_min = 0.50\n",
    "    modelchoice_minimum_fbeta_max = 0.65\n",
    "    predict_proba_value_min = 0.50\n",
    "    predict_proba_value_max = 0.65\n",
    "    \n",
    "    # S&P 500 Check alarm drop% \n",
    "    check_snp_t_y_original = [-1.0] \n",
    "    check_snp_t_y = check_snp_t_y_original # proba value is adjusted by snp drop%\n",
    "    \n",
    "    # buy valuecheck >= x (valuecheck[0] + valuecheck[1]*3)    \n",
    "    valuation_conversion_value_buy = 2.6\n",
    "    \n",
    "    # buy valuecheck >= x (valuecheck[0] + valuecheck[1]*3)    \n",
    "    valuation_conversion_value_etf_buy = 3\n",
    "\n",
    "    # sell valuecheck < x (valuecheck[0] + valuecheck[1]*3)  \n",
    "    valuation_conversion_value_sell = 3\n",
    "\n",
    "    # losscut valuecheck < x (valuecheck[0] + valuecheck[1]*3)  for tickers_to_sell_small\n",
    "    valuation_conversion_value_losscut = 2    \n",
    "    \n",
    "    # losscut valuecheck == x (valuecheck[0] + valuecheck[1]*3)  for all tickers\n",
    "    valuation_conversion_value_losscut_all = 1  \n",
    "    #########################################################################################################\n",
    "    \n",
    "    for stock in holding_period['index'].tolist():\n",
    "        if stock in current_index:\n",
    "            pass\n",
    "        else:\n",
    "            alarm()\n",
    "            print('Except >>>' + f'{stock} does not exist in Robinhood.')\n",
    "    \n",
    "    \n",
    "#___< Display >_____________________________________________________________________________________________\n",
    "    #########################################################################################################\n",
    "    print('\\n Date: ', '\\033[1m' + f\"{currentdate}\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', \"• Exchange Rate - Dollar to KRW: \" + '\\033[1m' + f\" ₩{int(krwex[0])} <== ₩{int(exchange_check())} \" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', \"• Dollar amount limit to buy: \" + '\\033[1m' + f\" ${dollar_limit_order_dollar} \" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', \"• Split condition dollar min. amount: \" + '\\033[1m' + f\" ${split_condition_dollar} \" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', \"• Split sales on Loss-cut: \" + '\\033[1m' + f\" divide by {sale_split} \" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', \"• Sell Monitor Freq. Limit(Max. num of stocks): \" + '\\033[1m' + f\"Every {sellmonitor_freq} times\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')    \n",
    "    print('\\n', f\"• Mid-Gain Adjust Percentage - Portfolio:   \" + '\\033[1m' + f\"{mid_gain_adjust_percent} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Mid-Gain Adjust Percentage - Market(ETF): \" + '\\033[1m' + f\"{mid_gain_market_reflect_percent_by_etf} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Min. Realize Gain(Mid)%: \" + '\\033[1m' + f\"  {realize_gain_min} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Max. Realize Gain(Mid)%: \" + '\\033[1m' + f\"  {realize_gain_max} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Realize Gain Adjustment%: \" + '\\033[1m' + f\" {event_risk_ratio} %\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Buy Limit Rate%: \" + '\\033[1m' + f\"{np.round(buy_limit_rate, 2)} %\" + '\\033[0m')\n",
    "    print('\\n', f\"  > S&P500 Buy price change initial limit: \" + '\\033[1m' + f\"{buy_snp_t_y} %\" + '\\033[0m')\n",
    "    print('\\n', f\"  > Gold Buy price change limit: \" + '\\033[1m' + f\"{buy_gold_t_y} %\" + '\\033[0m')\n",
    "    print('\\n', f\"  > Oil Buy price change limit: \" + '\\033[1m' + f\"{buy_oil_t_y} %\" + '\\033[0m')\n",
    "    print('\\n', f\"  > Bitcoin Buy price change limit: \" + '\\033[1m' + f\"{buy_bit_t_y} %\" + '\\033[0m')\n",
    "    print('\\n', f\"  > ETF Buy price change initial limit: \" + '\\033[1m' + f\"{buy_snp_t_y} %\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Valuation Conversion Value - Buy (S&P): \" + '\\033[1m' + f\"{valuation_conversion_value_buy}\" + '\\033[0m')   \n",
    "    print('\\n', f\"• Valuation Conversion Value - Buy (Else): \" + '\\033[1m' + f\"{valuation_conversion_value_etf_buy}\" + '\\033[0m')  \n",
    "    print('\\n', f\"• Valuation Conversion Value - Sell: \" + '\\033[1m' + f\"{valuation_conversion_value_sell}\" + '\\033[0m') \n",
    "    print('\\n', f\"• Valuation Conversion Value - LossCut(for small): \" + '\\033[1m' + f\"{valuation_conversion_value_losscut}\" + '\\033[0m')\n",
    "    print('\\n', f\"• Valuation Conversion Value - LossCut(for all): \" + '\\033[1m' + f\"{valuation_conversion_value_losscut_all}\" + '\\033[0m') \n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Volume Sign Candle Limit- Change of Volume per Change of Price: \" + '\\033[1m' + f\"{VC_per_PC_limit_V}\" + '\\033[0m')\n",
    "    print('\\n', f\"• Volume Sign Candle Limit- Change of Price: \" + '\\033[1m' + f\"{PC_limit_V}\" + '\\033[0m')\n",
    "    print('\\n', f\"• Volume Speed vs Mean Limit: \" + '\\033[1m' + f\"{VS_vs_MEAN_limit}\" + '\\033[0m')\n",
    "    print('\\n', f\"• Volume Speed Acceleration Limit: \" + '\\033[1m' + f\"{VS_Accel_limit}\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• S&P 500 Check alarm drop%: \" + '\\033[1m' + f\"{check_snp_t_y[0]} %\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Bluechips# for Slist: \" + '\\033[1m' + f\"{bluechips} EA\" + '\\033[0m')\n",
    "    print('\\n', f\"• Buy Sign Lookback Days: \" + '\\033[1m' + f\"{buy_sign_lookback_days} days (*inactive)\" + '\\033[0m')\n",
    "    print('\\n', f\"• Super Pass - Buy Sign Lookback Days: \" + '\\033[1m' + f\"{superpass_buysign_lookbackdays} days\" + '\\033[0m')\n",
    "    print('\\n', f\"• MDD Lookback Days: \" + '\\033[1m' + f\"{mdd_lookback_days} days\" + '\\033[0m')\n",
    "    print('\\n', f\"• Financial Statement Filtering: \" + '\\033[1m' + f\"{fs_cut} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Financial Statement Filtering Plus: \" + '\\033[1m' + f\"{fs_cut_plus} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Volatility Cut Value (vs S&P500): \" + '\\033[1m' + f\"{vola_cut_rate_value} times\" + '\\033[0m')\n",
    "    print('\\n', f\"• RSI Cut Max: \" + '\\033[1m' + f\"{rsi_max_cut}\" + '\\033[0m')\n",
    "    print('\\n', f\"• 1M Perf. vs Sector - (ETF Ticker x Value): \" + '\\033[1m' + f\"{portfolio_sector_ticker} times\" + '\\033[0m')\n",
    "    print('\\n', f\"• Top ticker sorting by [Volume x Price]: \" + '\\033[1m' + f\"{volume_ticker_percent} times\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Average Line: \" + '\\033[1m' + f\"{averageline} days\" + '\\033[0m')\n",
    "    print('\\n', f\"• Moving Average Search Break: \" + '\\033[1m' + f\"{moving_avg_searchbreak} <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Moving Average Value: \" + '\\033[1m' + f\"{int(moving_avg_value)} days (Min: {moving_avg_value_min} | Max: {moving_avg_value_max}) <<< Do NOT change\" + '\\033[0m')\n",
    "    print('\\n', f\"• Min.fbeta Score verifying optimization: \" + '\\033[1m' + f\"{int(minimum_fbeta*100)} %\" + '\\033[0m')\n",
    "    print('\\n', f\"• Initial Proba Conversion Rate: \" + '\\033[1m' + f\"{int(proba_initial_value*100)} % <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Proba Conversion Rate (min): \" + '\\033[1m' + f\"{int(predict_proba_value_min*100)} % <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Proba Conversion Rate (max): \" + '\\033[1m' + f\"{int(predict_proba_value_max*100)} % <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Model Choice Min. fbeta (min): \" + '\\033[1m' + f\"{int(modelchoice_minimum_fbeta_min*100)} % <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Model Choice Min. fbeta (max): \" + '\\033[1m' + f\"{int(modelchoice_minimum_fbeta_max*100)} % <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Initial Model Choosing Min. fbeta Score: \" + '\\033[1m' + f\"{int(initial_modelchoice_minimum_fbeta*100)} % <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Min. number of Positive-models: \" + '\\033[1m' + f\"{int(min_num_positive_models)} EA <<<\" + '\\033[0m')\n",
    "    print('\\n', f\"• Aggregrated Min. Positive Percent: \" + '\\033[1m' + f\"{int(min_positive_percent*100)} % <<<\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Buymonitor Whitelist:\\n\" + '\\033[1m' + f\"   {sorted(buymonitor_whitelist)}\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________')\n",
    "    print('\\n', f\"• Mid-Term Gain Cut(%) on Event Date: \" + '\\033[1m' + f\"{int(event_risk_ratio*100)} %\" + '\\033[0m')\n",
    "    print('\\n', f\"  > The upcoming event: \" + '\\033[1m' + f\"{upcoming_event}\" + '\\033[0m')\n",
    "    print('\\n', f\"  > Event date: \" + '\\033[1m' + f\"{str(eventdate)}\" + '\\033[0m')\n",
    "    print('\\n', f\"  > Days remaining until the upcoming event: \" + '\\033[1m' + f\"{int(upcoming_remain)} days\" + '\\033[0m')\n",
    "    print('_____________________________________________________________________________________________________\\n\\n')\n",
    "    print('===[ Portfolio Summary ]=============================================================================\\n')\n",
    "    print(performance_updated_df) \n",
    "    print(\"\\n• Equity change: \" + '\\033[1m' + f\"             $ {equity_change}\" + '\\033[0m')\n",
    "    print(\"\\n• Performance average: \" + '\\033[1m' + f\"         {stock_return_percent} %\" + '\\033[0m')\n",
    "    print(\"\\n• Number of stocks on hand: \" + '\\033[1m' + f\"    {int(num_of_purchased)} stocks\" + '\\033[0m')\n",
    "    print(\"\\n• Sell monitoring cycle: \" + '\\033[1m' + f\"       {sellmonitor_freq} cycles\" + '\\033[0m')\n",
    "    print(\"\\n• Loss-cut Min. Holding Period: \" + '\\033[1m' + f\"{min_holding_period} days\" + '\\033[0m')\n",
    "    print(\"\\n• Stocks not working: \" + '\\033[1m' + f\"   {sorted(dead_stock)}\" + '\\033[0m')\n",
    "    print(\"\\n• Stocks on hold: \" + '\\033[1m' + f\"       {sorted(current_index)}\" + '\\033[0m')\n",
    "    print(\"\\n• Stocks aged the limit: \" + '\\033[1m' + f\"{sorted(holding_period_index)}\" + '\\033[0m')\n",
    "    print('\\n• Hoding Period:\\n\\n' + '\\033[1m' + f'{holding_period}' + '\\033[0m')\n",
    "\n",
    "    # sell monitor cycle check\n",
    "    if sellmonitor_freq == 1:\n",
    "        print('\\033[1m' + 'The number of holding stocks has reached the limit. No buy monitor executes.\\n\\n' + '\\033[0m')\n",
    "    elif sellmonitor_freq < 1:\n",
    "        message = 'Sell Monitor frequency is less than 1. Code cell breaks.'\n",
    "        print('\\n\\n' + '\\033[1m' + 'Except >>> ' + message + '\\n\\n' + '\\033[0m')\n",
    "        alarm()    \n",
    "        # code cell break ###############\n",
    "        class StopExecution(Exception):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "\n",
    "        raise StopExecution\n",
    "        # code cell break ###############\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # performance review\n",
    "    if timechecknow() < 10:\n",
    "#         time.sleep(30)\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        time.sleep(5)\n",
    "    real_performance_selma()\n",
    "    \n",
    "#     print('\\n=============================================================================[ Portfolio Summary ]===\\n')\n",
    "#     if timechecknow() < 0:\n",
    "#         # price - moving average plot for holding tickers\n",
    "#         for ticker in sorted(robin['index'].tolist()):\n",
    "#             try:\n",
    "#                 sector_stock_plot(ticker)\n",
    "#             except:\n",
    "#                 pass\n",
    "#             dmv_plot(ticker)\n",
    "#             rsi_plot(ticker)\n",
    "#             strategy_analysis = strategy_analysis_main(ticker, 'on', 'off')[0]\n",
    "#             news_sentiment_analysis(ticker, 'on') \n",
    "#             if strategy_analysis == -1:\n",
    "#                 message = f\"{ticker} has a sell sign\"\n",
    "#                 voice_message(message)\n",
    "#                 print(message)\n",
    "#             print('\\n\\n\\n\\n\\n')\n",
    "#         strategy_analysis_main('005930.KS', 'on', 'off')\n",
    "#         print('\\n\\n\\n\\n\\n')\n",
    "        \n",
    "#     if timechecknow() < 0:\n",
    "#         print('\\n\\n\\n\\n\\n===[ Top & Bottom Performer ]========================================================================\\n')\n",
    "#         # strategy analyiss \n",
    "#         ticker = df.sort_values('1M Perf', ascending = False).head(1)['Symbol'].values[0]\n",
    "#         strategy_analysis_main(ticker, 'on', 'off')\n",
    "#         news_sentiment_analysis(ticker, 'on') \n",
    "#         print('\\n\\n\\n\\n\\n')\n",
    "#         ticker = df['Symbol'].tolist()[int(len(df)/2)]\n",
    "#         strategy_analysis_main(ticker, 'on', 'off')\n",
    "#         news_sentiment_analysis(ticker, 'on') \n",
    "        \n",
    "#         # treemap for srank\n",
    "#         print('\\n\\n\\n')\n",
    "#         srank = pd.read_csv('srank.csv')\n",
    "#         for index_value in ['Market Cap', 'Volume', '5D Perf', '1M Perf', '6M Perf', 'YTD Perf', 'EBITDA Margin', 'Return on Equity', 'Quick Ratio']:\n",
    "#             treemap(srank, index_value)\n",
    "        \n",
    "    # sean index prediction\n",
    "    sean_index_plot()\n",
    "    rsi_plot('^GSPC')\n",
    "    daily_news_sentiment_all('on')\n",
    "    \n",
    "    print('\\n' + current_time() + '\\n\\n' + '\\033[1m' + tarot() + '\\033[0m' + '\\n')\n",
    "    \n",
    "    # # if yfinance is not on service\n",
    "    # https://financedata.github.io/posts/finance-data-reader-users-guide.html\n",
    "\n",
    "    # import FinanceDataReader as fdr\n",
    "    # print(fdr.__version__)\n",
    "    # fdr.DataReader('GOOGL')['Close'].plot();\n",
    "\n",
    "    # df_NASDAQ = fdr.StockListing('NASDAQ')\n",
    "    # df_NYSE = fdr.StockListing('NYSE')\n",
    "    # df_snp = fdr.StockListing('SP500')\n",
    "    # df_amex = fdr.StockListing('AMEX')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbb4ae",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d812c3e7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "hide_input": true
   },
   "source": [
    "# Indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7953a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## Macroeconomic Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb226155",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main_macro():\n",
    "    \n",
    "    # jobless claim\n",
    "    url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    joblessclaim = webContent[710:]\n",
    "    joblessclaim = joblessclaim.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(joblessclaim)):\n",
    "        if i%2 == 0:\n",
    "            date.append(joblessclaim[i])\n",
    "        else:\n",
    "            number.append(joblessclaim[i])\n",
    "    jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "    # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "    jls.fillna(method = 'ffill', inplace = True)\n",
    "    jls = jls[jls['Date'] <= currentdate]\n",
    "\n",
    "    jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "    jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "    jls = jls.tail(averageline)\n",
    "\n",
    "    # cpi index\n",
    "    url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    cpi = webContent[4075:]\n",
    "    cpi = cpi.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(cpi)):\n",
    "        if i%2 == 0:\n",
    "            date.append(cpi[i])\n",
    "        else:\n",
    "            number.append(cpi[i])\n",
    "    cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "    cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "    cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "    cpidx = cpidx.tail(20)\n",
    "\n",
    "    # inflation index\n",
    "    url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    inf = webContent[1222:]\n",
    "    inf = inf.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(inf)):\n",
    "        if i%2 == 0:\n",
    "            date.append(inf[i])\n",
    "        else:\n",
    "            number.append(inf[i])\n",
    "    inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "    drp = inf[inf.Inf == '.'].index.values\n",
    "    inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "    inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "    inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "    inf = inf.tail(averageline)\n",
    "\n",
    "    # 30-Year Fixed Rate Mortgage Average & cleaning\n",
    "    url = 'https://fred.stlouisfed.org/data/MORTGAGE30US.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    mortg = webContent[1895:]\n",
    "    mortg = mortg.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(mortg)):\n",
    "        if i%2 == 0:\n",
    "            date.append(mortg[i])\n",
    "        else:\n",
    "            number.append(mortg[i])\n",
    "    mortg = pd.DataFrame({'Date': date, 'Mortg': number})\n",
    "    mortg['Date'] = mortg['Date'].astype('datetime64[ns]')\n",
    "    mortg['Mortg'] = pd.to_numeric(mortg['Mortg'])\n",
    "    mortg = mortg.tail(averageline)\n",
    "    \n",
    "    # University of Michigan: Consumer Sentiment \n",
    "    url = 'https://fred.stlouisfed.org/data/Umcsent.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Umcsent = webContent[6897:]\n",
    "    Umcsent = Umcsent.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Umcsent)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Umcsent[i])\n",
    "        else:\n",
    "            number.append(Umcsent[i])\n",
    "    Umcsent = pd.DataFrame({'Date': date, 'Umcsent': number})\n",
    "    Umcsent['Date'] = Umcsent['Date'].astype('datetime64[ns]')\n",
    "    Umcsent['Umcsent'] = pd.to_numeric(Umcsent['Umcsent'])\n",
    "    Umcsent = Umcsent.tail(20)\n",
    "\n",
    "    # University of Michigan: Inflation Expectation\n",
    "    url = 'https://fred.stlouisfed.org/data/mich.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    umcinf = webContent[1109:]\n",
    "    umcinf = umcinf.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(umcinf)):\n",
    "        if i%2 == 0:\n",
    "            date.append(umcinf[i])\n",
    "        else:\n",
    "            number.append(umcinf[i])\n",
    "    umcinf = pd.DataFrame({'Date': date, 'umcinf': number})\n",
    "    umcinf['Date'] = umcinf['Date'].astype('datetime64[ns]')\n",
    "    umcinf['umcinf'] = pd.to_numeric(umcinf['umcinf'])\n",
    "    umcinf = umcinf.tail(20)\n",
    "    \n",
    "    # Real GDP\n",
    "    url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Rgdp = webContent[1005:]\n",
    "    Rgdp = Rgdp.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Rgdp)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Rgdp[i])\n",
    "        else:\n",
    "            number.append(Rgdp[i])\n",
    "    Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "    Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "    Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "    Rgdp = Rgdp.tail(20)\n",
    "    \n",
    "  \n",
    "    # 10 year treasury note\n",
    "    rate = yfinance_df_setting('^TNX').tail(averageline)   \n",
    "\n",
    "    # CBOE Volatility Index\n",
    "    cboe = yfinance_df_setting('^VIX').tail(averageline)   \n",
    "\n",
    "    # Crude Oil\n",
    "    mar = yfinance_df_setting('CL=F').tail(averageline)   \n",
    "\n",
    "    # Bitcoin (*Last)\n",
    "    bit = yfinance_df_setting('BTC-USD').tail(averageline)   \n",
    "\n",
    "    # bond ETF 10 yaer\n",
    "    ief = yfinance_df_setting('IEF').tail(averageline)\n",
    "\n",
    "    # Gold\n",
    "    gold = yfinance_df_setting('GC=F').tail(averageline)     \n",
    "\n",
    "    # Gold ETF\n",
    "    gld = yfinance_df_setting('GLD').tail(averageline)   \n",
    "\n",
    "    # Dollar Index\n",
    "    dollar = yfinance_df_setting('DX-Y.NYB').tail(averageline) \n",
    "\n",
    "    # Semi-Con\n",
    "    sox = yfinance_df_setting('^SOX').tail(averageline)   \n",
    "\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize = (16, 100))\n",
    "\n",
    "    plt.subplot(16,1,1)\n",
    "    plt.errorbar(x = jls.tail(52)['Date'], y = jls.tail(52)['Jclaim'], label = 'Jobless Claim', color = sb.color_palette()[0])\n",
    "    plt.title('Jobless Claim')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    print(jls.tail())\n",
    "    \n",
    "    plt.subplot(16,1,2)\n",
    "    plt.errorbar(x = cboe['Date'], y = cboe['Adj Close'], label = 'Cboe', color = sb.color_palette()[1])\n",
    "    plt.title('Cboe Valitality')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,3)\n",
    "    plt.errorbar(x = cpidx['Date'], y = cpidx['Cpi'], label = 'CPI',  color = sb.color_palette()[2])\n",
    "    plt.title('CPI')\n",
    "#     plt.yscale('log')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    plt.subplot(16,1,4)\n",
    "    plt.errorbar(x = Rgdp['Date'], y = Rgdp['Rgdp'], label = 'Real GDP',  color = sb.color_palette()[3])\n",
    "    plt.title('Real GDP')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,5)\n",
    "    plt.errorbar(x = Umcsent['Date'], y = Umcsent['Umcsent'], label = 'UMC Consumer Sentiment ', color = sb.color_palette()[4])\n",
    "    plt.title('UMC Consumer Sentiment ')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    plt.subplot(16,1,6)\n",
    "    plt.errorbar(x = inf['Date'], y = inf['Inf'], label = 'Inflation rate', color = sb.color_palette()[5])\n",
    "    plt.title('Inflation')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,7)\n",
    "    plt.errorbar(x = umcinf['Date'], y = umcinf['umcinf'], label = 'UMC Inflation Expectation', color = sb.color_palette()[6])\n",
    "    plt.title('UMC Inflation Expectation')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    plt.subplot(16,1,8)\n",
    "    plt.errorbar(x = rate['Date'], y = rate['Adj Close'], label = 'Treasury Yield 10 Years', color = sb.color_palette()[7])\n",
    "    plt.title('Treasury Yield 10 Years')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,9)\n",
    "    plt.errorbar(x = ief['Date'], y = ief['Adj Close'], label = 'iShares 7-10 Year Treasury Bond ETF', color = sb.color_palette()[8])\n",
    "    plt.title('iShares 7-10 Year Treasury Bond ETF: IEF')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    plt.subplot(16,1,10)\n",
    "    plt.errorbar(x = mar['Date'], y = mar['Adj Close'], label = 'Crude Oil', color = sb.color_palette()[9])\n",
    "    plt.title('Crude Oil')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,11)\n",
    "    plt.errorbar(x = bit['Date'], y = bit['Adj Close'], label = 'BitCoin', color = sb.color_palette()[0])\n",
    "    plt.title('BitCoin')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,12)\n",
    "    plt.errorbar(x = mortg['Date'], y = mortg['Mortg'], label = '30-Year Fixed Rate Mortgage Average', color = sb.color_palette()[1])\n",
    "    plt.title('30-Year Fixed Rate Mortgage Average')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,13)\n",
    "    plt.errorbar(x = gold['Date'], y = gold['Adj Close'], label = 'Gold', color = sb.color_palette()[2])\n",
    "    plt.title('Gold')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,14)\n",
    "    plt.errorbar(x = gld['Date'], y = gld['Adj Close'], label = 'GLD', color = sb.color_palette()[3])\n",
    "    plt.title('GLD')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,15)\n",
    "    plt.errorbar(x = dollar['Date'], y = dollar['Adj Close'], label = 'Dollar', color = sb.color_palette()[4])\n",
    "    plt.title('Dollar')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(16,1,16)\n",
    "    plt.errorbar(x = sox['Date'], y = sox['Adj Close'], label = 'SOX', color = sb.color_palette()[5])\n",
    "    plt.title('SOX')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_macro()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee42695",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "source": [
    "## Microeconomic Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f4b6bff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def main_micro():\n",
    "\n",
    "    sector_tickers = {'S&P 500' : '^GSPC',\n",
    "                      'Energy' : 'XLE',\n",
    "                      'Financials' : 'XLF', \n",
    "                      'Real Estate' : 'XLRE', \n",
    "                      'Materials' : 'XLB',\n",
    "                      'Information Technology' : 'XLK', \n",
    "                      'Consumer Staples' : 'XLP', \n",
    "                      'Health Care' : 'XLV', \n",
    "                      'Industrials' :'XLI', \n",
    "                      'Consumer Discretionary' : 'XLY', \n",
    "                      'Utilities' : 'XLU', \n",
    "                      'Communication Services' : 'XLC',\n",
    "                      'QQQ' : 'QQQ',\n",
    "                      'SPY' : 'SPY', \n",
    "                      'SPYD' : 'SPYD', \n",
    "                      'SPYG' : 'SPYG', \n",
    "                      'SPYV' : 'SPYV'}\n",
    "\n",
    "    plt.figure(figsize = (16, 80))\n",
    "    c = 1\n",
    "    for k, v in sector_tickers.items():\n",
    "        price_record = yfinance_df_setting(v).tail(averageline) \n",
    "        plt.subplot(len(sector_tickers),1,c)\n",
    "        c+=1\n",
    "        plt.errorbar(x = price_record['Date'], y = price_record['Adj Close'], label = k, color = sb.color_palette()[int(np.random.randint(10, size = 11)[0])])\n",
    "#         plt.yscale('log')\n",
    "        plt.title(k)\n",
    "        plt.grid(axis = 'x')\n",
    "        plt.grid(axis = 'y')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_micro()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa9a901",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Buffet Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b99b850",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request, urllib.error, urllib.parse\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def main_Buffet():\n",
    "    \n",
    "    # Real GDP\n",
    "    url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Rgdp = webContent[1005:]\n",
    "    Rgdp = Rgdp.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Rgdp)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Rgdp[i])\n",
    "        else:\n",
    "            number.append(Rgdp[i])\n",
    "    Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "    Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "    Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "    df = yfinance_df_setting('^GSPC')\n",
    "\n",
    "    df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "    df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "    for i in range(len(df.Rgdp)):\n",
    "        if df.Rgdp[i] == 0:\n",
    "            df.Rgdp[i] = df.Rgdp[i-1]\n",
    "    df.dropna(inplace = True)\n",
    "\n",
    "    df['Buffett'] = df['Adj Close'] / df['Rgdp']*10\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    ######################################## Plot ##\n",
    "    plt.figure(figsize = (16,18))\n",
    "\n",
    "    plt.subplot(3,1,1)\n",
    "    df = df[-5000:]\n",
    "    plt.errorbar(x = df['Date'], y = df['Buffett'], label = 'Buffett (log)', color = sb.color_palette()[0])\n",
    "    # plt.errorbar(x = df['Date'], y = df['Adj Close'], label = 'S&P 500', color = sb.color_palette()[1])\n",
    "    plt.title('Buffett Index (LT)')\n",
    "#     plt.yscale('log')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    plt.subplot(3,1,2)\n",
    "    df = df[-1000:]\n",
    "    plt.errorbar(x = df['Date'], y = df['Buffett'], label = 'Buffett (log)', color = sb.color_palette()[0])\n",
    "    # plt.errorbar(x = df['Date'], y = df['Adj Close'], label = 'S&P 500', color = sb.color_palette()[1])\n",
    "    plt.title('Buffett Index (ST)')\n",
    "#     plt.yscale('log')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    \n",
    "    plt.subplot(3,1,3)\n",
    "    df = df[-500:]\n",
    "    plt.errorbar(x = df['Date'], y = df['Buffett'], label = 'Buffett (log)', color = sb.color_palette()[0])\n",
    "    # plt.errorbar(x = df['Date'], y = df['Adj Close'], label = 'S&P 500', color = sb.color_palette()[1])\n",
    "    plt.title('Buffett Index (SST)')\n",
    "#     plt.yscale('log')\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "\n",
    "    # #define colors to use\n",
    "    # col1 = 'steelblue'\n",
    "    # col2 = 'red'\n",
    "\n",
    "    # #define subplots\n",
    "    # fig,ax = plt.subplots()\n",
    "    # fig.set_size_inches(16,12)\n",
    "    # fig.set_dpi(100)\n",
    "\n",
    "    # #add first line to plot\n",
    "    # ax.plot(df.Date, df['Adj Close'], color=col1)\n",
    "\n",
    "    # #add x-axis label\n",
    "    # ax.set_xlabel('Date')\n",
    "\n",
    "    # #add y-axis label\n",
    "    # ax.set_ylabel('S&P 500', color=col1)\n",
    "\n",
    "    # #define second y-axis that shares x-axis with current plot\n",
    "    # ax2 = ax.twinx()\n",
    "\n",
    "    # #add second line to plot\n",
    "    # ax2.plot(df.Date, df.Buffett, color=col2)\n",
    "\n",
    "    # #add second y-axis label\n",
    "    # ax2.set_ylabel('Buffett Index', color=col2)\n",
    "\n",
    "    # plt.grid(axis = 'x')\n",
    "    # plt.grid(axis = 'y')\n",
    "\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_Buffet()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34de45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "source": [
    "## Sector RSI Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95c1f088",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        for ticker in ['BTC-USD', 'GLD', 'USO']:\n",
    "            rsi_plot(ticker) \n",
    "        for ticker in etf_index_dic.ticker.tolist():\n",
    "            ticker_name = etf_index_dic[etf_index_dic['ticker'] == ticker]['sector'].values[0]\n",
    "            rsi_plot(ticker)  \n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86779223",
   "metadata": {
    "heading_collapsed": true,
    "hide_input": true
   },
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e65d0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Prediction - Aim: S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ced97a1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# find the highest rsquared date\n",
    "def rsquare(dfc):\n",
    "    \n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    dfc['intercept'] = 1\n",
    "    lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    results = lm.fit()\n",
    "    return results.rsquared, results.pvalues\n",
    "\n",
    "def main_snp():\n",
    "    \n",
    "    # find the highest Rsquared of S&P 500 \n",
    "    ticker = '^GSPC'\n",
    "    tkr = yfinance_df(ticker)   \n",
    "    sfive = tkr\n",
    "\n",
    "    # 10 year treasery bond yield\n",
    "    rate = yfinance_df('^TNX')   \n",
    "    irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "    # CBOE Volatility Index\n",
    "    cboe = yfinance_df('^VIX')   \n",
    "    cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "    # Gold Feb 22 (GC=F)\n",
    "    gold = yfinance_df('GC=F')   \n",
    "    gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "    # Crude Oil\n",
    "    mar = yfinance_df('CL=F')   \n",
    "    mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Dollar Index\n",
    "    dollar = yfinance_df('DX-Y.NYB')\n",
    "    dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Bitcoin (*Last)\n",
    "    bit = yfinance_df('BTC-USD')   \n",
    "    bit = bit[['Date', 'Adj Close']]\n",
    "\n",
    "    # cpi index\n",
    "    url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    cpi = webContent[4075:]\n",
    "    cpi = cpi.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(cpi)):\n",
    "        if i%2 == 0:\n",
    "            date.append(cpi[i])\n",
    "        else:\n",
    "            number.append(cpi[i])\n",
    "    cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "    cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "    cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "    # jobless claim index\n",
    "    url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    joblessclaim = webContent[710:]\n",
    "    joblessclaim = joblessclaim.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(joblessclaim)):\n",
    "        if i%2 == 0:\n",
    "            date.append(joblessclaim[i])\n",
    "        else:\n",
    "            number.append(joblessclaim[i])\n",
    "    jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "    # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "    jls.fillna(method = 'ffill', inplace = True)\n",
    "    jls = jls[jls['Date'] <= currentdate]    \n",
    "    \n",
    "    jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "    jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "    jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "    cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "    # inflation index\n",
    "    url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    inf = webContent[1222:]\n",
    "    inf = inf.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(inf)):\n",
    "        if i%2 == 0:\n",
    "            date.append(inf[i])\n",
    "        else:\n",
    "            number.append(inf[i])\n",
    "    inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "    drp = inf[inf.Inf == '.'].index.values\n",
    "    inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "    inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "    inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "    # merge yfinance tickers & cleaning\n",
    "    df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "    df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "    # cleaning - cpi and Jclaim\n",
    "    for i in range(len(df.Cpi)):\n",
    "        if df.Cpi[i] == 0.0:\n",
    "            df.Cpi[i] = df.Cpi[i-1]    \n",
    "    df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "    for i in range(len(df.Jclaim)):\n",
    "        if df.Jclaim[i] == 0.0:\n",
    "            df.Jclaim[i] = df.Jclaim[i-1]\n",
    "    drp = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=drp)\n",
    "\n",
    "    # cleaning - inflation rate since 1990\n",
    "    df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df[df['Date'] > '2003-01-01']\n",
    "    df = df.fillna(0)\n",
    "    if df.tail(1).Inf.values[0] == 0:\n",
    "        df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "\n",
    "#     ## added but low P-value, also distort Int-rate    \n",
    "\n",
    "#     # 30-Year Fixed Rate Mortgage Average & cleaning\n",
    "#     url = 'https://fred.stlouisfed.org/data/MORTGAGE30US.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     mortg = webContent[32820:]\n",
    "#     mortg = mortg.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(mortg)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(mortg[i])\n",
    "#         else:\n",
    "#             number.append(mortg[i])\n",
    "#     mortg = pd.DataFrame({'Date': date, 'Mortg': number})\n",
    "#     mortg['Date'] = mortg['Date'].astype('datetime64[ns]')\n",
    "#     mortg['Mortg'] = pd.to_numeric(mortg['Mortg'])\n",
    "\n",
    "#     df = df.merge(mortg, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df[df['Date'] >= '2003-01-03']\n",
    "#     df = df.fillna(0).reset_index(drop=True)\n",
    "#     for i in range(len(df.Mortg)):\n",
    "#         if df.Mortg[i] == 0.0:\n",
    "#             df.Mortg[i] = df.Mortg[i-1]\n",
    "#     drp = df[df['Mortg'] == 0].index.values\n",
    "#     df = df.drop(index=drp)\n",
    "    \n",
    "    \n",
    "#     # University of Michigan: Consumer Sentiment \n",
    "#     url = 'https://fred.stlouisfed.org/data/Umcsent.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     Umcsent = webContent[6897:]\n",
    "#     Umcsent = Umcsent.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(Umcsent)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(Umcsent[i])\n",
    "#         else:\n",
    "#             number.append(Umcsent[i])\n",
    "#     Umcsent = pd.DataFrame({'Date': date, 'Umcsent': number})\n",
    "#     Umcsent['Date'] = Umcsent['Date'].astype('datetime64[ns]')\n",
    "#     Umcsent['Umcsent'] = pd.to_numeric(Umcsent['Umcsent'])\n",
    "    \n",
    "#     # cleaning\n",
    "#     df = df.merge(Umcsent, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.fillna(0)\n",
    "#     if df.tail(1).Umcsent.values[0] == 0:\n",
    "#         df.tail(1).Umcsent = df.tail(2).Umcsent.values[0]\n",
    "\n",
    "    # merge - cboe     \n",
    "    df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - gold\n",
    "    df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - Crude Oil Mar 22 \n",
    "    df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - Dollar Index \n",
    "    df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "    df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "\n",
    "    # dropna and save the source date\n",
    "    df = df.dropna()\n",
    "\n",
    "    # fillavg = df[df['Inf'] == 0].index.values\n",
    "    # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "    dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "    df = df.drop(index = dfdrop)   \n",
    "\n",
    "    # dfdrop = df[df['Inf'] == 0].index.values\n",
    "    # df = df.drop(index = dfdrop)\n",
    "\n",
    "    fillavg = df[df['Inf'] == 0].index.values\n",
    "    df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "\n",
    "    dfdrop = df[df['Gold'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    # Fillna Cboe with the highest value for the past 20 working days. \n",
    "    cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "    df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "\n",
    "    # Real GDP\n",
    "    url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Rgdp = webContent[1005:]\n",
    "    Rgdp = Rgdp.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Rgdp)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Rgdp[i])\n",
    "        else:\n",
    "            number.append(Rgdp[i])\n",
    "    Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "    Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "    Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "    df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "    df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "    for i in range(len(df.Rgdp)):\n",
    "        if df.Rgdp[i] == 0:\n",
    "            df.Rgdp[i] = df.Rgdp[i-1]\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    ################################################################################## add from below\n",
    "\n",
    "    if df.all().sum() != len(df.columns):\n",
    "        df.to_csv('errorlog_prediction_snp.csv', index = False)\n",
    "        print('\\nZero value checked: ', ticker, '\\n')\n",
    "        print(df.all())\n",
    "        alarm() # if any zero value, alarm\n",
    "    else:\n",
    "        df.to_csv('predict_aim_sourcedata_snp.csv', index = False)\n",
    "        \n",
    "    if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "        print('> The last line == currentdate\\n')\n",
    "    else:\n",
    "        print('>>> The last line != currentdate\\n')\n",
    "#         alarm()\n",
    "        \n",
    "    ##################################### source data save\n",
    "\n",
    "    # range selection - optimized since [2014-09-17]\n",
    "\n",
    "    highest = 0\n",
    "\n",
    "    date = []\n",
    "    ml = []\n",
    "    for i in range(len(df.Date.values)-100):\n",
    "        dfc = df[df['Date'] > df.Date.values[i]]\n",
    "        result = rsquare(dfc)[0]\n",
    "\n",
    "        date.append(df.Date.values[i])\n",
    "        ml.append(result)    \n",
    "\n",
    "        if highest < result:\n",
    "            highest = result\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.errorbar(dff.Date, dff.Result)\n",
    "    plt.title('S&P500 R-squared')\n",
    "\n",
    "    print('__________________________________\\n')\n",
    "    print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "    print('__________________________________')\n",
    "\n",
    "    # rday: most optimized day cut\n",
    "    # numpy.datetime64 to string\n",
    "    ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "    rday_snp = ts.strftime('%Y-%m-%d')\n",
    "    with open(\"rday_snp.txt\", 'w') as f:\n",
    "        f.write(str(rday_snp)) \n",
    "\n",
    "    # Hypothese: \n",
    "    # R-squared represents market uncertainties \n",
    "    # S&P 500 having its highest R-squared represents the genuine market price\n",
    "    # The null hypothesis is that S&P 500 follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "    # originated from S&P 500 stocks which follow long-term price trend. \n",
    "    # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "    dfr = df[df['Date'] > rday_snp]\n",
    "    result_optimized = rsquare(dfr)\n",
    "\n",
    "    # linear regression\n",
    "    prediction = pred_lstm(dfr)    \n",
    "    current = dfr.Stock_price.tail(1).values\n",
    "    current_to_predict = current/prediction\n",
    "\n",
    "    print('\\n• Current S&P500:    ', '%.5f' %current)\n",
    "    print('\\n• Predicted S&P500:  ', '%.5f' %prediction)\n",
    "    print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "    if current < prediction:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "    else:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "    print('__________________________________\\n')\n",
    "\n",
    "    # classifier\n",
    "    prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "    if prediction[1] == 1:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    else:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    print('__________________________________\\n\\n')\n",
    "\n",
    "    # print out stats summary for multi-linear regression\n",
    "    import statsmodels.api as sm\n",
    "    dfr['intercept'] = 1\n",
    "    lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "    results = lm.fit()\n",
    "    print(results.summary())\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_snp()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd70d15",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Prediction - Aim: Oil (USO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "186ba05c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    \n",
    "\n",
    "\n",
    "# find the highest rsquared date\n",
    "def rsquare(dfc):\n",
    "    \n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    dfc['intercept'] = 1\n",
    "    lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    results = lm.fit()\n",
    "    return results.rsquared, results.pvalues\n",
    "\n",
    "def machinelearning(dfr):\n",
    "    # Machine learning\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    x = dfr.drop(columns = ['Date', 'Stock_price', 'intercept', 'Oil'])\n",
    "    x = x.to_numpy()\n",
    "    y = dfr.Stock_price.to_numpy()\n",
    "    \n",
    "    scaler = StandardScaler()   ####\n",
    "    x = scaler.fit_transform(x) ####\n",
    "\n",
    "    poly_feat = PolynomialFeatures(degree = 5)\n",
    "    x = poly_feat.fit_transform(x)\n",
    "#     poly_model = LinearRegression(fit_intercept = False).fit(x, y)\n",
    "    \n",
    "    sample_house = [x[x.shape[0]-1]]\n",
    "    model = LinearRegression()  \n",
    "    model.fit(x, y) \n",
    "    \n",
    "    prediction = model.predict(sample_house)\n",
    "    return prediction\n",
    "\n",
    "def main_oil():\n",
    "\n",
    "    # find the highest Rsquared of GOLD \n",
    "    ticker = 'USO'\n",
    "    tkr = yfinance_df(ticker)   \n",
    "    sfive = tkr\n",
    "\n",
    "    # 10 year treasery bond yield\n",
    "    rate = yfinance_df('^TNX')   \n",
    "    irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "    # CBOE Volatility Index\n",
    "    cboe = yfinance_df('^VIX')   \n",
    "    cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "    # S&P 500\n",
    "    snp = yfinance_df('^GSPC')   \n",
    "    snp = snp[['Date', 'Adj Close']]\n",
    "\n",
    "    # Crude Oil\n",
    "    mar = yfinance_df('CL=F')   \n",
    "    mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Dollar Index\n",
    "    dollar = yfinance_df('DX-Y.NYB')\n",
    "    dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Bitcoin (*Last)\n",
    "    bit = yfinance_df('BTC-USD')   \n",
    "    bit = bit[['Date', 'Adj Close']]\n",
    "\n",
    "    ################################# End of gathering => Clearning starts  ####################################\n",
    "\n",
    "    # cleaning starts\n",
    "    # cpi index\n",
    "    url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    cpi = webContent[4075:]\n",
    "    cpi = cpi.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(cpi)):\n",
    "        if i%2 == 0:\n",
    "            date.append(cpi[i])\n",
    "        else:\n",
    "            number.append(cpi[i])\n",
    "    cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "    cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "    cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "    # jobless claim index\n",
    "    url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    joblessclaim = webContent[710:]\n",
    "    joblessclaim = joblessclaim.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(joblessclaim)):\n",
    "        if i%2 == 0:\n",
    "            date.append(joblessclaim[i])\n",
    "        else:\n",
    "            number.append(joblessclaim[i])\n",
    "    jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "    # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "    jls.fillna(method = 'ffill', inplace = True)\n",
    "    jls = jls[jls['Date'] <= currentdate]\n",
    "    \n",
    "    jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "    jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "    jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "    cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "    # inflation index\n",
    "    url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    inf = webContent[1222:]\n",
    "    inf = inf.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(inf)):\n",
    "        if i%2 == 0:\n",
    "            date.append(inf[i])\n",
    "        else:\n",
    "            number.append(inf[i])\n",
    "    inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "    drp = inf[inf.Inf == '.'].index.values\n",
    "    inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "    inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "    inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "    # merge yfinance tickers & cleaning\n",
    "    df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "    df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "    # cleaning - cpi and Jclaim\n",
    "    for i in range(len(df.Cpi)):\n",
    "        if df.Cpi[i] == 0.0:\n",
    "            df.Cpi[i] = df.Cpi[i-1]    \n",
    "    df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "    for i in range(len(df.Jclaim)):\n",
    "        if df.Jclaim[i] == 0.0:\n",
    "            df.Jclaim[i] = df.Jclaim[i-1]\n",
    "    drp = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=drp)\n",
    "\n",
    "    # cleaning - inflation rate since 1990\n",
    "    df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df[df['Date'] > '2003-01-01']\n",
    "    df = df.fillna(0)\n",
    "    if df.tail(1).Inf.values[0] == 0:\n",
    "        df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "    # merge - cboe     \n",
    "    df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - snp\n",
    "    df = df.merge(snp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'snp'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - Crude Oil Mar 22 \n",
    "    df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - Dollar Index \n",
    "    df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "    df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "\n",
    "    ######################## Do Not change above, starts under the last line #######################\n",
    "    # dropna and save the source date\n",
    "    df = df.dropna()\n",
    "\n",
    "    # fillavg = df[df['Inf'] == 0].index.values\n",
    "    # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "    dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "    df = df.drop(index = dfdrop)   \n",
    "\n",
    "    # dfdrop = df[df['Inf'] == 0].index.values\n",
    "    # df = df.drop(index = dfdrop)\n",
    "\n",
    "    fillavg = df[df['Inf'] == 0].index.values\n",
    "    df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "    dfdrop = df[df['snp'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    # Fillna Cboe with the highest value for the past 20 working days. \n",
    "    cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "    df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "    \n",
    "    # Real GDP\n",
    "    url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Rgdp = webContent[1005:]\n",
    "    Rgdp = Rgdp.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Rgdp)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Rgdp[i])\n",
    "        else:\n",
    "            number.append(Rgdp[i])\n",
    "    Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "    Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "    Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "    df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "    df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "    for i in range(len(df.Rgdp)):\n",
    "        if df.Rgdp[i] == 0:\n",
    "            df.Rgdp[i] = df.Rgdp[i-1]\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    \n",
    "    if df.all().sum() != len(df.columns):\n",
    "        df.to_csv('errorlog_prediction_oil.csv', index = False)\n",
    "        print('\\nZero value checked: ', ticker, '\\n')\n",
    "        print(df.all())\n",
    "        alarm() # if any zero value, alarm\n",
    "    else:\n",
    "        df.to_csv('predict_aim_sourcedata_oil.csv', index = False)\n",
    "        \n",
    "    if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "        print('> The last line == currentdate\\n')\n",
    "    else:\n",
    "        print('>>> The last line != currentdate\\n')\n",
    "#         alarm()        \n",
    "        \n",
    "    ##################################### source data save\n",
    "\n",
    "    # range selection - optimized since [2014-09-17]\n",
    "\n",
    "    highest = 0\n",
    "\n",
    "    date = []\n",
    "    ml = []\n",
    "    for i in range(len(df.Date.values)-100):\n",
    "        dfc = df[df['Date'] > df.Date.values[i]]\n",
    "        result = rsquare(dfc)[0]\n",
    "\n",
    "        date.append(df.Date.values[i])\n",
    "        ml.append(result)    \n",
    "\n",
    "        if highest < result:\n",
    "            highest = result\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.errorbar(dff.Date, dff.Result)\n",
    "    plt.title('Oil R-squared')\n",
    "\n",
    "    print('__________________________________\\n')\n",
    "    print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "    print('__________________________________')\n",
    "\n",
    "    # rday: most optimized day cut\n",
    "    # numpy.datetime64 to string\n",
    "    ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "    rday_oil = ts.strftime('%Y-%m-%d')\n",
    "    with open(\"rday_oil.txt\", 'w') as f:\n",
    "        f.write(str(rday_oil)) \n",
    "\n",
    "    # Hypothese: \n",
    "    # R-squared represents market uncertainties \n",
    "    # GOLD having its highest R-squared represents the genuine market price\n",
    "    # The null hypothesis is that GOLD follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "    # originated from GOLD stocks which follow long-term price trend. \n",
    "    # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "    dfr = df[df['Date'] > rday_oil]\n",
    "    result_optimized = rsquare(dfr)\n",
    "\n",
    "    # linear regression\n",
    "    prediction = pred_lstm(dfr)    \n",
    "    current = dfr.Stock_price.tail(1).values\n",
    "    current_to_predict = current/prediction\n",
    "\n",
    "    print('\\n• Current Oil:    ', '%.5f' %current)\n",
    "    print('\\n• Predicted Oil:  ', '%.5f' %prediction)\n",
    "    print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "    if current < prediction:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "    else:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "    print('__________________________________\\n')\n",
    "    \n",
    "  # classifier\n",
    "    prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "\n",
    "    if prediction[1] == 1:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    else:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    print('__________________________________\\n\\n')\n",
    "    \n",
    "    # print out stats summary for multi-linear regression\n",
    "    import statsmodels.api as sm\n",
    "    dfr['intercept'] = 1\n",
    "    lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Bitcoin', 'Dollar', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "    results = lm.fit()\n",
    "    print(results.summary())\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_oil()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074b13c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Prediction - Aim: Gold (GLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42176af4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    \n",
    "\n",
    "\n",
    "# find the highest rsquared date\n",
    "def rsquare(dfc):\n",
    "    \n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    dfc['intercept'] = 1\n",
    "    lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'snp', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    results = lm.fit()\n",
    "    return results.rsquared, results.pvalues\n",
    "\n",
    "def machinelearning(dfr):\n",
    "    # Machine learning\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    x = dfr.drop(columns = ['Date', 'Stock_price', 'intercept', 'Cboe'])\n",
    "    x = x.to_numpy()\n",
    "    y = dfr.Stock_price.to_numpy()\n",
    "    \n",
    "    scaler = StandardScaler()   ####\n",
    "    x = scaler.fit_transform(x) ####\n",
    "\n",
    "    poly_feat = PolynomialFeatures(degree = 5)\n",
    "    x = poly_feat.fit_transform(x)\n",
    "#     poly_model = LinearRegression(fit_intercept = False).fit(x, y)\n",
    "    \n",
    "    sample_house = [x[x.shape[0]-1]]\n",
    "    model = LinearRegression()  \n",
    "    model.fit(x, y) \n",
    "    \n",
    "    prediction = model.predict(sample_house)\n",
    "    return prediction\n",
    "\n",
    "def main_gold():\n",
    "\n",
    "    # find the highest Rsquared of GOLD \n",
    "    ticker = 'GLD'\n",
    "    tkr = yfinance_df(ticker)   \n",
    "    sfive = tkr\n",
    "\n",
    "    # 10 year treasery bond yield\n",
    "    rate = yfinance_df('^TNX')   \n",
    "    irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "    # CBOE Volatility Index\n",
    "    cboe = yfinance_df('^VIX')   \n",
    "    cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "    # S&P 500\n",
    "    snp = yfinance_df('^GSPC')   \n",
    "    snp = snp[['Date', 'Adj Close']]\n",
    "\n",
    "    # Crude Oil\n",
    "    mar = yfinance_df('CL=F')   \n",
    "    mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Dollar Index\n",
    "    dollar = yfinance_df('DX-Y.NYB')\n",
    "    dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Bitcoin (*Last)\n",
    "    bit = yfinance_df('BTC-USD')   \n",
    "    bit = bit[['Date', 'Adj Close']]\n",
    "\n",
    "    ################################# End of gathering => Clearning starts  ####################################\n",
    "\n",
    "    # cleaning starts\n",
    "    # cpi index\n",
    "    url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    cpi = webContent[4075:]\n",
    "    cpi = cpi.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(cpi)):\n",
    "        if i%2 == 0:\n",
    "            date.append(cpi[i])\n",
    "        else:\n",
    "            number.append(cpi[i])\n",
    "    cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "    cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "    cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "    # jobless claim index\n",
    "    url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    joblessclaim = webContent[710:]\n",
    "    joblessclaim = joblessclaim.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(joblessclaim)):\n",
    "        if i%2 == 0:\n",
    "            date.append(joblessclaim[i])\n",
    "        else:\n",
    "            number.append(joblessclaim[i])\n",
    "    jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "    # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "    jls.fillna(method = 'ffill', inplace = True)\n",
    "    jls = jls[jls['Date'] <= currentdate]\n",
    "    \n",
    "    jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "    jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "    jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "    cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "    # inflation index\n",
    "    url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    inf = webContent[1222:]\n",
    "    inf = inf.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(inf)):\n",
    "        if i%2 == 0:\n",
    "            date.append(inf[i])\n",
    "        else:\n",
    "            number.append(inf[i])\n",
    "    inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "    drp = inf[inf.Inf == '.'].index.values\n",
    "    inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "    inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "    inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "    # merge yfinance tickers & cleaning\n",
    "    df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "    df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "    # cleaning - cpi and Jclaim\n",
    "    for i in range(len(df.Cpi)):\n",
    "        if df.Cpi[i] == 0.0:\n",
    "            df.Cpi[i] = df.Cpi[i-1]    \n",
    "    df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "    for i in range(len(df.Jclaim)):\n",
    "        if df.Jclaim[i] == 0.0:\n",
    "            df.Jclaim[i] = df.Jclaim[i-1]\n",
    "    drp = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=drp)\n",
    "\n",
    "    # cleaning - inflation rate since 1990\n",
    "    df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df[df['Date'] > '2003-01-01']\n",
    "    df = df.fillna(0)\n",
    "    if df.tail(1).Inf.values[0] == 0:\n",
    "        df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "    # merge - cboe     \n",
    "    df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - snp\n",
    "    df = df.merge(snp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'snp'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - Crude Oil Mar 22 \n",
    "    df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - Dollar Index \n",
    "    df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "    df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "\n",
    "    ######################## Do Not change above, starts under the last line #######################\n",
    "    # dropna and save the source date\n",
    "    df = df.dropna()\n",
    "\n",
    "    # fillavg = df[df['Inf'] == 0].index.values\n",
    "    # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "    dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "    df = df.drop(index = dfdrop)   \n",
    "\n",
    "    # dfdrop = df[df['Inf'] == 0].index.values\n",
    "    # df = df.drop(index = dfdrop)\n",
    "\n",
    "    fillavg = df[df['Inf'] == 0].index.values\n",
    "    df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "    dfdrop = df[df['snp'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    # Fillna Cboe with the highest value for the past 20 working days. \n",
    "    cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "    df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "    \n",
    "    # Real GDP\n",
    "    url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Rgdp = webContent[1005:]\n",
    "    Rgdp = Rgdp.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Rgdp)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Rgdp[i])\n",
    "        else:\n",
    "            number.append(Rgdp[i])\n",
    "    Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "    Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "    Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "    df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "    df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "    for i in range(len(df.Rgdp)):\n",
    "        if df.Rgdp[i] == 0:\n",
    "            df.Rgdp[i] = df.Rgdp[i-1]\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.drop(columns = ['Close'])\n",
    "\n",
    "    if df.all().sum() != len(df.columns):\n",
    "        df.to_csv('errorlog_prediction_gold.csv', index = False)\n",
    "        print('\\nZero value checked: ', ticker, '\\n')\n",
    "        print(df.all())\n",
    "        alarm() # if any zero value, alarm\n",
    "    else:\n",
    "        df.to_csv('predict_aim_sourcedata_gold.csv', index = False)\n",
    "        \n",
    "    if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "        print('> The last line == currentdate\\n')\n",
    "    else:\n",
    "        print('>>> The last line != currentdate\\n')\n",
    "#         alarm()\n",
    "        \n",
    "    ##################################### source data save\n",
    "\n",
    "    # range selection - optimized since [2014-09-17]\n",
    "\n",
    "    highest = 0\n",
    "\n",
    "    date = []\n",
    "    ml = []\n",
    "    for i in range(len(df.Date.values)-100):\n",
    "        dfc = df[df['Date'] > df.Date.values[i]]\n",
    "        result = rsquare(dfc)[0]\n",
    "\n",
    "        date.append(df.Date.values[i])\n",
    "        ml.append(result)    \n",
    "\n",
    "        if highest < result:\n",
    "            highest = result\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.errorbar(dff.Date, dff.Result)\n",
    "    plt.title('GOLD R-squared')\n",
    "\n",
    "    print('__________________________________\\n')\n",
    "    print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "    print('__________________________________')\n",
    "\n",
    "    # rday: most optimized day cut\n",
    "    # numpy.datetime64 to string\n",
    "    ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "    rday_gold = ts.strftime('%Y-%m-%d')\n",
    "    with open(\"rday_gold.txt\", 'w') as f:\n",
    "        f.write(str(rday_gold)) \n",
    "\n",
    "    # Hypothese: \n",
    "    # R-squared represents market uncertainties \n",
    "    # GOLD having its highest R-squared represents the genuine market price\n",
    "    # The null hypothesis is that GOLD follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "    # originated from GOLD stocks which follow long-term price trend. \n",
    "    # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "    dfr = df[df['Date'] > rday_gold]\n",
    "    result_optimized = rsquare(dfr)\n",
    "\n",
    "    # linear regression\n",
    "    prediction = pred_lstm(dfr)    \n",
    "    current = dfr.Stock_price.tail(1).values\n",
    "    current_to_predict = current/prediction\n",
    "\n",
    "    print('\\n• Current Gold:    ', '%.5f' %current)\n",
    "    print('\\n• Predicted Gold:  ', '%.5f' %prediction)\n",
    "    print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "    if current < prediction:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "    else:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "    print('__________________________________\\n')\n",
    "\n",
    "  # classifier\n",
    "    prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "\n",
    "    if prediction[1] == 1:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    else:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    print('__________________________________\\n\\n')\n",
    "\n",
    "    # print out stats summary for multi-linear regression\n",
    "    import statsmodels.api as sm\n",
    "    dfr['intercept'] = 1\n",
    "    lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'snp', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "    results = lm.fit()\n",
    "    print(results.summary())\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_gold()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8741b9c3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true
   },
   "source": [
    "## Prediction - Aim: BitCoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f40fa85",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    \n",
    "\n",
    "    \n",
    "# find the highest rsquared date\n",
    "def rsquare(dfc):\n",
    "\n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    dfc['intercept'] = 1\n",
    "    lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Oil', 'Dollar', 'Gold', 'Rgdp']])\n",
    "    results = lm.fit()\n",
    "    return results.rsquared, results.pvalues\n",
    "\n",
    "def machinelearning(dfr):\n",
    "    # Machine learning\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    x = dfr.drop(columns = ['Date', 'intercept', 'Stock_price'])\n",
    "    x = x.to_numpy()\n",
    "    y = dfr.Stock_price.to_numpy()\n",
    "\n",
    "    scaler = StandardScaler()   ####\n",
    "    x = scaler.fit_transform(x) ####\n",
    "\n",
    "    poly_feat = PolynomialFeatures(degree = 5)\n",
    "    x = poly_feat.fit_transform(x)\n",
    "#     poly_model = LinearRegression(fit_intercept = False).fit(x, y)\n",
    "\n",
    "    sample_house = [x[x.shape[0]-1]]\n",
    "    model = LinearRegression()  \n",
    "    model.fit(x, y) \n",
    "    \n",
    "    prediction = model.predict(sample_house)\n",
    "    return prediction\n",
    "\n",
    "def main_bit():\n",
    "    \n",
    "    # find the highest Rsquared of bitcoin \n",
    "    ticker = 'BTC-USD'\n",
    "    tkr = yfinance_df(ticker)   \n",
    "    sfive = tkr\n",
    "\n",
    "    # 10 year treasery bond yield\n",
    "    rate = yfinance_df('^TNX')   \n",
    "    irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "    # CBOE Volatility Index\n",
    "    cboe = yfinance_df('^VIX')   \n",
    "    cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "    # S&P 500\n",
    "    snp = yfinance_df('^GSPC')   \n",
    "    snp = snp[['Date', 'Adj Close']]\n",
    "\n",
    "    # Crude Oil\n",
    "    mar = yfinance_df('CL=F')   \n",
    "    mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Dollar Index\n",
    "    dollar = yfinance_df('DX-Y.NYB')\n",
    "    dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "    # Gold\n",
    "    gold = yfinance_df('GC=F')   \n",
    "    gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "    ################################# End of gathering => Clearning starts  ####################################\n",
    "\n",
    "    # cleaning starts\n",
    "    # cpi index\n",
    "    url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    cpi = webContent[4075:]\n",
    "    cpi = cpi.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(cpi)):\n",
    "        if i%2 == 0:\n",
    "            date.append(cpi[i])\n",
    "        else:\n",
    "            number.append(cpi[i])\n",
    "    cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "    cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "    cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "    # jobless claim index\n",
    "    url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    joblessclaim = webContent[710:]\n",
    "    joblessclaim = joblessclaim.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(joblessclaim)):\n",
    "        if i%2 == 0:\n",
    "            date.append(joblessclaim[i])\n",
    "        else:\n",
    "            number.append(joblessclaim[i])\n",
    "    jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "    # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "    jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "    jls.fillna(method = 'ffill', inplace = True)\n",
    "    jls = jls[jls['Date'] <= currentdate]\n",
    "    \n",
    "    jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "    jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "    jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "    cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "    # inflation index\n",
    "    url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    inf = webContent[1222:]\n",
    "    inf = inf.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(inf)):\n",
    "        if i%2 == 0:\n",
    "            date.append(inf[i])\n",
    "        else:\n",
    "            number.append(inf[i])\n",
    "    inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "    drp = inf[inf.Inf == '.'].index.values\n",
    "    inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "    inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "    inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "    # merge yfinance tickers & cleaning\n",
    "    df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "    df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "    # cleaning - cpi and Jclaim\n",
    "    for i in range(len(df.Cpi)):\n",
    "        if df.Cpi[i] == 0.0:\n",
    "            df.Cpi[i] = df.Cpi[i-1]    \n",
    "    df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "    for i in range(len(df.Jclaim)):\n",
    "        if df.Jclaim[i] == 0.0:\n",
    "            df.Jclaim[i] = df.Jclaim[i-1]\n",
    "    drp = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=drp)\n",
    "\n",
    "    # cleaning - inflation rate since 1990\n",
    "    df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df[df['Date'] > '2003-01-01']\n",
    "    df = df.fillna(0)\n",
    "    if df.tail(1).Inf.values[0] == 0:\n",
    "        df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "    # merge - cboe     \n",
    "    df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - snp\n",
    "    df = df.merge(snp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'snp'}).reset_index(drop=True)\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # merge - Crude Oil Mar 22 \n",
    "    df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - Dollar Index \n",
    "    df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "    # merge - gold\n",
    "    df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "    df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "\n",
    "    ######################## Do Not change above, starts under the last line #######################\n",
    "    # dropna and save the source date\n",
    "    df = df.dropna()\n",
    "\n",
    "    # fillavg = df[df['Inf'] == 0].index.values\n",
    "    # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "    dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "    df = df.drop(index = dfdrop)   \n",
    "\n",
    "    # dfdrop = df[df['Inf'] == 0].index.values\n",
    "    # df = df.drop(index = dfdrop)\n",
    "\n",
    "    fillavg = df[df['Inf'] == 0].index.values\n",
    "    df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "    dfdrop = df[df['snp'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "    df = df.drop(index=dfdrop)\n",
    "\n",
    "    # Fillna Cboe with the highest value for the past 20 working days. \n",
    "    cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "    df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "\n",
    "    # Real GDP\n",
    "    url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "    response = urllib.request.urlopen(url)\n",
    "    webContent = response.read().decode('UTF-8')\n",
    "    Rgdp = webContent[1005:]\n",
    "    Rgdp = Rgdp.split()\n",
    "    date = []\n",
    "    number = []\n",
    "    for i in range(len(Rgdp)):\n",
    "        if i%2 == 0:\n",
    "            date.append(Rgdp[i])\n",
    "        else:\n",
    "            number.append(Rgdp[i])\n",
    "    Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "    Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "    Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "    df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "    df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "    for i in range(len(df.Rgdp)):\n",
    "        if df.Rgdp[i] == 0:\n",
    "            df.Rgdp[i] = df.Rgdp[i-1]\n",
    "    df.dropna(inplace = True)\n",
    "    df = df.drop(columns = ['Close'])\n",
    "    \n",
    "    if df.all().sum() != len(df.columns):\n",
    "        df.to_csv('errorlog_prediction_bitcoin.csv', index = False)\n",
    "        print('\\nZero value checked: ', ticker, '\\n')\n",
    "        print(df.all())\n",
    "        alarm() # if any zero value, alarm\n",
    "    else:\n",
    "        df.to_csv('predict_aim_sourcedata_bitcoin.csv', index = False)\n",
    "        \n",
    "    if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "        print('> The last line == currentdate\\n')\n",
    "    else:\n",
    "        print('>>> The last line != currentdate\\n')\n",
    "#         alarm()\n",
    "        \n",
    "    ##################################### source data save\n",
    "\n",
    "    # range selection - optimized since [2014-09-17]\n",
    "\n",
    "    highest = 0\n",
    "\n",
    "    date = []\n",
    "    ml = []\n",
    "    for i in range(len(df.Date.values)-100):\n",
    "        dfc = df[df['Date'] > df.Date.values[i]]\n",
    "        result = rsquare(dfc)[0]\n",
    "\n",
    "        date.append(df.Date.values[i])\n",
    "        ml.append(result)    \n",
    "\n",
    "        if highest < result:\n",
    "            highest = result\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "    plt.figure(figsize = (16,9))\n",
    "    plt.errorbar(dff.Date, dff.Result)\n",
    "    plt.title('Bitcoin R-squared')\n",
    "\n",
    "    print('__________________________________\\n')\n",
    "    print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "    print('__________________________________')\n",
    "\n",
    "    # rday: most optimized day cut\n",
    "    # numpy.datetime64 to string\n",
    "    ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "    rday_bit = ts.strftime('%Y-%m-%d')\n",
    "    with open(\"rday_bit.txt\", 'w') as f:\n",
    "        f.write(str(rday_bit)) \n",
    "\n",
    "    # Hypothese: \n",
    "    # R-squared represents market uncertainties \n",
    "    # bitcoin having its highest R-squared represents the genuine market price\n",
    "    # The null hypothesis is that bitcoin follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "    # originated from bitcoin stocks which follow long-term price trend. \n",
    "    # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "    dfr = df[df['Date'] > rday_bit]\n",
    "    result_optimized = rsquare(dfr)\n",
    "\n",
    "    # linear regression\n",
    "    prediction = pred_lstm(dfr)    \n",
    "    current = dfr.Stock_price.tail(1).values\n",
    "    current_to_predict = current/prediction\n",
    "\n",
    "    print('\\n• Current Bitcoin:    ', '%.5f' %current)\n",
    "    print('\\n• Predicted Bitcoin:  ', '%.5f' %prediction)\n",
    "    print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "    if current < prediction:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "    else:\n",
    "        print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "    print('__________________________________\\n')\n",
    "\n",
    "    # classifier\n",
    "    prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "\n",
    "    if prediction[1] == 1:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    else:\n",
    "        print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "        print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "    print('__________________________________\\n\\n')\n",
    "\n",
    "    # print out stats summary for multi-linear regression\n",
    "    import statsmodels.api as sm\n",
    "    dfr['intercept'] = 1\n",
    "    lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Oil', 'Dollar', 'Gold', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "    results = lm.fit()\n",
    "    print(results.summary())\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    minutepassed = timechecknow()\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -20:\n",
    "        main_bit()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79301dd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Prediction - Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5edd197a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# import yfinance as yf\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sb\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # find the highest rsquared date\n",
    "# def rsquare(dfc):\n",
    "    \n",
    "#     # Regression analysis\n",
    "#     import statsmodels.api as sm\n",
    "#     dfc['intercept'] = 1\n",
    "#     lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'snp', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "#     results = lm.fit()\n",
    "#     return results.rsquared, results.pvalues\n",
    "\n",
    "# def machinelearning(dfr):\n",
    "#     # Machine learning\n",
    "#     from sklearn.linear_model import LinearRegression\n",
    "#     from sklearn.linear_model import Lasso\n",
    "#     from sklearn.preprocessing import PolynomialFeatures\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#     x = dfr.drop(columns = ['Date', 'Stock_price', 'intercept'])\n",
    "#     x = x.to_numpy()\n",
    "#     y = dfr.Stock_price.to_numpy()\n",
    "    \n",
    "#     scaler = StandardScaler()   ####\n",
    "#     x = scaler.fit_transform(x) ####\n",
    "\n",
    "#     poly_feat = PolynomialFeatures(degree = 5)\n",
    "#     x = poly_feat.fit_transform(x)\n",
    "# #     poly_model = LinearRegression(fit_intercept = False).fit(x, y)\n",
    "    \n",
    "#     sample_house = [x[x.shape[0]-1]]\n",
    "#     model = LinearRegression()  \n",
    "#     model.fit(x, y) \n",
    "    \n",
    "#     prediction = model.predict(sample_house)\n",
    "#     return prediction\n",
    "\n",
    "# def main_inf():\n",
    "    \n",
    "#     # find the highest Rsquared of S&P 500 \n",
    "#     ticker = '^GSPC'\n",
    "#     tkr = yfinance_df(ticker)   \n",
    "#     sfive = tkr\n",
    "\n",
    "#     # 10 year treasery bond yield\n",
    "#     rate = yfinance_df('^TNX')   \n",
    "#     irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "#     # CBOE Volatility Index\n",
    "#     cboe = yfinance_df('^VIX')   \n",
    "#     cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Gold Feb 22 (GC=F)\n",
    "#     gold = yfinance_df('GC=F')   \n",
    "#     gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Crude Oil\n",
    "#     mar = yfinance_df('CL=F')   \n",
    "#     mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Dollar Index\n",
    "#     dollar = yfinance_df('DX-Y.NYB')\n",
    "#     dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Bitcoin (*Last)\n",
    "#     bit = yfinance_df('BTC-USD')   \n",
    "#     bit = bit[['Date', 'Adj Close']]\n",
    "\n",
    "#     # cpi index\n",
    "#     url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     cpi = webContent[4075:]\n",
    "#     cpi = cpi.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(cpi)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(cpi[i])\n",
    "#         else:\n",
    "#             number.append(cpi[i])\n",
    "#     cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "#     cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "#     cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "#     # jobless claim index\n",
    "#     url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     joblessclaim = webContent[710:]\n",
    "#     joblessclaim = joblessclaim.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(joblessclaim)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(joblessclaim[i])\n",
    "#         else:\n",
    "#             number.append(joblessclaim[i])\n",
    "#     jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "#     # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "#     jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "#     jls.fillna(method = 'ffill', inplace = True)\n",
    "#     jls = jls[jls['Date'] <= currentdate]\n",
    "    \n",
    "#     jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "#     jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "#     jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "#     cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "#     # inflation index\n",
    "#     url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     inf = webContent[1222:]\n",
    "#     inf = inf.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(inf)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(inf[i])\n",
    "#         else:\n",
    "#             number.append(inf[i])\n",
    "#     inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "#     drp = inf[inf.Inf == '.'].index.values\n",
    "#     inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "#     inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "#     inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "#     # merge yfinance tickers & cleaning\n",
    "#     df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "#     df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "#     # cleaning - cpi and Jclaim\n",
    "#     for i in range(len(df.Cpi)):\n",
    "#         if df.Cpi[i] == 0.0:\n",
    "#             df.Cpi[i] = df.Cpi[i-1]    \n",
    "#     df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "#     for i in range(len(df.Jclaim)):\n",
    "#         if df.Jclaim[i] == 0.0:\n",
    "#             df.Jclaim[i] = df.Jclaim[i-1]\n",
    "#     drp = df[df['Int_rate'] == 0].index.values\n",
    "#     df = df.drop(index=drp)\n",
    "\n",
    "#     # cleaning - inflation rate since 1990\n",
    "#     df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df[df['Date'] > '2003-01-01']\n",
    "#     df = df.fillna(0)\n",
    "#     if df.tail(1).Inf.values[0] == 0:\n",
    "#         df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "\n",
    "# #     ## added but low P-value, also distort Int-rate    \n",
    "\n",
    "# #     # 30-Year Fixed Rate Mortgage Average & cleaning\n",
    "# #     url = 'https://fred.stlouisfed.org/data/MORTGAGE30US.txt'\n",
    "# #     response = urllib.request.urlopen(url)\n",
    "# #     webContent = response.read().decode('UTF-8')\n",
    "# #     mortg = webContent[32820:]\n",
    "# #     mortg = mortg.split()\n",
    "# #     date = []\n",
    "# #     number = []\n",
    "# #     for i in range(len(mortg)):\n",
    "# #         if i%2 == 0:\n",
    "# #             date.append(mortg[i])\n",
    "# #         else:\n",
    "# #             number.append(mortg[i])\n",
    "# #     mortg = pd.DataFrame({'Date': date, 'Mortg': number})\n",
    "# #     mortg['Date'] = mortg['Date'].astype('datetime64[ns]')\n",
    "# #     mortg['Mortg'] = pd.to_numeric(mortg['Mortg'])\n",
    "\n",
    "# #     df = df.merge(mortg, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "# #     df = df[df['Date'] >= '2003-01-03']\n",
    "# #     df = df.fillna(0).reset_index(drop=True)\n",
    "# #     for i in range(len(df.Mortg)):\n",
    "# #         if df.Mortg[i] == 0.0:\n",
    "# #             df.Mortg[i] = df.Mortg[i-1]\n",
    "# #     drp = df[df['Mortg'] == 0].index.values\n",
    "# #     df = df.drop(index=drp)\n",
    "    \n",
    "    \n",
    "# #     # University of Michigan: Consumer Sentiment \n",
    "# #     url = 'https://fred.stlouisfed.org/data/Umcsent.txt'\n",
    "# #     response = urllib.request.urlopen(url)\n",
    "# #     webContent = response.read().decode('UTF-8')\n",
    "# #     Umcsent = webContent[6897:]\n",
    "# #     Umcsent = Umcsent.split()\n",
    "# #     date = []\n",
    "# #     number = []\n",
    "# #     for i in range(len(Umcsent)):\n",
    "# #         if i%2 == 0:\n",
    "# #             date.append(Umcsent[i])\n",
    "# #         else:\n",
    "# #             number.append(Umcsent[i])\n",
    "# #     Umcsent = pd.DataFrame({'Date': date, 'Umcsent': number})\n",
    "# #     Umcsent['Date'] = Umcsent['Date'].astype('datetime64[ns]')\n",
    "# #     Umcsent['Umcsent'] = pd.to_numeric(Umcsent['Umcsent'])\n",
    "    \n",
    "# #     # cleaning\n",
    "# #     df = df.merge(Umcsent, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "# #     df = df.fillna(0)\n",
    "# #     if df.tail(1).Umcsent.values[0] == 0:\n",
    "# #         df.tail(1).Umcsent = df.tail(2).Umcsent.values[0]\n",
    "\n",
    "#     # merge - cboe     \n",
    "#     df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     # merge - gold\n",
    "#     df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     # merge - Crude Oil Mar 22 \n",
    "#     df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "#     # merge - Dollar Index \n",
    "#     df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "#     # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "#     df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "\n",
    "#     # dropna and save the source date\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     # fillavg = df[df['Inf'] == 0].index.values\n",
    "#     # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "#     dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "#     df = df.drop(index = dfdrop)   \n",
    "\n",
    "#     # dfdrop = df[df['Inf'] == 0].index.values\n",
    "#     # df = df.drop(index = dfdrop)\n",
    "\n",
    "#     fillavg = df[df['Inf'] == 0].index.values\n",
    "#     df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "\n",
    "#     dfdrop = df[df['Gold'] == 0].index.values\n",
    "#     df = df.drop(index=dfdrop)\n",
    "\n",
    "#     dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "#     df = df.drop(index=dfdrop)\n",
    "\n",
    "#     # Fillna Cboe with the highest value for the past 20 working days. \n",
    "#     cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "#     df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "\n",
    "#     # Real GDP\n",
    "#     url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     Rgdp = webContent[1005:]\n",
    "#     Rgdp = Rgdp.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(Rgdp)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(Rgdp[i])\n",
    "#         else:\n",
    "#             number.append(Rgdp[i])\n",
    "#     Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "#     Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "#     Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "#     df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "#     df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "#     for i in range(len(df.Rgdp)):\n",
    "#         if df.Rgdp[i] == 0:\n",
    "#             df.Rgdp[i] = df.Rgdp[i-1]\n",
    "#     df.dropna(inplace = True)\n",
    "#     df = df.drop(columns = ['Close'])\n",
    "    \n",
    "#     ################################################################################## add from below\n",
    "    \n",
    "#     # outcome change ##############################################\n",
    "#     df.rename(columns = {'Stock_price':'change'}, inplace = True)\n",
    "#     df.rename(columns = {'Inf':'Stock_price'}, inplace = True)\n",
    "#     df.rename(columns = {'change':'snp'}, inplace = True)  \n",
    "#     ###############################################################\n",
    "\n",
    "#     if df.all().sum() != len(df.columns):\n",
    "#         df.to_csv('errorlog_prediction_inf.csv', index = False)\n",
    "#         print('\\nZero value checked: ', ticker, '\\n')\n",
    "#         print(df.all())\n",
    "#         alarm() # if any zero value, alarm\n",
    "#     else:\n",
    "#         df.to_csv('predict_aim_sourcedata_inf.csv', index = False)\n",
    "        \n",
    "#     if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "#         print('> The last line == currentdate\\n')\n",
    "#     else:\n",
    "#         print('>>> The last line != currentdate\\n')\n",
    "# #         alarm()\n",
    "        \n",
    "#     ##################################### source data save\n",
    "\n",
    "#     # range selection - optimized since [2014-09-17]\n",
    "\n",
    "#     highest = 0\n",
    "\n",
    "#     date = []\n",
    "#     ml = []\n",
    "#     for i in range(len(df.Date.values)-100):\n",
    "#         dfc = df[df['Date'] > df.Date.values[i]]\n",
    "#         result = rsquare(dfc)[0]\n",
    "\n",
    "#         date.append(df.Date.values[i])\n",
    "#         ml.append(result)    \n",
    "\n",
    "#         if highest < result:\n",
    "#             highest = result\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#     dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "#     plt.figure(figsize = (16,9))\n",
    "#     plt.errorbar(dff.Date, dff.Result)\n",
    "#     plt.title('Inf R-squared')\n",
    "\n",
    "#     print('__________________________________\\n')\n",
    "#     print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "#     print('__________________________________')\n",
    "\n",
    "#     # rday: most optimized day cut\n",
    "#     # numpy.datetime64 to string\n",
    "#     ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "#     rday_snp = ts.strftime('%Y-%m-%d')\n",
    "# #     with open(\"rday_snp.txt\", 'w') as f:\n",
    "# #         f.write(str(rday_snp)) \n",
    "\n",
    "#     # Hypothese: \n",
    "#     # R-squared represents market uncertainties \n",
    "#     # S&P 500 having its highest R-squared represents the genuine market price\n",
    "#     # The null hypothesis is that S&P 500 follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "#     # originated from S&P 500 stocks which follow long-term price trend. \n",
    "#     # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "#     dfr = df[df['Date'] > rday_snp]\n",
    "#     result_optimized = rsquare(dfr)\n",
    "\n",
    "#     # linear regression\n",
    "#     prediction = pred_lstm(dfr)    \n",
    "#     current = dfr.Stock_price.tail(1).values\n",
    "#     current_to_predict = current/prediction\n",
    "\n",
    "#     print('\\n• Current Inflation:    ', '%.5f' %current)\n",
    "#     print('\\n• Predicted Inflation:  ', '%.5f' %prediction)\n",
    "#     print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "#     if current < prediction:\n",
    "#         print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "#     else:\n",
    "#         print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "#     print('__________________________________\\n')\n",
    "\n",
    "#     # classifier\n",
    "#     prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "\n",
    "#     if prediction[1] == 1:\n",
    "#         print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "#         print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "#     else:\n",
    "#         print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "#         print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "#     print('__________________________________\\n\\n')\n",
    "\n",
    "#     # print out stats summary for multi-linear regression\n",
    "#     import statsmodels.api as sm\n",
    "#     dfr['intercept'] = 1\n",
    "#     lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'snp', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "#     results = lm.fit()\n",
    "#     print(results.summary())\n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     minutepassed = timechecknow()\n",
    "#     if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -30:\n",
    "#         main_inf()\n",
    "#     else:\n",
    "#         print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e1f3e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Prediction - Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5e3dfa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# import yfinance as yf\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sb\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # find the highest rsquared date\n",
    "# def rsquare(dfc):\n",
    "    \n",
    "#     # Regression analysis\n",
    "#     import statsmodels.api as sm\n",
    "#     dfc['intercept'] = 1\n",
    "#     lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Inf', 'snp', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "#     results = lm.fit()\n",
    "#     return results.rsquared, results.pvalues\n",
    "\n",
    "# def machinelearning(dfr):\n",
    "#     # Machine learning\n",
    "#     from sklearn.linear_model import LinearRegression\n",
    "#     from sklearn.linear_model import Lasso\n",
    "#     from sklearn.preprocessing import PolynomialFeatures\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#     x = dfr.drop(columns = ['Date', 'Stock_price', 'intercept'])\n",
    "#     x = x.to_numpy()\n",
    "#     y = dfr.Stock_price.to_numpy()\n",
    "    \n",
    "#     scaler = StandardScaler()   ####\n",
    "#     x = scaler.fit_transform(x) ####\n",
    "\n",
    "#     poly_feat = PolynomialFeatures(degree = 5)\n",
    "#     x = poly_feat.fit_transform(x)\n",
    "# #     poly_model = LinearRegression(fit_intercept = False).fit(x, y)\n",
    "    \n",
    "#     sample_house = [x[x.shape[0]-1]]\n",
    "#     model = LinearRegression()  \n",
    "#     model.fit(x, y) \n",
    "    \n",
    "#     prediction = model.predict(sample_house)\n",
    "#     return prediction\n",
    "\n",
    "# def main_int():\n",
    "    \n",
    "#     # find the highest Rsquared of S&P 500 \n",
    "#     ticker = '^GSPC'\n",
    "#     tkr = yfinance_df(ticker)   \n",
    "#     sfive = tkr\n",
    "\n",
    "#     # 10 year treasery bond yield\n",
    "#     rate = yfinance_df('^TNX')   \n",
    "#     irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "#     # CBOE Volatility Index\n",
    "#     cboe = yfinance_df('^VIX')   \n",
    "#     cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Gold Feb 22 (GC=F)\n",
    "#     gold = yfinance_df('GC=F')   \n",
    "#     gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Crude Oil\n",
    "#     mar = yfinance_df('CL=F')   \n",
    "#     mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Dollar Index\n",
    "#     dollar = yfinance_df('DX-Y.NYB')\n",
    "#     dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Bitcoin (*Last)\n",
    "#     bit = yfinance_df('BTC-USD')   \n",
    "#     bit = bit[['Date', 'Adj Close']]\n",
    "\n",
    "#     # cpi index\n",
    "#     url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     cpi = webContent[4075:]\n",
    "#     cpi = cpi.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(cpi)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(cpi[i])\n",
    "#         else:\n",
    "#             number.append(cpi[i])\n",
    "#     cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "#     cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "#     cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "#     # jobless claim index\n",
    "#     url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     joblessclaim = webContent[710:]\n",
    "#     joblessclaim = joblessclaim.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(joblessclaim)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(joblessclaim[i])\n",
    "#         else:\n",
    "#             number.append(joblessclaim[i])\n",
    "#     jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "#     # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "#     jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "#     jls.fillna(method = 'ffill', inplace = True)\n",
    "#     jls = jls[jls['Date'] <= currentdate]\n",
    "    \n",
    "#     jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "#     jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "#     jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "#     cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "#     # inflation index\n",
    "#     url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     inf = webContent[1222:]\n",
    "#     inf = inf.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(inf)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(inf[i])\n",
    "#         else:\n",
    "#             number.append(inf[i])\n",
    "#     inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "#     drp = inf[inf.Inf == '.'].index.values\n",
    "#     inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "#     inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "#     inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "#     # merge yfinance tickers & cleaning\n",
    "#     df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "#     df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "#     # cleaning - cpi and Jclaim\n",
    "#     for i in range(len(df.Cpi)):\n",
    "#         if df.Cpi[i] == 0.0:\n",
    "#             df.Cpi[i] = df.Cpi[i-1]    \n",
    "#     df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "#     for i in range(len(df.Jclaim)):\n",
    "#         if df.Jclaim[i] == 0.0:\n",
    "#             df.Jclaim[i] = df.Jclaim[i-1]\n",
    "#     drp = df[df['Int_rate'] == 0].index.values\n",
    "#     df = df.drop(index=drp)\n",
    "\n",
    "#     # cleaning - inflation rate since 1990\n",
    "#     df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df[df['Date'] > '2003-01-01']\n",
    "#     df = df.fillna(0)\n",
    "#     if df.tail(1).Inf.values[0] == 0:\n",
    "#         df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "\n",
    "# #     ## added but low P-value, also distort Int-rate    \n",
    "\n",
    "# #     # 30-Year Fixed Rate Mortgage Average & cleaning\n",
    "# #     url = 'https://fred.stlouisfed.org/data/MORTGAGE30US.txt'\n",
    "# #     response = urllib.request.urlopen(url)\n",
    "# #     webContent = response.read().decode('UTF-8')\n",
    "# #     mortg = webContent[32820:]\n",
    "# #     mortg = mortg.split()\n",
    "# #     date = []\n",
    "# #     number = []\n",
    "# #     for i in range(len(mortg)):\n",
    "# #         if i%2 == 0:\n",
    "# #             date.append(mortg[i])\n",
    "# #         else:\n",
    "# #             number.append(mortg[i])\n",
    "# #     mortg = pd.DataFrame({'Date': date, 'Mortg': number})\n",
    "# #     mortg['Date'] = mortg['Date'].astype('datetime64[ns]')\n",
    "# #     mortg['Mortg'] = pd.to_numeric(mortg['Mortg'])\n",
    "\n",
    "# #     df = df.merge(mortg, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "# #     df = df[df['Date'] >= '2003-01-03']\n",
    "# #     df = df.fillna(0).reset_index(drop=True)\n",
    "# #     for i in range(len(df.Mortg)):\n",
    "# #         if df.Mortg[i] == 0.0:\n",
    "# #             df.Mortg[i] = df.Mortg[i-1]\n",
    "# #     drp = df[df['Mortg'] == 0].index.values\n",
    "# #     df = df.drop(index=drp)\n",
    "    \n",
    "    \n",
    "# #     # University of Michigan: Consumer Sentiment \n",
    "# #     url = 'https://fred.stlouisfed.org/data/Umcsent.txt'\n",
    "# #     response = urllib.request.urlopen(url)\n",
    "# #     webContent = response.read().decode('UTF-8')\n",
    "# #     Umcsent = webContent[6897:]\n",
    "# #     Umcsent = Umcsent.split()\n",
    "# #     date = []\n",
    "# #     number = []\n",
    "# #     for i in range(len(Umcsent)):\n",
    "# #         if i%2 == 0:\n",
    "# #             date.append(Umcsent[i])\n",
    "# #         else:\n",
    "# #             number.append(Umcsent[i])\n",
    "# #     Umcsent = pd.DataFrame({'Date': date, 'Umcsent': number})\n",
    "# #     Umcsent['Date'] = Umcsent['Date'].astype('datetime64[ns]')\n",
    "# #     Umcsent['Umcsent'] = pd.to_numeric(Umcsent['Umcsent'])\n",
    "    \n",
    "# #     # cleaning\n",
    "# #     df = df.merge(Umcsent, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "# #     df = df.fillna(0)\n",
    "# #     if df.tail(1).Umcsent.values[0] == 0:\n",
    "# #         df.tail(1).Umcsent = df.tail(2).Umcsent.values[0]\n",
    "\n",
    "#     # merge - cboe     \n",
    "#     df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     # merge - gold\n",
    "#     df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     # merge - Crude Oil Mar 22 \n",
    "#     df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "#     # merge - Dollar Index \n",
    "#     df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "#     # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "#     df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "\n",
    "#     # dropna and save the source date\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     # fillavg = df[df['Inf'] == 0].index.values\n",
    "#     # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "#     dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "#     df = df.drop(index = dfdrop)   \n",
    "\n",
    "#     # dfdrop = df[df['Inf'] == 0].index.values\n",
    "#     # df = df.drop(index = dfdrop)\n",
    "\n",
    "#     fillavg = df[df['Inf'] == 0].index.values\n",
    "#     df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "\n",
    "#     dfdrop = df[df['Gold'] == 0].index.values\n",
    "#     df = df.drop(index=dfdrop)\n",
    "\n",
    "#     dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "#     df = df.drop(index=dfdrop)\n",
    "\n",
    "#     # Fillna Cboe with the highest value for the past 20 working days. \n",
    "#     cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "#     df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "\n",
    "#     # Real GDP\n",
    "#     url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     Rgdp = webContent[1005:]\n",
    "#     Rgdp = Rgdp.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(Rgdp)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(Rgdp[i])\n",
    "#         else:\n",
    "#             number.append(Rgdp[i])\n",
    "#     Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "#     Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "#     Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "#     df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "#     df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "#     for i in range(len(df.Rgdp)):\n",
    "#         if df.Rgdp[i] == 0:\n",
    "#             df.Rgdp[i] = df.Rgdp[i-1]\n",
    "#     df.dropna(inplace = True)\n",
    "#     df = df.drop(columns = ['Close'])\n",
    "    \n",
    "#     ################################################################################## add from below\n",
    "    \n",
    "#     # outcome change ##############################################\n",
    "#     df.rename(columns = {'Stock_price':'change'}, inplace = True)\n",
    "#     df.rename(columns = {'Int_rate':'Stock_price'}, inplace = True)\n",
    "#     df.rename(columns = {'change':'snp'}, inplace = True)  \n",
    "#     ###############################################################\n",
    "\n",
    "#     if df.all().sum() != len(df.columns):\n",
    "#         df.to_csv('errorlog_prediction_int.csv', index = False)\n",
    "#         print('\\nZero value checked: ', ticker, '\\n')\n",
    "#         print(df.all())\n",
    "#         alarm() # if any zero value, alarm\n",
    "#     else:\n",
    "#         df.to_csv('predict_aim_sourcedata_int.csv', index = False)\n",
    "        \n",
    "#     if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "#         print('> The last line == currentdate\\n')\n",
    "#     else:\n",
    "#         print('>>> The last line != currentdate\\n')\n",
    "# #         alarm()\n",
    "        \n",
    "#     ##################################### source data save\n",
    "\n",
    "#     # range selection - optimized since [2014-09-17]\n",
    "\n",
    "#     highest = 0\n",
    "\n",
    "#     date = []\n",
    "#     ml = []\n",
    "#     for i in range(len(df.Date.values)-100):\n",
    "#         dfc = df[df['Date'] > df.Date.values[i]]\n",
    "#         result = rsquare(dfc)[0]\n",
    "\n",
    "#         date.append(df.Date.values[i])\n",
    "#         ml.append(result)    \n",
    "\n",
    "#         if highest < result:\n",
    "#             highest = result\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#     dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "#     plt.figure(figsize = (16,9))\n",
    "#     plt.errorbar(dff.Date, dff.Result)\n",
    "#     plt.title('Int R-squared')\n",
    "\n",
    "#     print('__________________________________\\n')\n",
    "#     print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "#     print('__________________________________')\n",
    "\n",
    "#     # rday: most optimized day cut\n",
    "#     # numpy.datetime64 to string\n",
    "#     ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "#     rday_snp = ts.strftime('%Y-%m-%d')\n",
    "# #     with open(\"rday_snp.txt\", 'w') as f:\n",
    "# #         f.write(str(rday_snp)) \n",
    "\n",
    "#     # Hypothese: \n",
    "#     # R-squared represents market uncertainties \n",
    "#     # S&P 500 having its highest R-squared represents the genuine market price\n",
    "#     # The null hypothesis is that S&P 500 follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "#     # originated from S&P 500 stocks which follow long-term price trend. \n",
    "#     # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "#     dfr = df[df['Date'] > rday_snp]\n",
    "#     result_optimized = rsquare(dfr)\n",
    "\n",
    "#     # linear regression\n",
    "#     prediction = pred_lstm(dfr)    \n",
    "#     current = dfr.Stock_price.tail(1).values\n",
    "#     current_to_predict = current/prediction\n",
    "\n",
    "#     print('\\n• Current Int. Rate:    ', '%.5f' %current)\n",
    "#     print('\\n• Predicted Int. Rate:  ', '%.5f' %prediction)\n",
    "#     print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "#     if current < prediction:\n",
    "#         print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "#     else:\n",
    "#         print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "#     print('__________________________________\\n')\n",
    "\n",
    "#     # classifier\n",
    "#     prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "\n",
    "#     if prediction[1] == 1:\n",
    "#         print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "#         print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "#     else:\n",
    "#         print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "#         print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "#     print('__________________________________\\n\\n')\n",
    "\n",
    "#     # print out stats summary for multi-linear regression\n",
    "#     import statsmodels.api as sm\n",
    "#     dfr['intercept'] = 1\n",
    "#     lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Inf', 'snp', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "#     results = lm.fit()\n",
    "#     print(results.summary())\n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     minutepassed = timechecknow()\n",
    "#     if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -30:\n",
    "#         main_int()\n",
    "#     else:\n",
    "#         print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d2b4ec",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## Prediction - Dollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbb5a4c0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# import yfinance as yf\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sb\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # find the highest rsquared date\n",
    "# def rsquare(dfc):\n",
    "    \n",
    "#     # Regression analysis\n",
    "#     import statsmodels.api as sm\n",
    "#     dfc['intercept'] = 1\n",
    "#     lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Inf', 'snp', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Int_rate', 'Rgdp']])\n",
    "#     results = lm.fit()\n",
    "#     return results.rsquared, results.pvalues\n",
    "\n",
    "# def machinelearning(dfr):\n",
    "#     # Machine learning\n",
    "#     from sklearn.linear_model import LinearRegression\n",
    "#     from sklearn.linear_model import Lasso\n",
    "#     from sklearn.preprocessing import PolynomialFeatures\n",
    "#     from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#     x = dfr.drop(columns = ['Date', 'Stock_price', 'intercept'])\n",
    "#     x = x.to_numpy()\n",
    "#     y = dfr.Stock_price.to_numpy()\n",
    "    \n",
    "#     scaler = StandardScaler()   ####\n",
    "#     x = scaler.fit_transform(x) ####\n",
    "\n",
    "#     poly_feat = PolynomialFeatures(degree = 5)\n",
    "#     x = poly_feat.fit_transform(x)\n",
    "# #     poly_model = LinearRegression(fit_intercept = False).fit(x, y)\n",
    "    \n",
    "#     sample_house = [x[x.shape[0]-1]]\n",
    "#     model = LinearRegression()  \n",
    "#     model.fit(x, y) \n",
    "    \n",
    "#     prediction = model.predict(sample_house)\n",
    "#     return prediction\n",
    "\n",
    "# def main_dollar():\n",
    "    \n",
    "#     # find the highest Rsquared of S&P 500 \n",
    "#     ticker = '^GSPC'\n",
    "#     tkr = yfinance_df(ticker)   \n",
    "#     sfive = tkr\n",
    "\n",
    "#     # 10 year treasery bond yield\n",
    "#     rate = yfinance_df('^TNX')   \n",
    "#     irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "#     # CBOE Volatility Index\n",
    "#     cboe = yfinance_df('^VIX')   \n",
    "#     cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Gold Feb 22 (GC=F)\n",
    "#     gold = yfinance_df('GC=F')   \n",
    "#     gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Crude Oil\n",
    "#     mar = yfinance_df('CL=F')   \n",
    "#     mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Dollar Index\n",
    "#     dollar = yfinance_df('DX-Y.NYB')\n",
    "#     dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "#     # Bitcoin (*Last)\n",
    "#     bit = yfinance_df('BTC-USD')   \n",
    "#     bit = bit[['Date', 'Adj Close']]\n",
    "\n",
    "#     # cpi index\n",
    "#     url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     cpi = webContent[4075:]\n",
    "#     cpi = cpi.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(cpi)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(cpi[i])\n",
    "#         else:\n",
    "#             number.append(cpi[i])\n",
    "#     cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "#     cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "#     cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "#     # jobless claim index\n",
    "#     url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     joblessclaim = webContent[710:]\n",
    "#     joblessclaim = joblessclaim.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(joblessclaim)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(joblessclaim[i])\n",
    "#         else:\n",
    "#             number.append(joblessclaim[i])\n",
    "#     jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "    \n",
    "#     # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "#     jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "#     jls.fillna(method = 'ffill', inplace = True)\n",
    "#     jls = jls[jls['Date'] <= currentdate]\n",
    "    \n",
    "#     jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "#     jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "#     jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "#     cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "#     # inflation index\n",
    "#     url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     inf = webContent[1222:]\n",
    "#     inf = inf.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(inf)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(inf[i])\n",
    "#         else:\n",
    "#             number.append(inf[i])\n",
    "#     inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "#     drp = inf[inf.Inf == '.'].index.values\n",
    "#     inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "#     inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "#     inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "#     # merge yfinance tickers & cleaning\n",
    "#     df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.merge(sfive, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "#     df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "#     # cleaning - cpi and Jclaim\n",
    "#     for i in range(len(df.Cpi)):\n",
    "#         if df.Cpi[i] == 0.0:\n",
    "#             df.Cpi[i] = df.Cpi[i-1]    \n",
    "#     df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "#     for i in range(len(df.Jclaim)):\n",
    "#         if df.Jclaim[i] == 0.0:\n",
    "#             df.Jclaim[i] = df.Jclaim[i-1]\n",
    "#     drp = df[df['Int_rate'] == 0].index.values\n",
    "#     df = df.drop(index=drp)\n",
    "\n",
    "#     # cleaning - inflation rate since 1990\n",
    "#     df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df[df['Date'] > '2003-01-01']\n",
    "#     df = df.fillna(0)\n",
    "#     if df.tail(1).Inf.values[0] == 0:\n",
    "#         df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "\n",
    "# #     ## added but low P-value, also distort Int-rate    \n",
    "\n",
    "# #     # 30-Year Fixed Rate Mortgage Average & cleaning\n",
    "# #     url = 'https://fred.stlouisfed.org/data/MORTGAGE30US.txt'\n",
    "# #     response = urllib.request.urlopen(url)\n",
    "# #     webContent = response.read().decode('UTF-8')\n",
    "# #     mortg = webContent[32820:]\n",
    "# #     mortg = mortg.split()\n",
    "# #     date = []\n",
    "# #     number = []\n",
    "# #     for i in range(len(mortg)):\n",
    "# #         if i%2 == 0:\n",
    "# #             date.append(mortg[i])\n",
    "# #         else:\n",
    "# #             number.append(mortg[i])\n",
    "# #     mortg = pd.DataFrame({'Date': date, 'Mortg': number})\n",
    "# #     mortg['Date'] = mortg['Date'].astype('datetime64[ns]')\n",
    "# #     mortg['Mortg'] = pd.to_numeric(mortg['Mortg'])\n",
    "\n",
    "# #     df = df.merge(mortg, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "# #     df = df[df['Date'] >= '2003-01-03']\n",
    "# #     df = df.fillna(0).reset_index(drop=True)\n",
    "# #     for i in range(len(df.Mortg)):\n",
    "# #         if df.Mortg[i] == 0.0:\n",
    "# #             df.Mortg[i] = df.Mortg[i-1]\n",
    "# #     drp = df[df['Mortg'] == 0].index.values\n",
    "# #     df = df.drop(index=drp)\n",
    "    \n",
    "    \n",
    "# #     # University of Michigan: Consumer Sentiment \n",
    "# #     url = 'https://fred.stlouisfed.org/data/Umcsent.txt'\n",
    "# #     response = urllib.request.urlopen(url)\n",
    "# #     webContent = response.read().decode('UTF-8')\n",
    "# #     Umcsent = webContent[6897:]\n",
    "# #     Umcsent = Umcsent.split()\n",
    "# #     date = []\n",
    "# #     number = []\n",
    "# #     for i in range(len(Umcsent)):\n",
    "# #         if i%2 == 0:\n",
    "# #             date.append(Umcsent[i])\n",
    "# #         else:\n",
    "# #             number.append(Umcsent[i])\n",
    "# #     Umcsent = pd.DataFrame({'Date': date, 'Umcsent': number})\n",
    "# #     Umcsent['Date'] = Umcsent['Date'].astype('datetime64[ns]')\n",
    "# #     Umcsent['Umcsent'] = pd.to_numeric(Umcsent['Umcsent'])\n",
    "    \n",
    "# #     # cleaning\n",
    "# #     df = df.merge(Umcsent, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "# #     df = df.fillna(0)\n",
    "# #     if df.tail(1).Umcsent.values[0] == 0:\n",
    "# #         df.tail(1).Umcsent = df.tail(2).Umcsent.values[0]\n",
    "\n",
    "#     # merge - cboe     \n",
    "#     df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     # merge - gold\n",
    "#     df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     # merge - Crude Oil Mar 22 \n",
    "#     df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "#     # merge - Dollar Index \n",
    "#     df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "#     # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "#     df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "#     df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "\n",
    "#     # dropna and save the source date\n",
    "#     df = df.dropna()\n",
    "\n",
    "#     # fillavg = df[df['Inf'] == 0].index.values\n",
    "#     # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "#     dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "#     df = df.drop(index = dfdrop)   \n",
    "\n",
    "#     # dfdrop = df[df['Inf'] == 0].index.values\n",
    "#     # df = df.drop(index = dfdrop)\n",
    "\n",
    "#     fillavg = df[df['Inf'] == 0].index.values\n",
    "#     df['Inf'].loc[fillavg] = df['Inf'].tail(5).max() \n",
    "\n",
    "\n",
    "#     dfdrop = df[df['Gold'] == 0].index.values\n",
    "#     df = df.drop(index=dfdrop)\n",
    "\n",
    "#     dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "#     df = df.drop(index=dfdrop)\n",
    "\n",
    "#     # Fillna Cboe with the highest value for the past 20 working days. \n",
    "#     cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "#     df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "\n",
    "#     # Real GDP\n",
    "#     url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "#     response = urllib.request.urlopen(url)\n",
    "#     webContent = response.read().decode('UTF-8')\n",
    "#     Rgdp = webContent[1005:]\n",
    "#     Rgdp = Rgdp.split()\n",
    "#     date = []\n",
    "#     number = []\n",
    "#     for i in range(len(Rgdp)):\n",
    "#         if i%2 == 0:\n",
    "#             date.append(Rgdp[i])\n",
    "#         else:\n",
    "#             number.append(Rgdp[i])\n",
    "#     Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "#     Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "#     Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "#     df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "#     df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "#     for i in range(len(df.Rgdp)):\n",
    "#         if df.Rgdp[i] == 0:\n",
    "#             df.Rgdp[i] = df.Rgdp[i-1]\n",
    "#     df.dropna(inplace = True)\n",
    "#     df = df.drop(columns = ['Close'])\n",
    "    \n",
    "#     ################################################################################## add from below\n",
    "    \n",
    "#     # outcome change ###################################################################\n",
    "#     df.rename(columns = {'Stock_price':'change'}, inplace = True)\n",
    "#     df.rename(columns = {'Dollar':'Stock_price'}, inplace = True) #<- Change index here\n",
    "#     df.rename(columns = {'change':'snp'}, inplace = True)  \n",
    "#     ####################################################################################\n",
    "\n",
    "#     if df.all().sum() != len(df.columns):\n",
    "#         df.to_csv('errorlog_prediction_dollar.csv', index = False)\n",
    "#         print('\\nZero value checked: ', ticker, '\\n')\n",
    "#         print(df.all())\n",
    "#         alarm() # if any zero value, alarm\n",
    "#     else:\n",
    "#         df.to_csv('predict_aim_sourcedata_dollar.csv', index = False)\n",
    "        \n",
    "#     if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "#         print('> The last line == currentdate\\n')\n",
    "#     else:\n",
    "#         print('>>> The last line != currentdate\\n')\n",
    "# #         alarm()\n",
    "        \n",
    "#     ##################################### source data save\n",
    "\n",
    "#     # range selection - optimized since [2014-09-17]\n",
    "\n",
    "#     highest = 0\n",
    "\n",
    "#     date = []\n",
    "#     ml = []\n",
    "#     for i in range(len(df.Date.values)-100):\n",
    "#         dfc = df[df['Date'] > df.Date.values[i]]\n",
    "#         result = rsquare(dfc)[0]\n",
    "\n",
    "#         date.append(df.Date.values[i])\n",
    "#         ml.append(result)    \n",
    "\n",
    "#         if highest < result:\n",
    "#             highest = result\n",
    "#         else:\n",
    "#             pass\n",
    "\n",
    "#     dff = pd.DataFrame({'Date': date, 'Result': ml})    \n",
    "#     plt.figure(figsize = (16,9))\n",
    "#     plt.errorbar(dff.Date, dff.Result)\n",
    "#     plt.title('Dollar R-squared')\n",
    "\n",
    "#     print('__________________________________\\n')\n",
    "#     print('The highest R-squared: \\n\\n', dff[dff['Result']==dff['Result'].max()])\n",
    "#     print('__________________________________')\n",
    "\n",
    "#     # rday: most optimized day cut\n",
    "#     # numpy.datetime64 to string\n",
    "#     ts = pd.to_datetime(str(dff[dff['Result']==dff['Result'].max()].Date.values[0])) \n",
    "#     rday_snp = ts.strftime('%Y-%m-%d')\n",
    "# #     with open(\"rday_snp.txt\", 'w') as f:\n",
    "# #         f.write(str(rday_snp)) \n",
    "\n",
    "#     # Hypothese: \n",
    "#     # R-squared represents market uncertainties \n",
    "#     # S&P 500 having its highest R-squared represents the genuine market price\n",
    "#     # The null hypothesis is that S&P 500 follows the best-optimized market, which is most genuine because we do long-term investing\n",
    "#     # originated from S&P 500 stocks which follow long-term price trend. \n",
    "#     # therefore, we apply the highest rsquared day; Rday. \n",
    "\n",
    "#     dfr = df[df['Date'] > rday_snp]\n",
    "#     result_optimized = rsquare(dfr)\n",
    "\n",
    "#     # linear regression\n",
    "#     prediction = pred_lstm(dfr)    \n",
    "#     current = dfr.Stock_price.tail(1).values\n",
    "#     current_to_predict = current/prediction\n",
    "\n",
    "#     print('\\n• Current Dollar:    ', '%.5f' %current)\n",
    "#     print('\\n• Predicted Dollar:  ', '%.5f' %prediction)\n",
    "#     print('\\n• Current to Predicted: ', '%.5f' %current_to_predict)\n",
    "   \n",
    "#     if current < prediction:\n",
    "#         print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "#     else:\n",
    "#         print(f\"\\n• Linear Regression Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "#     print('__________________________________\\n')\n",
    "\n",
    "#     # classifier\n",
    "#     prediction = classifer_data_input(dfr, moving_avg_value) # [0]: precision / [1]: prediction result\n",
    "\n",
    "#     if prediction[1] == 1:\n",
    "#         print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'UP' + '\\033[0m')\n",
    "#         print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "#     else:\n",
    "#         print(f\"• {moving_avg_value} days MVG Result: \", '\\033[1m' + 'DOWN' + '\\033[0m')\n",
    "#         print('\\n• Precision Score =', np.round(prediction[0]*100, 2), '%')\n",
    "#     print('__________________________________\\n\\n')\n",
    "\n",
    "#     # print out stats summary for multi-linear regression\n",
    "#     import statsmodels.api as sm\n",
    "#     dfr['intercept'] = 1\n",
    "#     lm = sm.OLS(dfr['Stock_price'], dfr[['intercept', 'Cpi', 'Jclaim', 'Inf', 'snp', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Int_rate', 'Rgdp']]) # exclude 'Bitcoin' due to high pvalue\n",
    "#     results = lm.fit()\n",
    "#     print(results.summary())\n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     minutepassed = timechecknow()\n",
    "#     if day_check() != 'Saturday' and day_check() != 'Sunday' and minutepassed < -30:\n",
    "#         main_dollar()\n",
    "#     else:\n",
    "#         print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe646f82",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647a9437",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prediction source data is cut based on the most released date of Inf_rate, \n",
    "# which has no update during the weekends.\n",
    "from os.path import exists\n",
    "import time\n",
    "today_to_add = date.today()\n",
    "import pandas_datareader as data\n",
    "data_source = 'yahoo'\n",
    "\n",
    "def compensation_plot(df):\n",
    "  \n",
    "    today = pd.read_csv('predict_valuation_today %s.csv' %currentdate)\n",
    "\n",
    "    # include only list to Buy monitor\n",
    "    df = pd.read_csv('predict_valuation.csv').sort_values('symbol')\n",
    "    df = df.loc[df['symbol'].isin(today['symbol'].values)]\n",
    "\n",
    "    df1 = df.query('symbol == \"BTC-USD\" or symbol == \"USO\" or symbol == \"GLD\"')\n",
    "    df = df.query('symbol != \"BTC-USD\" and symbol != \"USO\" and symbol != \"GLD\"')\n",
    "\n",
    "    plt.figure(figsize = (16,15))\n",
    "\n",
    "    plt.subplot(2,1,1)\n",
    "    plot1 = pd.DataFrame(df.groupby('symbol')['error'].mean()).sort_values('symbol').reset_index()\n",
    "    plt.bar(plot1.symbol, plot1.error)\n",
    "    plt.title('Compensation error: gap rate between Prediction and Price')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "    plt.subplot(2,1,2)\n",
    "    plot2 = pd.DataFrame(df.groupby('symbol')['compensation_corr'].mean()).sort_values('symbol').reset_index()\n",
    "    plt.bar(plot2.symbol, plot2.compensation_corr, color = 'g')\n",
    "    plt.title('Correlation coefficient with S&P 500 for compensation(*Must match with above)')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    plt.show()\n",
    "    \n",
    "#     print(df1.groupby('symbol')[['error','compensation_corr']].mean())\n",
    "\n",
    "    if len(plot1.symbol) != len(plot2.symbol):\n",
    "        print('!! plot1 is not equal to plot2 !!')\n",
    "        alarm()\n",
    "    else:\n",
    "        pass    \n",
    "    \n",
    "    pd.set_option('display.max_row', 50)\n",
    "    predict_valuation_today = pd.read_csv('predict_valuation_today %s.csv' %currentdate).sort_values(['up_down', 'valuation', 'error'], ascending = False)\n",
    "    print('\\nAverage Error(%): ', np.average(predict_valuation_today.error), '\\n')\n",
    "    print(predict_valuation_today.reset_index(drop=True)[['symbol', 'up_down', 'valuation']])\n",
    "    \n",
    "\n",
    "def predict(target, ticker):\n",
    "    \n",
    "    target = None\n",
    "    \n",
    "    if ticker == 'GLD':\n",
    "        target = 'GLD'\n",
    "    elif ticker == 'USO':\n",
    "        target = 'USO'       \n",
    "    elif ticker == 'BTC-USD':\n",
    "        target = 'BTC-USD'\n",
    "    else:\n",
    "        target = '^GSPC'    \n",
    "\n",
    "    # target to get corr\n",
    "    target_index = yfinance_df(target)  \n",
    "    target_index = target_index[['Date', 'Adj Close']]\n",
    "    # ticker to get corr\n",
    "    ticker_index = yfinance_df(ticker)   \n",
    "    # corr\n",
    "    cor = pd.merge(target_index, ticker_index, how = 'inner', on = 'Date')\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    cor['intercept'] = 1\n",
    "    lm = sm.OLS(cor['Adj Close_y'], cor[['intercept', 'Adj Close_x']])\n",
    "    results = lm.fit()\n",
    "#     compensation_cor = results.rsquared * results.params[1]    \n",
    "#     compensation_cor = results.params[1] * np.absolute(cor.corr()['Adj Close_x'][1])\n",
    "    compensation_cor = np.absolute(cor.corr()['Adj Close_x'][1])\n",
    "    \n",
    "    # check if exist for today\n",
    "    # if exists, only update the current rows\n",
    "    \n",
    "    # remove past data\n",
    "    sourcedata_exists = exists(f\"predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv\")\n",
    "    if sourcedata_exists == True:\n",
    "        import os\n",
    "        os.remove(f\"predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    sourcedata_exists = exists(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "    if target == '^GSPC' and sourcedata_exists == True:\n",
    "        print(f\"File Exists - Target: {target} & Ticker: {ticker} ..... Updating the current values\", end = '                                                                                                                                          \\r')\n",
    "        df = pd.read_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "        realtime_vars = {'Int_rate':'^TNX', \n",
    "                         'Stock_price':ticker, \n",
    "                         'Cboe':'^VIX', \n",
    "                         'Gold':'GC=F', \n",
    "                         'Oil':'CL=F', \n",
    "                         'Bitcoin':'BTC-USD', \n",
    "                         'Dollar':'DX-Y.NYB'}\n",
    "        for i, v in realtime_vars.items():\n",
    "            df.loc[df.shape[0]-1, [i]] = yfinance_df_min(v)['Adj Close'].values[0]\n",
    "    else:\n",
    "        print(f\"Initial downloading - Target: {target} & Ticker: {ticker}\", end = '                                                                                                         \\r')\n",
    "    # if not exists, gather data from the beginning\n",
    "    # 10 year treasery bold yield\n",
    "        rate = yfinance_df('^TNX')  \n",
    "        irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "        # CBOE Volatility Index\n",
    "        cboe = yfinance_df('^VIX')\n",
    "        cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "        if target == '^GSPC':\n",
    "            # Gold Feb 22 (GC=F)\n",
    "            gold = yfinance_df('GC=F')  \n",
    "            gold = gold[['Date', 'Adj Close']]    \n",
    "\n",
    "        elif target == 'GLD' or target == 'USO' or target == 'BTC-USD':\n",
    "            # S&P 500\n",
    "            snp = yfinance_df('^GSPC')\n",
    "            snp = snp[['Date', 'Adj Close']]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Crude Oil\n",
    "        mar = yfinance_df('CL=F')  \n",
    "        mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "        if target == '^GSPC' or target == 'GLD' or target == 'USO':\n",
    "            # Bitcoin (*last)\n",
    "            bit = yfinance_df('BTC-USD')   \n",
    "            bit = bit[['Date', 'Adj Close']]\n",
    "        elif target == 'BTC-USD':\n",
    "            # Gold\n",
    "            gold = yfinance_df('GC=F')   \n",
    "            gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Dollar Index\n",
    "        dollar = yfinance_df('DX-Y.NYB')\n",
    "        dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "        # cpi index\n",
    "        url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        cpi = webContent[4075:]\n",
    "        cpi = cpi.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(cpi)):\n",
    "            if i%2 == 0:\n",
    "                date.append(cpi[i])\n",
    "            else:\n",
    "                number.append(cpi[i])\n",
    "        cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "        cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "        cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "        # jobless claim index\n",
    "        url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        joblessclaim = webContent[710:]\n",
    "        joblessclaim = joblessclaim.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(joblessclaim)):\n",
    "            if i%2 == 0:\n",
    "                date.append(joblessclaim[i])\n",
    "            else:\n",
    "                number.append(joblessclaim[i])\n",
    "        jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "        \n",
    "        # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "        jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "        jls.fillna(method = 'ffill', inplace = True)\n",
    "        jls = jls[jls['Date'] <= currentdate]\n",
    "        \n",
    "        jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "        jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "        jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "        cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "        # inflation index\n",
    "        url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        inf = webContent[1222:]\n",
    "        inf = inf.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(inf)):\n",
    "            if i%2 == 0:\n",
    "                date.append(inf[i])\n",
    "            else:\n",
    "                number.append(inf[i])\n",
    "        inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "        drp = inf[inf.Inf == '.'].index.values\n",
    "        inf = inf.drop(index = drp).reset_index(drop=True)\n",
    "        inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "        inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # combine & cleaning\n",
    "        df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.merge(ticker_index, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "        df = df.fillna(0)\n",
    "        df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "        # cleaning - cpi and Jclaim \n",
    "        for i in range(len(df.Cpi)):\n",
    "            if df.Cpi[i] == 0:\n",
    "                df.Cpi[i] = df.Cpi[i-1]    \n",
    "        df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "        for i in range(len(df.Jclaim)):\n",
    "            if df.Jclaim[i] == 0.0:\n",
    "                df.Jclaim[i] = df.Jclaim[i-1]\n",
    "        drp = df[df['Int_rate'] == 0].index.values\n",
    "        df = df.drop(index=drp)\n",
    "\n",
    "        # cleaning - inflation rate since 1990\n",
    "        df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df[df['Date'] > '2003-01-01']\n",
    "        df = df.fillna(0)\n",
    "        if df.tail(1).Inf.values[0] == 0:\n",
    "            df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "        # merge - cboe     \n",
    "        df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        if target == '^GSPC':  \n",
    "            # merge - gold\n",
    "            df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "            df = df.fillna(0)\n",
    "\n",
    "        elif target == 'GLD' or target == 'USO' or target == 'BTC-USD':  \n",
    "            # merge - snp\n",
    "            df = df.merge(snp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'snp'}).reset_index(drop=True)\n",
    "            df = df.fillna(0)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # merge - Crude Oil Mar 22 \n",
    "        df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "        if target == 'BTC-USD':\n",
    "            # merge - gold\n",
    "            df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "            df = df.fillna(0)\n",
    "        elif target == '^GSPC' or target == 'GLD' or target == 'USO':\n",
    "            # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "            df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # merge - Dollar Index \n",
    "        df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "        # dropna and save the source date\n",
    "        df = df.dropna()\n",
    "\n",
    "        # fillavg = df[df['Inf'] == 0].index.values\n",
    "        # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "        dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "        df = df.drop(index = dfdrop)   \n",
    "\n",
    "        # dfdrop = df[df['Inf'] == 0].index.values\n",
    "        # df = df.drop(index = dfdrop)\n",
    "\n",
    "        fillavg = df[df['Inf'] == 0].index.values\n",
    "        df['Inf'].loc[fillavg] = df['Inf'].tail(5).max()             \n",
    "\n",
    "        # Fillna Cboe with the highest value for the past 20 working days. \n",
    "        cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "        df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "        \n",
    "        # Real GDP\n",
    "        url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        Rgdp = webContent[1005:]\n",
    "        Rgdp = Rgdp.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(Rgdp)):\n",
    "            if i%2 == 0:\n",
    "                date.append(Rgdp[i])\n",
    "            else:\n",
    "                number.append(Rgdp[i])\n",
    "        Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "        Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "        Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "        df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "        df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "        for i in range(len(df.Rgdp)):\n",
    "            if df.Rgdp[i] == 0:\n",
    "                df.Rgdp[i] = df.Rgdp[i-1]\n",
    "        df.dropna(inplace = True)\n",
    "\n",
    "        if target == '^GSPC':\n",
    "            dfdrop = df[df['Gold'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "\n",
    "        elif target == 'BTC-USD':\n",
    "            dfdrop = df[df['Gold'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "            dfdrop = df[df['snp'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "\n",
    "        elif target == 'GLD' or target == 'USO':\n",
    "            dfdrop = df[df['snp'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "\n",
    "        else:\n",
    "            pass            \n",
    "\n",
    "        dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "        df = df.drop(index=dfdrop)\n",
    "        df = df.drop(columns = ['Close'])\n",
    "        ### end of new gathering ####\n",
    "\n",
    "    if df.all().sum() != len(df.columns):\n",
    "        df.to_csv('errorlog_prediction_monitoring_%s.csv' %ticker, index = False)\n",
    "        print('\\nZero value checked - Check error log file: ', ticker, '\\n')    \n",
    "        alarm() # if any zero value, alarm\n",
    "        print(df.all())\n",
    "        error = yfinance_df(ticker) \n",
    "        print('Regeneration by yfinance with the ticker\\n', error.all())\n",
    "\n",
    "        print('\\nRe-checking: ', ticker, '\\n')\n",
    "        predict(ticker)\n",
    "    else:\n",
    "        if str(df[-1:].Date.values[0])[:10] == currentdate:\n",
    "            df.to_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\", index = False)\n",
    "        else:\n",
    "            print(\"\\n\\nThe last line of the dateframe does not match with the currentdate.\\n>>> The file is not saved. <<<\\n\\n\")\n",
    "    ################################################################################### new merge - add above\n",
    "    \n",
    "    with open(\"rday_oil.txt\", 'r') as f:\n",
    "        rday_oil = [line.rstrip('\\n') for line in f][0]\n",
    "    with open(\"rday_gold.txt\", 'r') as f:\n",
    "        rday_gold = [line.rstrip('\\n') for line in f][0]\n",
    "    with open(\"rday_bit.txt\", 'r') as f:\n",
    "        rday_bit = [line.rstrip('\\n') for line in f][0]\n",
    "    with open(\"rday_snp.txt\", 'r') as f:\n",
    "        rday_snp = [line.rstrip('\\n') for line in f][0]\n",
    "        \n",
    "    \n",
    "    # range selection - COVID19\n",
    "    if target == '^GSPC':\n",
    "        dfc = df[df['Date'] > rday_snp]\n",
    "    elif target == 'GLD':\n",
    "        dfc = df[df['Date'] > rday_gold]\n",
    "    elif target == 'USO':\n",
    "        dfc = df[df['Date'] > rday_oil]\n",
    "    elif target == 'BTC-USD':\n",
    "        dfc = df[df['Date'] > rday_bit]\n",
    "    else:\n",
    "        pass    \n",
    "    df_classifier = dfc.copy()\n",
    "\n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    dfc['intercept'] = 1\n",
    "    \n",
    "    if target == '^GSPC':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) \n",
    "    elif target == 'GLD':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'snp', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    elif target == 'BTC-USD':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Oil', 'Dollar', 'Gold', 'Rgdp']])\n",
    "    elif target == 'USO':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    results = lm.fit()\n",
    "\n",
    "    # linear regression\n",
    "    if target == '^GSPC':\n",
    "        dfl = dfc.drop(columns = ['intercept'])\n",
    "    elif target == 'GLD':\n",
    "        dfl = dfc.drop(columns = ['intercept', 'Cboe'])      \n",
    "    elif target == 'USO':\n",
    "        dfl = dfc.drop(columns = ['intercept', 'Oil']) \n",
    "    elif target == 'BTC-USD':\n",
    "        dfl = dfc.drop(columns = ['intercept'])\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    prediction = pred_lstm(dfl) \n",
    "    err = (prediction - dfc.tail(1).Stock_price.values[0])/dfc.tail(1).Stock_price.values[0]\n",
    "    tprice = yfinance_df(ticker).tail(1)['Adj Close'].values[0]\n",
    "    \n",
    "    # classifier\n",
    "    prediction_classifier = classifer_data_input(df_classifier, moving_avg_value) # [0]: precision / [1]: prediction result > pass df w/t r-day considered\n",
    "\n",
    "    # check the most recent date if yFinance includes it    \n",
    "    lastdate = str(df[-1:].Date.values[0])[:10]\n",
    "    if lastdate == currentdate:\n",
    "        prediction_up_down = prediction_classifier[1]\n",
    "    else:\n",
    "        prediction_up_down = 0\n",
    "    \n",
    "    return [today_to_add, ticker, prediction, tprice, results.rsquared, err, compensation_cor, prediction_up_down]\n",
    "\n",
    "\n",
    "def valuation(ticker):\n",
    "    portfolio = ticker\n",
    "\n",
    "    pricepredict = {}\n",
    "    date = []\n",
    "    symbol = []\n",
    "    prediction = []\n",
    "    price = []\n",
    "    rsquared = []\n",
    "    error = []\n",
    "    compensation = []\n",
    "    up_down = []\n",
    "    target = None\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "    for i in tqdm(range(len(portfolio))):\n",
    "        time.sleep(0.3)\n",
    "        try:\n",
    "            print('Compensating: ', portfolio[i], end = '                                                                                \\r')\n",
    "            result = predict(target, portfolio[i])\n",
    "            date.append(result[0])\n",
    "            symbol.append(result[1])\n",
    "            prediction.append(result[2])\n",
    "            price.append(result[3])\n",
    "            rsquared.append(result[4])\n",
    "            error.append(result[5])\n",
    "            compensation.append(result[6])\n",
    "            up_down.append(result[7])\n",
    "        except:\n",
    "            print('Except: Ticker valuation - Check error log file: ', portfolio[i])\n",
    "            alarm()\n",
    "            pass\n",
    "\n",
    "    pricepredict['date'] = date\n",
    "    pricepredict['time'] = timechecknow()\n",
    "    pricepredict['symbol'] = symbol\n",
    "    pricepredict['prediction'] = prediction\n",
    "    pricepredict['price'] = price\n",
    "    pricepredict['rsquared'] = rsquared\n",
    "    pricepredict['error'] = np.absolute(error)\n",
    "    pricepredict['compensation_corr'] = compensation\n",
    "    pricepredict['up_down'] = up_down\n",
    "\n",
    "    # to send today_predict\n",
    "    df = pd.DataFrame(pricepredict)  \n",
    "\n",
    "    # apply accmulated gap to compensate 'gap'\n",
    "    dfacc = pd.read_csv('predict_valuation.csv').sort_values('symbol')\n",
    "    dfacc = dfacc.dropna()\n",
    "\n",
    "    average_gap = []\n",
    "    for i in df.symbol:\n",
    "        if i in pd.DataFrame(dfacc.groupby('symbol').count()).reset_index().symbol.values:\n",
    "            average_gap.append(dfacc[dfacc['symbol'] == i].gap.min())\n",
    "        else:\n",
    "            average_gap.append(np.absolute(df[df['symbol'] == i].prediction.values[0] - df[df['symbol'] == i].price.values[0]))\n",
    "\n",
    "    df['gap'] = average_gap\n",
    "\n",
    "    # compensate - original predicted price is compensated by multipying coefficient correlation\n",
    "    #              this also partially compensates rsquared per ticker since corr*corr becomes rsquared\n",
    "    # again, df['gap'] here is calculated from accmulated data, 'predict', and save the result to 'predict_valuation_today'\n",
    "    for i in range(len(df['symbol'])):\n",
    "        if df['symbol'][i] == 'GLD' or df['symbol'][i] == 'USO' or df['symbol'][i] == 'BTC-USD':\n",
    "            pass\n",
    "        else:\n",
    "            if df['prediction'][i] < df['price'][i]:\n",
    "                df['prediction'][i] = df['prediction'][i]# + (df['gap'][i] * df['compensation_corr'][i])\n",
    "            else:\n",
    "                df['prediction'][i] = df['prediction'][i]# - (df['gap'][i] * df['compensation_corr'][i])\n",
    "            \n",
    "    # final valuation, and save the result to 'predict_valuation_today'\n",
    "    df.loc[df['prediction'] - df['price'] > 0, 'valuation'] = 1\n",
    "    df.loc[df['prediction'] - df['price'] <= 0, 'valuation'] = 0\n",
    "    df['gap'] = np.absolute(df['prediction'] - df['price'])\n",
    "    df['error'] = np.absolute(df['prediction'] - df['price']) / df['price']\n",
    "\n",
    "    today = df.sort_values('error').reset_index(drop=True)\n",
    "    today.to_csv('predict_valuation_today %s.csv' %currentdate, index = False)\n",
    "\n",
    "    # update 'predict_valuation_today' into 'predict_valuation'\n",
    "    df = pd.read_csv('predict_valuation.csv')\n",
    "    df = pd.concat([df, today])\n",
    "    df.to_csv('predict_valuation.csv', index = False)\n",
    "\n",
    "    # plot and data print\n",
    "    compensation_plot(df)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The valuation gets ready.' + '\\033[0m', '\\n')    \n",
    "    \n",
    "    \n",
    "# valuation(pd.read_csv('srank.csv').Symbol[:100].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9259dd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "hide_input": true
   },
   "source": [
    "#  Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ef4a23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## Portfolio - Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aafe8380",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "import csv\n",
    "import jupyter_beeper\n",
    "\n",
    "# email with attachments\n",
    "import email, smtplib, ssl\n",
    "from email import encoders\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "import pytz\n",
    "import sean\n",
    "\n",
    "# Tableau\n",
    "from sodapy import Socrata\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "snp = pd.read_csv('./snp500/snp500.csv')\n",
    "\n",
    "\n",
    "def tableau_snp500():\n",
    "    with open(\"currentdate.txt\", 'r') as f:\n",
    "        currentdate = int([line.rstrip('\\n') for line in f][0])\n",
    "    currentdate = str(currentdate)[0:4]+'-'+str(currentdate)[4:6]+'-'+str(currentdate)[6:8]\n",
    "\n",
    "    df = pd.read_csv(f\"snp500 {currentdate}.csv\")\n",
    "    # Sean score preparation\n",
    "    def score(grade):\n",
    "        if grade == 'A+':\n",
    "            return np.arange(1,5,0.3)[13]\n",
    "        elif grade == 'A':\n",
    "            return np.arange(1,5,0.3)[12]\n",
    "        elif grade == 'A-':\n",
    "            return np.arange(1,5,0.3)[11]\n",
    "        elif grade == 'B+':\n",
    "            return np.arange(1,5,0.3)[10]\n",
    "        elif grade == 'B':\n",
    "            return np.arange(1,5,0.3)[9]\n",
    "        elif grade == 'B-':\n",
    "            return np.arange(1,5,0.3)[8]\n",
    "        elif grade == 'C+':\n",
    "            return np.arange(1,5,0.3)[7]\n",
    "        elif grade == 'C':\n",
    "            return np.arange(1,5,0.3)[6]\n",
    "        elif grade == 'C-':\n",
    "            return np.arange(1,5,0.3)[5]\n",
    "        elif grade == 'D+':\n",
    "            return np.arange(1,5,0.3)[4]\n",
    "        elif grade == 'D':\n",
    "            return np.arange(1,5,0.3)[3]\n",
    "        elif grade == 'D-':\n",
    "            return np.arange(1,5,0.3)[2]\n",
    "        elif grade == 'F':\n",
    "            return np.arange(1,5,0.3)[1]\n",
    "        else:\n",
    "            return np.arange(1,5,0.3)[0]\n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Valuation Grade'])):\n",
    "        values.append(score(df['Valuation Grade'][i]))\n",
    "    df['Valuation Grade_score'] = values \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Growth Grade'])):\n",
    "        values.append(score(df['Growth Grade'][i]))\n",
    "    df['Growth Grade_score'] = values \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Profitability Grade'])):\n",
    "        values.append(score(df['Profitability Grade'][i]))\n",
    "    df['Profitability Grade_score'] = values    \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Momentum Grade'])):\n",
    "        values.append(score(df['Momentum Grade'][i]))\n",
    "    df['Momentum Grade_score'] = values \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['EPS Revision Grade'])):\n",
    "        values.append(score(df['EPS Revision Grade'][i]))\n",
    "    df['EPS Revision Grade_score'] = values \n",
    "    \n",
    "    snp_main = df\n",
    "    snp_se = pd.read_csv('./snp500/snp500.csv')\n",
    "    snp = pd.merge(snp_main, snp_se, how = 'inner', on = 'Symbol')\n",
    "\n",
    "    # select required columns \n",
    "    data = snp[['Symbol',\n",
    "     'Valuation Grade_score',\n",
    "     'Growth Grade_score',           \n",
    "     'Profitability Grade_score',           \n",
    "     'Momentum Grade_score',           \n",
    "     'EPS Revision Grade_score',\n",
    "     'Volume',\n",
    "     'Avg. Vol',\n",
    "     'Prev Close',\n",
    "     'Open',\n",
    "     'Day Low',\n",
    "     'Day High',\n",
    "     '52W Low_x',\n",
    "     '52W High_x',\n",
    "     'Quant',\n",
    "     'SA Authors',\n",
    "     'Wall St.',\n",
    "     'Shares',\n",
    "     'Cost',\n",
    "     'Today`s Gain',\n",
    "     'Today`s % Gain',\n",
    "     'Total Change',\n",
    "     'Total % Change',\n",
    "     'Value',\n",
    "     'SA Authors Score',\n",
    "     'Wall St. Score',\n",
    "     'Quant Score',\n",
    "     'Valuation Grade',\n",
    "     'Growth Grade',\n",
    "     'Profitability Grade',\n",
    "     'Momentum Grade',\n",
    "     'EPS Revision Grade',\n",
    "     'ETF Momentum',\n",
    "     'ETF Expenses',\n",
    "     'ETF Dividends',\n",
    "     'ETF Risk',\n",
    "     'ETF Flows',\n",
    "     'Upcoming Announce Date',\n",
    "     'Release Time',\n",
    "     'EPS Estimate',\n",
    "     'Revenue Estimate',\n",
    "     'EPS Revisions Grade',\n",
    "     'Last Quarter Announce Date',\n",
    "     'EPS Actual',\n",
    "     'EPS Surprise',\n",
    "     'Revenue Actual',\n",
    "     'Revenue Surprise',\n",
    "     'Safety',\n",
    "     'Growth',\n",
    "     'Yield',\n",
    "     'Consistency',\n",
    "     'Ex-Div Date',\n",
    "     'Payout Date',\n",
    "     'Yield TTM',\n",
    "     'Yield FWD',\n",
    "     '4Y Avg Yield',\n",
    "     'Div Rate TTM',\n",
    "     'Div Rate FWD',\n",
    "     'Payout Ratio',\n",
    "     '4Y Avg Payout',\n",
    "     'Div Growth 3Y',\n",
    "     'Div Growth 5Y',\n",
    "     'Years of Growth',\n",
    "     'Market Cap',\n",
    "     'EV',\n",
    "     'P/E TTM',\n",
    "     'P/E FWD',\n",
    "     'PEG TTM',\n",
    "     'PEG FWD',\n",
    "     'Price / Sales',\n",
    "     'EV / Sales',\n",
    "     'EV / EBITDA',\n",
    "     'Price / Book',\n",
    "     'Price / Cash Flow',\n",
    "     'Revenue YoY',\n",
    "     'Revenue FWD',\n",
    "     'Revenue 3Y',\n",
    "     'Revenue 5Y',\n",
    "     'EBITDA YoY',\n",
    "     'EBITDA FWD',\n",
    "     'EBITDA 3Y',\n",
    "     'Net Income 3Y',\n",
    "     'EPS YoY',\n",
    "     'EPS FWD',\n",
    "     'EPS 3Y',\n",
    "     'Tangible Book 3Y',\n",
    "     'Total Assets 3Y',\n",
    "     'FCF 3Y',\n",
    "     '52W Low_y',\n",
    "     '52W High_y',\n",
    "     '5D Perf',\n",
    "     '1M Perf',\n",
    "     '6M Perf',\n",
    "     'YTD Perf',\n",
    "     '1Y Perf',\n",
    "     '3Y Perf',\n",
    "     '3Y Total Return',\n",
    "     '5Y Perf',\n",
    "     '5Y Total Return',\n",
    "     '10Y Perf',\n",
    "     '10Y Total Return',\n",
    "     '10D SMA',\n",
    "     'Last Price Vs. 10D SMA',\n",
    "     '50D SMA',\n",
    "     'Last Price Vs. 50D SMA',\n",
    "     '100D SMA',\n",
    "     'Last Price Vs. 100D SMA',\n",
    "     '200D SMA',\n",
    "     'Last Price Vs. 200D SMA',\n",
    "     'Week Vol / Shares',\n",
    "     '24M Beta',\n",
    "     '60M Beta',\n",
    "     'Revenue TTM',\n",
    "     'NET Income TTM',\n",
    "     'Cash from Operations',\n",
    "     'Profit Margin',\n",
    "     'EBIT Margin',\n",
    "     'EBITDA Margin',\n",
    "     'Net Income Margin',\n",
    "     'FCF Margin',\n",
    "     'Return on Equity',\n",
    "     'Return on Assets',\n",
    "     'Return on Total Capital',\n",
    "     'Asset Turnover',\n",
    "     'Net Income / Employee',\n",
    "     'Shares Outstanding',\n",
    "     'Float %',\n",
    "     'Insider Shares',\n",
    "     'Insider %',\n",
    "     'Institutional Shares',\n",
    "     'Institutional Percent',\n",
    "     'Total Debt',\n",
    "     'ST Debt',\n",
    "     'LT Debt',\n",
    "     'Total Cash',\n",
    "     'Debt to FCF',\n",
    "     'Current Ratio',\n",
    "     'Quick Ratio',\n",
    "     'Covered Ratio',\n",
    "     'Debt to Equity',\n",
    "     'LT Debt to Total Capital',\n",
    "     'Price',\n",
    "     'Change',\n",
    "     'Change_percent',\n",
    "     'date',\n",
    "     'avg_score',\n",
    "     'GICS Sector',\n",
    "     'GICS Sub-Industry',\n",
    "     'Headquarters Location']]\n",
    "\n",
    "    # select required columns \n",
    "    data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "    # fillna as zero\n",
    "    data = data.fillna(0)\n",
    "\n",
    "    # data type change\n",
    "    array_list = []\n",
    "    for i in data['Payout Date'].values:\n",
    "        if i == '0':\n",
    "            array_list.append('1/01/2000')\n",
    "        else:\n",
    "            array_list.append(i)\n",
    "    data['Payout Date'] = array_list\n",
    "    data['Payout Date'] = data['Payout Date'].astype('datetime64[ns]').dt.date\n",
    "\n",
    "    array_list = []\n",
    "    for i in data['Ex-Div Date'].values:\n",
    "        if i == '0':\n",
    "            array_list.append('1/01/2000')\n",
    "        else:\n",
    "            array_list.append(i)\n",
    "    data['Ex-Div Date'] = array_list\n",
    "    data['Ex-Div Date'] = data['Ex-Div Date'].astype('datetime64[ns]').dt.date\n",
    "\n",
    "    data['Last Quarter Announce Date'] = data['Last Quarter Announce Date'].astype('datetime64[ns]').dt.date\n",
    "\n",
    "    array_list = []\n",
    "    for i in data['Upcoming Announce Date'].values:\n",
    "        if i == '0':\n",
    "            array_list.append('1/01/2000')\n",
    "        else:\n",
    "            array_list.append(i)\n",
    "    data['Upcoming Announce Date'] = array_list\n",
    "    data['Upcoming Announce Date'] = data['Upcoming Announce Date'].astype('datetime64[ns]').dt.date\n",
    "\n",
    "    data['Last Quarter Announce Date'] = data['Last Quarter Announce Date'].astype('datetime64[ns]').dt.date\n",
    "    data['date'] = data['date'].astype('datetime64[ns]').dt.date\n",
    "\n",
    "    # change space as underbar\n",
    "    for i in data.columns:\n",
    "        change = i.replace(' ','_')\n",
    "        data.rename(columns = {i : change}, inplace = True)   \n",
    "\n",
    "    data.rename(columns = \n",
    "    {'GICS_Sub-Industry' : 'GICS_Sub_Industry',\n",
    "    '52W_Low_x' : '_52W_Low_x',  \n",
    "    '52W_High_x' : '_52W_High_x',\n",
    "    '4Y_Avg_Yield' : '_4Y_Avg_Yield',\n",
    "    '4Y_Avg_Payout' : '_4Y_Avg_Payout',\n",
    "    '52W_Low_y' : '_52W_Low_y',\n",
    "    '52W_High_y' : '_52W_High_y',\n",
    "    '5D_Perf' :'_5D_Perf',\n",
    "    '1M_Perf' :'_1M_Perf',\n",
    "    '6M_Perf' : '_6M_Perf',\n",
    "    '1Y_Perf' : '_1Y_Perf',\n",
    "    '3Y_Perf' : '_3Y_Perf',\n",
    "    '3Y_Total_Return' : '_3Y_Total_Return',\n",
    "    '5Y_Perf' : '_5Y_Perf',\n",
    "    '5Y_Total_Return' : '_5Y_Total_Return',\n",
    "    '10Y_Perf' : '_10Y_Perf',\n",
    "    '10Y_Total_Return' : '_10Y_Total_Return',\n",
    "    '10D_SMA' : '_10D_SMA',\n",
    "    '50D_SMA' : '_50D_SMA',\n",
    "    '100D_SMA' : '_100D_SMA',\n",
    "    '200D_SMA' : '_200D_SMA',\n",
    "    '24M_Beta' : '_24M_Beta',\n",
    "    '60M_Beta' : '_60M_Beta', \n",
    "    'Avg._Vol' : 'Avg_Vol', \n",
    "    'Wall_St.' : 'Wall_St',\n",
    "    'Wall_St._Score' : 'Wall_St_Score', \n",
    "    'Last_Price_Vs._10D_SMA' : 'Last_Price_Vs_10D_SMA',        \n",
    "    'Last_Price_Vs._50D_SMA' : 'Last_Price_Vs_50D_SMA',   \n",
    "    'Last_Price_Vs._100D_SMA' : 'Last_Price_Vs_100D_SMA',   \n",
    "    'Last_Price_Vs._200D_SMA' : 'Last_Price_Vs_200D_SMA', \n",
    "    'Today`s_Gain' : 'Today_Gain', \n",
    "    'Today`s_%_Gain' : 'Todays_Percent_Gain',\n",
    "    'Total_%_Change' : 'Total_Percent_Change', \n",
    "    'Ex-Div_Date' : 'Ex_Div_Date', \n",
    "    'P/E_TTM' : 'PE_TTM',                   \n",
    "    'P/E_FWD' : 'PE_FWD',                      \n",
    "    'PEG_TTM' : 'PEG_TTM',                    \n",
    "    'PEG_FWD' : 'PEG_FWD',                      \n",
    "    'Price_/_Sales' : 'Price_per_Sales',             \n",
    "    'EV_/_Sales' : 'EV_per_Sales',                  \n",
    "    'EV_/_EBITDA' : 'EV_per_EBITDA',                \n",
    "    'Price_/_Book' : 'Price_per_Book',                 \n",
    "    'Price_/_Cash_Flow' : 'Price_per_Cash_Flow', \n",
    "    'Week_Vol_/_Shares' : 'Week_Vol_per_Shares',\n",
    "    'Net_Income_/_Employee' : 'Net_Income_per_Employee',        \n",
    "    'Float_%' : 'Float_Percent',                      \n",
    "    'Insider_%' : 'Insider_Percent'\n",
    "    }, inplace = True)\n",
    "\n",
    "    # Create Dimensions\n",
    "    company_dim = data.copy()\n",
    "    company_dim = company_dim[['GICS_Sector', 'GICS_Sub_Industry']]\n",
    "    company_dim = company_dim.drop_duplicates(subset = 'GICS_Sector', keep = \"first\")\n",
    "    company_dim = company_dim.reset_index(drop = True)\n",
    "    company_dim.insert(0, \"company_id\", range(10, 10+len(company_dim)))\n",
    "    data = data.merge(company_dim[[\"GICS_Sector\", \"company_id\"]],\n",
    "                      left_on = \"GICS_Sector\",\n",
    "                      right_on = \"GICS_Sector\",\n",
    "                      how = \"left\")\n",
    "\n",
    "    location_dim = data.copy()\n",
    "    location_dim = location_dim[['Headquarters_Location']]\n",
    "    location_dim = location_dim.drop_duplicates(subset = 'Headquarters_Location', keep = \"first\")\n",
    "    location_dim = location_dim.reset_index(drop = True)\n",
    "    location_dim.insert(0, \"location_id\", range(100, 100+len(location_dim)))\n",
    "    data = data.merge(location_dim[['Headquarters_Location', 'location_id']],\n",
    "                      left_on = 'Headquarters_Location',\n",
    "                      right_on = 'Headquarters_Location',\n",
    "                      how = \"left\")\n",
    "\n",
    "    date_dim = data.copy()\n",
    "    date_dim = date_dim[['date']]\n",
    "    date_dim = date_dim.drop_duplicates(subset = 'date', keep = \"first\")\n",
    "    date_dim = date_dim.reset_index(drop = True)\n",
    "    date_dim.insert(0, \"date_id\", range(10000, 10000+len(date_dim)))\n",
    "    data = data.merge(date_dim[['date', 'date_id']],\n",
    "                      left_on = 'date',\n",
    "                      right_on = 'date',\n",
    "                      how = \"left\")\n",
    "\n",
    "    # Create Fact table\n",
    "    fact_columns = []\n",
    "    for i in data.columns:\n",
    "        fact_columns.append(i)\n",
    "\n",
    "    fact_columns_filtered = []\n",
    "    for i in fact_columns:\n",
    "        if i not in location_dim.columns.tolist() and i not in company_dim.columns.tolist() and i not in date_dim.columns.tolist():\n",
    "            fact_columns_filtered.append(i)\n",
    "\n",
    "    fact_columns_filtered.append('date_id')\n",
    "    fact_columns_filtered.append('company_id')\n",
    "    fact_columns_filtered.append('location_id')\n",
    "\n",
    "    fact_snp = data[fact_columns_filtered]\n",
    "\n",
    "    # send to BigQuery\n",
    "    # first, create a BigQuery client to connect to BigQuery\n",
    "    from google.cloud import bigquery\n",
    "    from google.oauth2 import service_account\n",
    "\n",
    "    key_path = r'C:\\Users\\aicpa\\My Drive\\__QED\\cpasean-4d80408d4add.json' # must edit to your credentials json file location\n",
    "    credentials = service_account.Credentials.from_service_account_file(key_path,\n",
    "                                                                        scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],)\n",
    "    client = bigquery.Client(credentials = credentials,\n",
    "                             project = credentials.project_id)\n",
    "\n",
    "    dataset_id = 'cpasean.qed_snp500'   # PASTE THIS DATASET ID FROM ABOVE STEPS\n",
    "    dataset_id = dataset_id.replace(':', '.')\n",
    "\n",
    "    # build a function to load tables to BigQuery\n",
    "\n",
    "    def load_table_to_bigquery(df, table_name):\n",
    "\n",
    "        dataset_id = 'cpasean.qed_snp500'\n",
    "\n",
    "        dataset_ref = client.dataset(dataset_id)\n",
    "        job_config = bigquery.LoadJobConfig()\n",
    "        job_config.autodetect = True\n",
    "        job_config.write_disposition = \"WRITE_TRUNCATE\"\n",
    "\n",
    "        upload_table_name = f\"cpasean.qed_snp500.{table_name}\"\n",
    "\n",
    "        load_job = client.load_table_from_dataframe(df,\n",
    "                                                   upload_table_name,\n",
    "                                                   job_config = job_config)\n",
    "\n",
    "        print(f\"starting job {load_job}\")\n",
    "\n",
    "    load_table_to_bigquery(df = location_dim, table_name = 'location_dim')\n",
    "    load_table_to_bigquery(df = date_dim, table_name = \"date_dim\")\n",
    "    load_table_to_bigquery(df = company_dim, table_name = \"company_dim\")\n",
    "    load_table_to_bigquery(df = fact_snp, table_name = \"fact_snp\")\n",
    "\n",
    "    # send to *.csv\n",
    "    location_dim.to_csv('location_dim.csv', index = False)\n",
    "    date_dim.to_csv('date_dim.csv', index = False)\n",
    "    company_dim.to_csv('company_dim.csv', index = False)\n",
    "    fact_snp.to_csv('fact_snp.csv', index = False)\n",
    "\n",
    "    print('\\n\\nData Warehousing has been done.\\n\\n')\n",
    "    \n",
    "    voice_message(\"\"\"\\\n",
    "        Data Warehousing has been done\"\"\")\n",
    "    \n",
    "    \n",
    "# extract pages from each spread sheet\n",
    "def frame(sheet):\n",
    "    \n",
    "    page = {}\n",
    "\n",
    "    for n in range(sheet.ncols):\n",
    "        page[sheet.cell_value(0,n)] = {}\n",
    "        for r in range(1, sheet.nrows):\n",
    "            page[sheet.cell_value(0,n)][r] = sheet.cell_value(r,n)     \n",
    "    df1 = pd.DataFrame(page)\n",
    "    df1.replace('-', 0, inplace = True)\n",
    "    df1.replace('NM', 0, inplace = True)\n",
    "    df1.replace(np.NaN, 0, inplace = True)\n",
    "    pages.append(df1)\n",
    "        \n",
    "def nxt():        \n",
    "    # combine each sheet in the one DataFrame    \n",
    "    df = pages[0].merge(pages[1], how = 'inner', left_on = 'Symbol', right_on = 'Symbol')\n",
    "    for i in range(2, 12):\n",
    "        df = df.merge(pages[i], how = 'inner', left_on = 'Symbol', right_on = 'Symbol')\n",
    "\n",
    "    # clean duplicated columns    \n",
    "    df['Price'] = pd.DataFrame(df['Price_x'].values)[0]\n",
    "    df = df.drop(columns = ['Price_x'], axis = 1)\n",
    "    df = df.drop(columns = ['Price_y'], axis = 1)\n",
    "\n",
    "    df['Change'] = pd.DataFrame(df['Change_x'].values)[0]\n",
    "    df = df.drop(columns = ['Change_x'], axis = 1)\n",
    "    df = df.drop(columns = ['Change_y'], axis = 1)\n",
    "\n",
    "    df['Change_percent'] = df['Change %_x']*100\n",
    "    df = df.drop(columns = ['Change %_x'], axis = 1)\n",
    "    df = df.drop(columns = ['Change %_y'], axis = 1)\n",
    "\n",
    "    # add date, avg score\n",
    "    df['date'] = currentdate\n",
    "    try:\n",
    "        df['avg_score'] = (df['SA Author Ratings'] + df['Wall Street Ratings'] + df['Quant Ratings'])/3\n",
    "    except:\n",
    "        df['avg_score'] = (df['SA Analysts Score'] + df['Wall Street Ratings'] + df['Quant Ratings'])/3\n",
    "    df = df.sort_values('avg_score', ascending = False).reset_index(drop=True)\n",
    "    df.drop(index = df.shape[0]-1, inplace = True)\n",
    "\n",
    "    # edit the new version of Seeking-Alpha download data into the numeric format\n",
    "    df = df.fillna(0)\n",
    "## enable below when data contains $ sign in object type. \n",
    "#     df['Revenue Estimate'] = df['Revenue Estimate'].str.replace('$', '')\n",
    "#     df['Revenue Actual'] = df['Revenue Actual'].str.replace('$', '')\n",
    "#     df['Revenue Surprise'] = df['Revenue Surprise'].str.replace('$', '')\n",
    "\n",
    "    def value_to_float(x):\n",
    "        if type(x) == float or type(x) == int:\n",
    "            return x\n",
    "        if 'Years' in x:\n",
    "            return float(x.strip(' Years'))\n",
    "        if 'Year' in x:\n",
    "            return float(x.strip(' Year'))\n",
    "        if 'A+' in x:\n",
    "            return np.arange(1,5,0.3)[13]\n",
    "        if 'A' in x:\n",
    "            return np.arange(1,5,0.3)[12]\n",
    "        if 'A-' in x:\n",
    "            return np.arange(1,5,0.3)[11]\n",
    "        if 'B+' in x:\n",
    "            return np.arange(1,5,0.3)[10]\n",
    "        if 'B' in x:\n",
    "            return np.arange(1,5,0.3)[9]\n",
    "        if 'B-' in x:\n",
    "            return np.arange(1,5,0.3)[8]\n",
    "        if 'C+' in x:\n",
    "            return np.arange(1,5,0.3)[7]\n",
    "        if 'C' in x:\n",
    "            return np.arange(1,5,0.3)[6]\n",
    "        if 'C-' in x:\n",
    "            return np.arange(1,5,0.3)[5]\n",
    "        if 'D+' in x:\n",
    "            return np.arange(1,5,0.3)[4]\n",
    "        if 'D' in x:\n",
    "            return np.arange(1,5,0.3)[3]\n",
    "        if 'D-' in x:\n",
    "            return np.arange(1,5,0.3)[2]\n",
    "        if 'F' in x:\n",
    "            return np.arange(1,5,0.3)[1]\n",
    "        if ',' in x:\n",
    "            return float(x.replace(',', ''))\n",
    "        if '$' in x:\n",
    "            return float(x.replace('$', ''))\n",
    "        if 'K' in x:\n",
    "            if len(x) > 1:\n",
    "                return float(x.replace('K', '')) * 1000\n",
    "            return 1000.0 # else\n",
    "        if 'M' in x:\n",
    "            if len(x) > 1:\n",
    "                return float(x.replace('M', '')) * 1000000\n",
    "            return 1000000.0\n",
    "        if 'B' in x:\n",
    "            if len(x) > 1:\n",
    "                return float(x.replace('B', '')) * 1000000000\n",
    "            return 1000000000.0\n",
    "        if 'T' in x:\n",
    "            if len(x) > 1:\n",
    "                return float(x.replace('T', '')) * 1000000000000\n",
    "            return 1000000000000.0\n",
    "        return x\n",
    "\n",
    "    for column in df.columns[1:22]:\n",
    "        df[column] = df[column].apply(value_to_float).astype(float)\n",
    "\n",
    "    for column in df.columns[27:32]:\n",
    "        df[column] = df[column].apply(value_to_float).astype(float)\n",
    "\n",
    "    for column in df.columns[34:36]:\n",
    "        df[column] = df[column].apply(value_to_float).astype(float)\n",
    "\n",
    "    for column in df.columns[37:42]:\n",
    "        df[column] = df[column].apply(value_to_float).astype(float)\n",
    "\n",
    "    for column in df.columns[48:57]:\n",
    "        df[column] = df[column].apply(value_to_float).astype(float)\n",
    "\n",
    "    for column in df.columns[58:139]:\n",
    "        df[column] = df[column].apply(value_to_float).astype(float)\n",
    "\n",
    "    df = df.rename(columns = {'EPS Revision Grade_x' : 'EPS Revision Grade', 'EPS Revision Grade_y' : 'EPS Revisions Grade'})\n",
    "    df = df.fillna(0)\n",
    "    df.to_csv('snp500 %s.csv' %currentdate, index = False)\n",
    "    \n",
    "    # combine previous snp500 with current snp500\n",
    "    df = pd.concat(map(pd.read_csv, ['snp500 %s.csv' %mostrecentdate, 'snp500 %s.csv' %currentdate]), ignore_index=True) #optional\n",
    "    df = df.fillna(0)\n",
    "    df = df.drop_duplicates().reset_index(drop = True)\n",
    "    df.to_csv('snp500 %s.csv' %currentdate, index = False) \n",
    "    df.to_csv('./Backup_snp500/snp500 %s.csv' %currentdate, index = False)\n",
    "    \n",
    "    # remove previous snp500\n",
    "    import os\n",
    "    os.remove('snp500 %s.csv' %mostrecentdate)  \n",
    "    os.remove('snp500 %s.xls' %mostrecentdate)  \n",
    "    print(f'snp500 {currentdate}.csv | srank.csv have been created.')\n",
    "    \n",
    "#     tableau_snp500()\n",
    "    \n",
    "def secondpick(bluechips):\n",
    "    # read accmulated snp500 data *Remove the historical data if not quite practical!!!\n",
    "    df = pd.read_csv('snp500 %s.csv' %currentdate)\n",
    "\n",
    "    # filter out date based on 'min_holding_period'\n",
    "    df = df[df['date'] >= str(datetime.today() - timedelta(days = min_holding_period))[:10]]\n",
    "    corr_analysis = df.corr().reset_index()[['index','Change_percent']]\n",
    "    corr_analysis = corr_analysis.sort_values('Change_percent', ascending = False).reset_index(drop = True)\n",
    "    corr_analysis = corr_analysis.loc[corr_analysis['index'].isin([\n",
    "        '1Y Perf', \n",
    "        'Price / Cash Flow',\n",
    "        '10Y Total Return', \n",
    "        '10Y Perf', \n",
    "        'P/E TTM', \n",
    "        '5Y Perf',\n",
    "        '5Y Total Return',\n",
    "        'EV / EBITDA', \n",
    "        'Price / Sales',\n",
    "        'Last Price Vs. 10D SMA', \n",
    "        'Quick Ratio', \n",
    "        '3Y Perf',\n",
    "        '3Y Total Return', \n",
    "        'Week Vol / Shares', \n",
    "        'Current Ratio',\n",
    "        'EV / Sales', \n",
    "        'P/E FWD',\n",
    "        'Revenue 3Y', \n",
    "        'Revenue FWD', \n",
    "        'EBITDA FWD', \n",
    "        'Yield FWD', \n",
    "        'Yield TTM', \n",
    "        'Revenue YoY', \n",
    "        'FCF 3Y', \n",
    "        '4Y Avg Yield',\n",
    "        'EBITDA 3Y', \n",
    "        '24M Beta', \n",
    "        'Covered Ratio', \n",
    "        'Profitability Grade_score', \n",
    "        'Return on Assets', \n",
    "        'Asset Turnover',\n",
    "        'Profit Margin',\n",
    "        'Payout Ratio', \n",
    "        'FCF Margin',\n",
    "        'Div Rate FWD', \n",
    "        'EPS 3Y', \n",
    "        '60M Beta', \n",
    "        'Div Rate TTM', \n",
    "        'EPS Revision Grade_score',\n",
    "        'Valuation Grade_score', \n",
    "        'Float %', \n",
    "        'Net Income Margin',\n",
    "        'EBIT Margin', \n",
    "        '1M Perf', \n",
    "        'Price / Book', \n",
    "        'Net Income / Employee',\n",
    "        'YTD Perf', \n",
    "        'EBITDA Margin', \n",
    "        'Momentum Grade_score', \n",
    "        'Institutional Percent', \n",
    "        'EPS FWD', \n",
    "        'Wall St. Score', \n",
    "        'PEG FWD', \n",
    "        '6M Perf', \n",
    "        'EPS Estimate',\n",
    "        'EPS Actual',\n",
    "        'Debt to FCF', \n",
    "        'Div Growth 3Y',\n",
    "        'LT Debt to Total Capital', \n",
    "        'Return on Total Capital',\n",
    "        'Div Growth 5Y', \n",
    "        'PEG TTM',\n",
    "        'Last Price Vs. 100D SMA', \n",
    "        'Last Price Vs. 200D SMA', \n",
    "        'Last Price Vs. 50D SMA', \n",
    "        'avg_score',\n",
    "        'Growth Grade_score', \n",
    "        'SA Authors Score', \n",
    "        'Debt to Equity',\n",
    "        'Return on Equity', \n",
    "        'EPS YoY', \n",
    "        'Quant Score', \n",
    "        'EBITDA YoY'])].reset_index(drop = True)\n",
    "\n",
    "    neg_vars = ['Price / Cash Flow', \n",
    "                'P/E TTM', \n",
    "                'EV / EBITDA', \n",
    "                'Price / Sales',\n",
    "                'EV / Sales', \n",
    "                'P/E FWD',\n",
    "                '24M Beta', \n",
    "                '60M Beta', \n",
    "                'Price / Book', \n",
    "                'PEG FWD', \n",
    "                'Debt to FCF', \n",
    "                'LT Debt to Total Capital', \n",
    "                'PEG TTM',\n",
    "                'Debt to Equity']\n",
    "\n",
    "    # corr: the higest corr.\n",
    "    corr_analysis_topindex = corr_analysis['index'].values.tolist()[0]\n",
    "\n",
    "    today = df[df['date'] == currentdate]\n",
    "    df = today.copy() # grading is NOT based on the accmulated file ####################\n",
    "    df = df.reset_index(drop = True)  \n",
    "    \n",
    "    # Sean score preparation\n",
    "    def score(grade):\n",
    "        if grade == 'A+':\n",
    "            return np.arange(1,5,0.3)[13]\n",
    "        elif grade == 'A':\n",
    "            return np.arange(1,5,0.3)[12]\n",
    "        elif grade == 'A-':\n",
    "            return np.arange(1,5,0.3)[11]\n",
    "        elif grade == 'B+':\n",
    "            return np.arange(1,5,0.3)[10]\n",
    "        elif grade == 'B':\n",
    "            return np.arange(1,5,0.3)[9]\n",
    "        elif grade == 'B-':\n",
    "            return np.arange(1,5,0.3)[8]\n",
    "        elif grade == 'C+':\n",
    "            return np.arange(1,5,0.3)[7]\n",
    "        elif grade == 'C':\n",
    "            return np.arange(1,5,0.3)[6]\n",
    "        elif grade == 'C-':\n",
    "            return np.arange(1,5,0.3)[5]\n",
    "        elif grade == 'D+':\n",
    "            return np.arange(1,5,0.3)[4]\n",
    "        elif grade == 'D':\n",
    "            return np.arange(1,5,0.3)[3]\n",
    "        elif grade == 'D-':\n",
    "            return np.arange(1,5,0.3)[2]\n",
    "        elif grade == 'F':\n",
    "            return np.arange(1,5,0.3)[1]\n",
    "        else:\n",
    "            return np.arange(1,5,0.3)[0]\n",
    "\n",
    "    # 03042022: grading is NOT based on the accmulated file, 'snp500 2xxx-xx-xx'\n",
    "    values = []\n",
    "    for i in range(len(df['Valuation Grade'])):\n",
    "        values.append(score(df['Valuation Grade'][i]))\n",
    "    df['Valuation Grade_score'] = values \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Growth Grade'])):\n",
    "        values.append(score(df['Growth Grade'][i]))\n",
    "    df['Growth Grade_score'] = values \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Profitability Grade'])):\n",
    "        values.append(score(df['Profitability Grade'][i]))\n",
    "    df['Profitability Grade_score'] = values    \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['Momentum Grade'])):\n",
    "        values.append(score(df['Momentum Grade'][i]))\n",
    "    df['Momentum Grade_score'] = values \n",
    "\n",
    "    values = []\n",
    "    for i in range(len(df['EPS Revision Grade'])):\n",
    "        values.append(score(df['EPS Revision Grade'][i]))\n",
    "    df['EPS Revision Grade_score'] = values \n",
    "    \n",
    "    # y-axis is set as 'srank_corr_var'\n",
    "    import statsmodels.api as sm\n",
    "    df['intercept'] = 1\n",
    "    lm = sm.OLS(df[corr_analysis_topindex], df[['intercept', 'Valuation Grade_score', 'Growth Grade_score', 'Profitability Grade_score', 'Momentum Grade_score', 'EPS Revision Grade_score', 'Quant Score', 'SA Authors Score', 'Wall St. Score']])\n",
    "\n",
    "    results = lm.fit()\n",
    "#     print(results.summary())\n",
    "    print('\\n')\n",
    "\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    ###########################################################################################################################################################################################\n",
    "    #Create Sean Score with 6 grades\n",
    "    if corr_analysis_topindex in neg_vars:\n",
    "        df['sean_score'] = -(df['Valuation Grade_score']*results.params[1] + df['Growth Grade_score']*results.params[2] + df['Profitability Grade_score']*results.params[3] + df['Momentum Grade_score']*results.params[4] + df['EPS Revision Grade_score']*results.params[5] + df['Quant Score']*results.params[6] + df['SA Authors Score']*results.params[7] + df['Wall St. Score']*results.params[8])/8 \n",
    "    else:\n",
    "        df['sean_score'] = (df['Valuation Grade_score']*results.params[1] + df['Growth Grade_score']*results.params[2] + df['Profitability Grade_score']*results.params[3] + df['Momentum Grade_score']*results.params[4] + df['EPS Revision Grade_score']*results.params[5] + df['Quant Score']*results.params[6] + df['SA Authors Score']*results.params[7] + df['Wall St. Score']*results.params[8])/8 \n",
    "#     today = df[df['date'] == currentdate]\n",
    "#     today['sean_score'] = (today['Valuation Grade_score']*results.params[1] + today['Growth Grade_score']*results.params[2] + today['Profitability Grade_score']*results.params[3] + today['Momentum Grade_score']*results.params[4] + today['EPS Revision Grade_score']*results.params[5] + today['Quant Score']*results.params[6] + today['SA Authors Score']*results.params[7] + today['Wall St. Score']*results.params[8])/8\n",
    "    ###########################################################################################################################################################################################\n",
    "    \n",
    "    dfsort = pd.DataFrame(df.groupby('Symbol').mean()).reset_index()\n",
    "    \n",
    "    # get earning date\n",
    "    earningdate = df[['Symbol', 'Upcoming Announce Date']]\n",
    "    earningdate = earningdate.groupby('Symbol')['Symbol', 'Upcoming Announce Date'].tail(1)\n",
    "\n",
    "    # get ex-dividend date\n",
    "    divdate = df[['Symbol', 'Ex-Div Date']]\n",
    "    divdate = divdate.groupby('Symbol')['Symbol', 'Ex-Div Date'].tail(1)\n",
    "\n",
    "    dfsort = dfsort.merge(divdate, how = 'outer', left_on = 'Symbol', right_on = 'Symbol')\n",
    "    dfsort = dfsort.merge(earningdate, how = 'outer', left_on = 'Symbol', right_on = 'Symbol')\n",
    "\n",
    "    sean_rank = dfsort.copy()\n",
    "    sean_rank = sean_rank.sort_values('sean_score', ascending = False)\n",
    "    sean_rank.reset_index(inplace = True)\n",
    "    \n",
    "    # set number of tickers \n",
    "    # delete 'snp500 ----, --, --' if applying the up-to-dated rating\n",
    "    srank = sean_rank.merge(snp, how = 'inner', left_on = 'Symbol', right_on = 'Symbol')\n",
    "    srank = srank.drop(columns = 'index')\n",
    "    srank.to_csv('srank.csv', index = False)\n",
    "    srank.to_csv('./Backup_srank/srank %s.csv' %currentdate, index = False)\n",
    "    slist = srank.head(bluechips).Symbol.values   # setting the number of bluechips\n",
    "    \n",
    "    with open('slist.txt', 'w') as f:\n",
    "        for s in slist:\n",
    "            f.write(str(s) + '\\n')\n",
    "            \n",
    "#     print(slist)\n",
    "\n",
    "\n",
    "def main_design():\n",
    "    # excute only if the file exists    \n",
    "    from os.path import exists\n",
    "\n",
    "    # check if the previous date file exists. If yes, excel spreadsheet work starts\n",
    "    snp500_exists = exists('snp500 %s.csv' %mostrecentdate)\n",
    "    if snp500_exists == False:\n",
    "        print('S&P 500 has already been updated.')\n",
    "        pass\n",
    "    else:\n",
    "        for i in range(12):\n",
    "            sheet = xlrd.open_workbook('snp500 %s.xls' %currentdate).sheet_by_index(i)\n",
    "            frame(sheet)    \n",
    "        nxt() # Excel sheets combining done up to here.\n",
    "    secondpick(bluechips)    \n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and timechecknow() < 390:\n",
    "        # the program starts off from the main function\n",
    "        pages = []\n",
    "        main_design()\n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[0m', '\\n')\n",
    "        \n",
    "# import FinanceDataReader as fdr\n",
    "# fdr.DataReader('GOOGL').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b227352f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## Portfolio - Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a8fb3d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dir_cleaner(mostrecentdate):\n",
    "    '''\n",
    "    Delete old file to avoid loading delay\n",
    "    \n",
    "    '''\n",
    "    with open('etf_portfolio.txt', 'r') as f:\n",
    "        etf_portfolio = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    portfolio_to_remove = pd.read_csv('snp500\\snp_sector_index.csv').sort_values('Symbol')['Symbol'].to_list()\n",
    "    portfolio_to_remove = portfolio_to_remove + etf_portfolio\n",
    "    \n",
    "    from os.path import exists\n",
    "    print('• Delete Previous Files: candle_analysis')\n",
    "    for ticker in portfolio_to_remove:\n",
    "        candle_analysis = exists(f'candle_analysis_{ticker}_{mostrecentdate}.csv')\n",
    "        if candle_analysis == True:\n",
    "            import os\n",
    "            os.remove(f'candle_analysis_{ticker}_{mostrecentdate}.csv')\n",
    "            print(f'candle_analysis_{ticker}_{mostrecentdate}.csv ==> Deleted')\n",
    "        else:\n",
    "            pass  \n",
    "    print('\\n==> Completed\\n\\n')\n",
    "\n",
    "    print('• Delete Previous Files: predict_aim_sourcedata_monitoring')\n",
    "    for ticker in portfolio_to_remove:\n",
    "        predict_aim_sourcedata_monitoring = exists(f'predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv')\n",
    "        if predict_aim_sourcedata_monitoring == True:\n",
    "            import os\n",
    "            os.remove(f'predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv')\n",
    "            print(f'predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv ==> Deleted')\n",
    "        else:\n",
    "            pass  \n",
    "    print('\\n==> Completed\\n\\n')\n",
    "    \n",
    "    print('• Delete Previous Files: candle_token')\n",
    "    for ticker in portfolio_to_remove:\n",
    "        candle_token = exists(f'candle_token_{ticker}.txt')\n",
    "        if candle_token == True:\n",
    "            import os\n",
    "            os.remove(f'candle_token_{ticker}.txt')\n",
    "            print(f'candle_token_{ticker}.txt ==> Deleted')\n",
    "        else:\n",
    "            pass  \n",
    "    print('\\n==> Completed\\n\\n')\n",
    "\n",
    "    print('• Delete Previous Files: strategy_anaysis')\n",
    "    for ticker in portfolio_to_remove:\n",
    "        strategy_anaysis = exists(f'strategy_anaysis_{ticker}_{mostrecentdate}.csv')\n",
    "        if strategy_anaysis == True:\n",
    "            import os\n",
    "            os.remove(f'strategy_anaysis_{ticker}_{mostrecentdate}.csv')\n",
    "            print(f'strategy_anaysis_{ticker}_{mostrecentdate}.csv ==> Deleted')\n",
    "        else:\n",
    "            pass  \n",
    "    print('\\n==> Completed\\n\\n')    \n",
    "    \n",
    "    print('• Delete Previous Files: stg_para_search')\n",
    "    for ticker in portfolio_to_remove:\n",
    "        stg_para_search = exists(f'stg_para_search_{ticker}_{mostrecentdate}.csv')\n",
    "        if stg_para_search == True:\n",
    "            import os\n",
    "            os.remove(f'stg_para_search_{ticker}_{mostrecentdate}.csv')\n",
    "            print(f'stg_para_search_{ticker}_{mostrecentdate}.csv ==> Deleted')\n",
    "        else:\n",
    "            pass  \n",
    "    print('\\n==> Completed\\n\\n')    \n",
    "    \n",
    "    \n",
    "def ticker_eventday_check():    \n",
    "    srank = pd.read_csv('srank.csv')\n",
    "\n",
    "    # remove ex-dividend tickers\n",
    "    srank['Ex-Div Date'] = srank['Ex-Div Date'].replace('0', '1/1/1')\n",
    "    srank['Ex-Div Date'] = srank['Ex-Div Date'].astype('datetime64[ns]')\n",
    "    exdivticker = srank.sort_values('Ex-Div Date', ascending = False)\n",
    "\n",
    "    nextdate_div = str(datetime.today() + timedelta(days = 1))[:10]   \n",
    "    exdivticker = exdivticker[exdivticker['Ex-Div Date'] == nextdate_div].reset_index(drop = True)\n",
    "    exdivticker = list(exdivticker['Symbol'].values)\n",
    "    #     print('\\n\\n\\n'+ 'Ex-div. ticker: \\n\\n', exdivticker)\n",
    "\n",
    "    # remove earning-day tickers\n",
    "    today_earning = str(datetime.today())[:10]   \n",
    "    today_earning = today_earning[5:7].lstrip('0')+'/'+today_earning[8:10].lstrip('0')+'/'+today_earning[:4]\n",
    "    plusday_earning = str(datetime.today() + timedelta(days = 1))[:10]   \n",
    "    plusday_earning = plusday_earning[5:7].lstrip('0')+'/'+plusday_earning[8:10].lstrip('0')+'/'+plusday_earning[:4]\n",
    "\n",
    "    earningticker_1 = srank[srank['Upcoming Announce Date'] == today_earning].Symbol.to_list()\n",
    "    earningticker_2 = srank[srank['Upcoming Announce Date'] == plusday_earning].Symbol.to_list()\n",
    "\n",
    "    event_tickers = exdivticker + earningticker_1 + earningticker_2\n",
    "    event_tickers = sorted([i for n, i in enumerate(event_tickers) if i not in event_tickers[:n]]) # duplicated\n",
    "    \n",
    "    return event_tickers\n",
    "\n",
    "\n",
    "def super_pass_filtering(lookbackdays = 3):\n",
    "    # super pass - buy sign within the short term period\n",
    "    \n",
    "    snp500 = pd.read_csv('./snp500/snp_sector_index.csv').Symbol.to_list()\n",
    "    with open('remove_tickers_superpass.txt', 'r') as f:\n",
    "        remove_tickers_superpass = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    tickers = snp500\n",
    "    tickers = [i for i in tickers if i not in remove_tickers_superpass]\n",
    "    tickers = [i for n, i in enumerate(tickers) if i not in tickers[:n]] #duplicated \n",
    "\n",
    "    # get sentiment tickers by inputting filters tickers\n",
    "    sentiment_tickers = sentiment_portfolio(tickers, 9)\n",
    "    \n",
    "    # get top volume tickers by inputting filters tickers\n",
    "    volume_tickers = top_volume_portfolio(tickers)\n",
    "\n",
    "    superpass_buysign = []\n",
    "    print(f\"\\n\\n\\nSuper Pass [{lookbackdays} Days Buy Sign Check]:\\n\\n {tickers}\\n\")\n",
    "    c = 0 \n",
    "    for stock in tickers:\n",
    "        c+=1\n",
    "        print(f\"> Ticker ({c}/{len(tickers)}): {stock}\", end = '                                                                 \\r')\n",
    "        try:\n",
    "            if buy_sign_day(stock, lookbackdays) == True:\n",
    "                superpass_buysign.append(stock)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('\\nException >>> Buy Sign Check: ', stock)\n",
    "            pass    \n",
    "        \n",
    "    superpass_buysign.extend(sentiment_tickers)\n",
    "    superpass_buysign.extend(volume_tickers)\n",
    "    portfolio_superpass = superpass_buysign\n",
    "    portfolio_superpass = sorted([i for n, i in enumerate(portfolio_superpass) if i not in portfolio_superpass[:n]]) \n",
    "\n",
    "    ticker_final_list = []\n",
    "    for ticker in portfolio_superpass:\n",
    "        print(f'Crossover Check - Ticker: {ticker}', end = '                            \\r')\n",
    "        analysis = analysis_price(ticker)\n",
    "        if analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False:\n",
    "            ticker_final_list.append(ticker)\n",
    "    portfolio_superpass = ticker_final_list    \n",
    "    \n",
    "    print(f'\\n\\n\\n\\n\\nCaution >>> Super Pass Tickers(saved):\\n{portfolio_superpass}')\n",
    "    with open(\"portfolio_superpass.txt\", 'w') as f:\n",
    "        for s in portfolio_superpass:\n",
    "            f.write(str(s) + '\\n')\n",
    "    \n",
    "    return portfolio_superpass\n",
    "\n",
    "\n",
    "def portfolio_strategy_test(portfolio_after_sector_comparison):\n",
    "    global portfolio\n",
    "    \n",
    "    portfolio_after_strategy_test = []\n",
    "    strategy_analysis_failed = []\n",
    "    print(f\"\\n\\n\\n\\n\\nSector Comparison Test Candidates ===> Strategy Test Candidates (>{min_SuccessRate}):\\n\\n {portfolio_after_sector_comparison}\\n\")\n",
    "    c = 0 \n",
    "    for stock in portfolio_after_sector_comparison:\n",
    "        c+=1\n",
    "        print(f\"> Ticker ({c}/{len(portfolio_after_sector_comparison)}): {stock}\", end = '                                                                 \\r')\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            strategy_analysis = strategy_analysis_main(stock, 'off', 'off')[0]\n",
    "            if strategy_analysis == 1:\n",
    "                portfolio_after_strategy_test.append(stock)\n",
    "            elif strategy_analysis == 0 or strategy_analysis == -10:\n",
    "                strategy_analysis = strategy_analysis_main(stock, 'off', 'off')[0]\n",
    "                if strategy_analysis >= 0:\n",
    "                    portfolio_after_strategy_test.append(stock)\n",
    "                elif strategy_analysis == -10:\n",
    "                    strategy_analysis_failed.append(stock)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print(f'\\nException >>> Strategy Test Candidates: {stock}\\n\\n')\n",
    "            pass\n",
    "    portfolio = portfolio_after_strategy_test\n",
    "    print(f\"\\n\\n\\n\\n\\nStrategy Test ===> Final Portfolio\\n\\n {portfolio}\\n\")\n",
    "    \n",
    "    # check success rate of strategy analysis\n",
    "    analysis_success_rate = (len(portfolio_after_sector_comparison)-len(strategy_analysis_failed)) / len(portfolio_after_sector_comparison)\n",
    "    print('\\033[1m' + f'\\n\\n\\nStrategy Analysis Success Rate: {np.round(analysis_success_rate*100, 2)} %\\n\\n\\n' + '\\033[0m')\n",
    "  \n",
    "    return portfolio, analysis_success_rate\n",
    "\n",
    "\n",
    "def pfo_creation(bluechips):\n",
    "    global yf_counter\n",
    "    global min_SuccessRate\n",
    "    \n",
    "    # reset the counter checking the number of request to yfinance\n",
    "    yf_counter = 0\n",
    "    \n",
    "    # less than a flexible percentile by snp trend\n",
    "    snp_yfinance = yfinance_df_setting('^GSPC')    \n",
    "    snp_long = snp_yfinance[-20:]['Adj Close'].mean()\n",
    "    snp_short = snp_yfinance[-10:]['Adj Close'].mean()\n",
    "    snptrend = np.round(((snp_short - snp_long)/snp_long),2)*100\n",
    "    if snptrend > 5:\n",
    "        snptrend = 5\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    print('\\n\\n'+ '• S&P 500 Moving Average(20 vs 70) Percent change: ', '\\033[1m' + str(np.round(snptrend,2)) + '\\033[0m', '%')    \n",
    "\n",
    "    # Do Not purchase a day before the ex-div date, which is two days before the record date.\n",
    "    # Investors who purchase shares any day before the ex-dividend date will be documented as owners of shares on the record date.\n",
    "    srank = pd.read_csv('srank.csv')\n",
    "    \n",
    "    # Get top 20 Volume x Price list\n",
    "    srank['Vol_Mul_Price'] = srank['Volume'] * srank['Open'] \n",
    "    srank = srank.sort_values('Vol_Mul_Price', ascending = False).reset_index(drop = True)\n",
    "    Vol_Mul_Price = srank[:int(srank.Symbol.nunique()*volume_ticker_percent)].Symbol.to_list()\n",
    " \n",
    "    # Get another portfolist list from the corr. with 'yesterday's return'\n",
    "    perf_review = pd.read_csv('perf_review %s.csv' %mostrecentdate).sort_values('date')\n",
    "    corr_analysis = perf_review.corr().reset_index()[['index','day_change']]\n",
    "    corr_analysis = corr_analysis.sort_values('day_change', ascending = False).reset_index(drop = True)\n",
    "    corr_analysis = corr_analysis.loc[corr_analysis['index'].isin([\n",
    "        'sean_score', \n",
    "        '1Y Perf', \n",
    "        'Price / Cash Flow',\n",
    "        '10Y Total Return', \n",
    "        '10Y Perf', \n",
    "        'P/E TTM', \n",
    "        '5Y Perf',\n",
    "        '5Y Total Return',\n",
    "        'EV / EBITDA', \n",
    "        'Price / Sales',\n",
    "        'Last Price Vs. 10D SMA', \n",
    "        'Quick Ratio', \n",
    "        '3Y Perf',\n",
    "        '3Y Total Return', \n",
    "        'Week Vol / Shares', \n",
    "        'Current Ratio',\n",
    "        'EV / Sales', \n",
    "        'P/E FWD',\n",
    "        'Revenue 3Y', \n",
    "        'Revenue FWD', \n",
    "        'EBITDA FWD', \n",
    "        'Yield FWD', \n",
    "        'Yield TTM', \n",
    "        'Revenue YoY', \n",
    "        'FCF 3Y', \n",
    "        '4Y Avg Yield',\n",
    "        'EBITDA 3Y', \n",
    "        '24M Beta', \n",
    "        'Covered Ratio', \n",
    "        'Profitability Grade_score', \n",
    "        'Return on Assets', \n",
    "        'Asset Turnover',\n",
    "        'Profit Margin',\n",
    "        'Payout Ratio', \n",
    "        'FCF Margin',\n",
    "        'Div Rate FWD', \n",
    "        'EPS 3Y', \n",
    "        '60M Beta', \n",
    "        'Div Rate TTM', \n",
    "        'EPS Revision Grade_score',\n",
    "        'Valuation Grade_score', \n",
    "        'Float %', \n",
    "        'Net Income Margin',\n",
    "        'EBIT Margin', \n",
    "        '1M Perf', \n",
    "        'Price / Book', \n",
    "        'Net Income / Employee',\n",
    "        'YTD Perf', \n",
    "        'EBITDA Margin', \n",
    "        'Momentum Grade_score', \n",
    "        'Institutional Percent', \n",
    "        'EPS FWD', \n",
    "        'Wall St. Score', \n",
    "        'PEG FWD', \n",
    "        '6M Perf', \n",
    "        'EPS Estimate',\n",
    "        'EPS Actual',\n",
    "        'Debt to FCF', \n",
    "        'Div Growth 3Y',\n",
    "        'LT Debt to Total Capital', \n",
    "        'Return on Total Capital',\n",
    "        'Div Growth 5Y', \n",
    "        'PEG TTM',\n",
    "        'Last Price Vs. 100D SMA', \n",
    "        'Last Price Vs. 200D SMA', \n",
    "        'Last Price Vs. 50D SMA', \n",
    "        'avg_score',\n",
    "        'Growth Grade_score', \n",
    "        'SA Authors Score', \n",
    "        'Debt to Equity',\n",
    "        'Return on Equity', \n",
    "        'EPS YoY', \n",
    "        'Quant Score', \n",
    "        'EBITDA YoY'])].reset_index(drop = True)\n",
    "\n",
    "    neg_vars = ['Price / Cash Flow', \n",
    "                'P/E TTM', \n",
    "                'EV / EBITDA', \n",
    "                'Price / Sales',\n",
    "                'EV / Sales', \n",
    "                'P/E FWD',\n",
    "                '24M Beta', \n",
    "                '60M Beta', \n",
    "                'Price / Book', \n",
    "                'PEG FWD', \n",
    "                'Debt to FCF', \n",
    "                'LT Debt to Total Capital', \n",
    "                'PEG TTM',\n",
    "                'Debt to Equity']\n",
    "\n",
    "    corr_analysis_topindex = corr_analysis.head(12)[['index', 'day_change']].values.tolist()\n",
    "    #     print('\\n'+ 'Corr. Top 3 Variables: ')\n",
    "    #     print('\\n', corr_analysis_topindex[0])\n",
    "    #     print('\\n', corr_analysis_topindex[1])\n",
    "    #     print('\\n', corr_analysis_topindex[2])\n",
    "\n",
    "    def corr_tickers(corr_index_number):\n",
    "        if corr_analysis_topindex[corr_index_number][0] in neg_vars:\n",
    "            return list(srank[srank[corr_analysis_topindex[corr_index_number][0]] > np.percentile(srank[corr_analysis_topindex[corr_index_number][0]].values.tolist(), fs_cut+snptrend)]['Symbol'].values)\n",
    "        else:\n",
    "            return list(srank[srank[corr_analysis_topindex[corr_index_number][0]] < np.percentile(srank[corr_analysis_topindex[corr_index_number][0]].values.tolist(), (100-fs_cut)-snptrend)]['Symbol'].values)\n",
    "\n",
    "    # the number of conditions to filter out\n",
    "    corr_1st = corr_tickers(0)\n",
    "    corr_2nd = corr_tickers(1)\n",
    "    corr_3rd = corr_tickers(2)\n",
    "    corr_4th = corr_tickers(3)\n",
    "    corr_5th = corr_tickers(4)\n",
    "    corr_6th = corr_tickers(5)\n",
    "    corr_7th = corr_tickers(6)\n",
    "    corr_8th = corr_tickers(7)\n",
    "    corr_9th = corr_tickers(8)\n",
    "    corr_10th = corr_tickers(9)\n",
    "    corr_11th = corr_tickers(10)\n",
    "    corr_12th = corr_tickers(11)\n",
    "\n",
    "    #     print('\\n\\n\\n'+ 'Filtered out - Corr. 1st: ', corr_analysis_topindex[0][0],'\\n\\n', corr_1st)\n",
    "    #     print('\\n'+ 'Filtered out - Corr. 2nd: ', corr_analysis_topindex[1][0],'\\n\\n', corr_2nd)\n",
    "    #     print('\\n'+ 'Filtered out - Corr. 3rd: ', corr_analysis_topindex[2][0],'\\n\\n', corr_3rd)\n",
    "\n",
    "\n",
    "# remove - ratio analysis ###############################################################################################################\n",
    "    # fs_cut_plus: Weight (-) more filter out / (+) less filter out\n",
    "    \n",
    "    # positive indices <<<<<<<<<<<<<<<<\n",
    "    fs_cut_final = fs_cut\n",
    "    roe = sorted(srank[srank['Return on Equity'] < np.percentile(srank['Return on Equity'].values.tolist(), (100-fs_cut_final)-snptrend)]['Symbol'].tolist())\n",
    "    #     print('\\n\\n\\n'+ 'Filtered out - ROE: \\n\\n', roe)\n",
    "\n",
    "    fs_cut_final = fs_cut\n",
    "    quick_ratio = sorted(srank[srank['Quick Ratio'] < np.percentile(srank['Quick Ratio'].values.tolist(), (100-fs_cut_final)-snptrend)]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - Quick Ratio: \\n\\n', quick_ratio)\n",
    "\n",
    "    fs_cut_final = fs_cut - fs_cut_plus/2\n",
    "    ebitda_margin = sorted(srank[srank['EBITDA Margin'] < np.percentile(srank['EBITDA Margin'].values.tolist(), (100-fs_cut_final)-snptrend)]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - EBITDA Margin: \\n\\n', ebitda_margin)\n",
    "\n",
    "    \n",
    "    \n",
    "    # negative indices <<<<<<<<<<<<<<<<\n",
    "    fs_cut_final = fs_cut\n",
    "    debt_fcf = sorted(srank[srank['Debt to FCF'] > np.percentile(srank['Debt to FCF'].values.tolist(), fs_cut_final+snptrend)]['Symbol'].tolist())    \n",
    "    #     print('\\n'+ 'Filtered out - Debt to FCF: \\n\\n', debt_fcf)      \n",
    "\n",
    "    fs_cut_final = fs_cut\n",
    "    debt_equity = sorted(srank[srank['Debt to Equity'] > np.percentile(srank['Debt to Equity'].values.tolist(), fs_cut_final+snptrend)]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - Debt to Equity TTM: \\n\\n', debt_equity)         \n",
    "    \n",
    "    fs_cut_final = fs_cut - fs_cut_plus/2\n",
    "    ev_ebitda = sorted(srank[srank['EV / EBITDA'] > np.percentile(srank['EV / EBITDA'].values.tolist(), fs_cut_final+snptrend)]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - EV / EBITDA: \\n\\n', ev_ebitda)   \n",
    "\n",
    "    fs_cut_final = fs_cut - fs_cut_plus/2\n",
    "    price_book = sorted(srank[srank['Price / Book'] > np.percentile(srank['Price / Book'].values.tolist(), fs_cut_final+snptrend)]['Symbol'].tolist())    \n",
    "    #     print('\\n'+ 'Filtered out - Price / Book: \\n\\n', Price / Book)      \n",
    "\n",
    "    \n",
    "\n",
    "    # fixed indices <<<<<<<<<<<<<<<<\n",
    "    ni_ttm = sorted(srank[srank['NET Income TTM'] < 0]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - PEG TTM: \\n\\n', peg_ttm)   \n",
    "    \n",
    "    peg_ttm = sorted(srank[srank['PEG TTM'] > 1]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - PEG TTM: \\n\\n', peg_ttm)   \n",
    "\n",
    "    cash_operation = sorted(srank[srank['Cash from Operations'] < 0]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - Operating Cash: \\n\\n', cash_operation)      \n",
    "    \n",
    "    rev_surprise = sorted(srank[srank['Revenue Surprise'] < 0]['Symbol'].tolist())\n",
    "    #     print('\\n'+ 'Filtered out - Revenue Surprise: \\n\\n', rev_surprise) \n",
    "    \n",
    "#########################################################################################################################################\n",
    " \n",
    "    # remove tickers having (1)earning call (2)ex-dividend day \n",
    "    event_tickers = ticker_eventday_check()\n",
    "\n",
    "    # filter-out ########################################################################################\n",
    "    # adjust the number of conditions to filter out (e.g. corr_2nd, 3rd, 4th,5th)\n",
    "    \n",
    "    remove_tickers = list(event_tickers + buymonitor_blacklist\n",
    "                          + corr_1st + corr_2nd + corr_3rd + corr_4th + corr_5th + corr_6th + corr_7th + corr_8th + corr_9th + corr_10th\n",
    "                          + roe + quick_ratio + ebitda_margin\n",
    "                          + debt_fcf + debt_equity + ev_ebitda + price_book\n",
    "                          + ni_ttm + peg_ttm + cash_operation + rev_surprise)\n",
    "    remove_tickers = [i for n, i in enumerate(remove_tickers) if i not in remove_tickers[:n]] #duplicated \n",
    "    ##################################################################################################### \n",
    "    # adjust the number of conditions to filter out (e.g. corr_2nd, 3rd, 4th,5th)\n",
    "    \n",
    "    remove_tickers_superpass = list(event_tickers + buymonitor_blacklist\n",
    "                                  + corr_1st + corr_2nd + corr_3rd + corr_4th + corr_5th + corr_6th + corr_7th + corr_8th + corr_9th + corr_10th + corr_11th + corr_12th\n",
    "                                  + roe + quick_ratio + ebitda_margin\n",
    "                                  + debt_fcf + debt_equity + ev_ebitda + price_book\n",
    "                                  + ni_ttm + peg_ttm + cash_operation)\n",
    "    remove_tickers_superpass = [i for n, i in enumerate(remove_tickers_superpass) if i not in remove_tickers_superpass[:n]] #duplicated \n",
    "    \n",
    "    with open(\"remove_tickers_superpass.txt\", 'w') as f:\n",
    "        for s in remove_tickers_superpass:\n",
    "            f.write(str(s) + '\\n')\n",
    "    ##################################################################################################### \n",
    "    \n",
    "    # get another portfolio from srank.csv according to the top three R-squared\n",
    "    # and then proportionally add tickers per the R-squared value\n",
    "    highest_1st = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[0], ascending = False).head(int(corr_analysis_topindex[0][1]*50))['Symbol'].tolist()\n",
    "    highest_2nd = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[1], ascending = False).head(int(corr_analysis_topindex[1][1]*50))['Symbol'].tolist()\n",
    "    highest_3rd = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[2], ascending = False).head(int(corr_analysis_topindex[2][1]*50))['Symbol'].tolist()\n",
    "    highest_4th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[3], ascending = False).head(int(corr_analysis_topindex[3][1]*50))['Symbol'].tolist()\n",
    "    highest_5th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[4], ascending = False).head(int(corr_analysis_topindex[4][1]*50))['Symbol'].tolist()\n",
    "    highest_6th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[5], ascending = False).head(int(corr_analysis_topindex[5][1]*50))['Symbol'].tolist()\n",
    "    highest_7th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[6], ascending = False).head(int(corr_analysis_topindex[6][1]*50))['Symbol'].tolist()\n",
    "    highest_8th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[7], ascending = False).head(int(corr_analysis_topindex[7][1]*50))['Symbol'].tolist()\n",
    "    highest_9th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[8], ascending = False).head(int(corr_analysis_topindex[8][1]*50))['Symbol'].tolist()\n",
    "    highest_10th = srank.sort_values(corr_analysis.head(bluechips)['index'].values.tolist()[9], ascending = False).head(int(corr_analysis_topindex[9][1]*50))['Symbol'].tolist()\n",
    "    \n",
    "    highest_corr_index = highest_1st + highest_2nd + highest_3rd + highest_4th + highest_5th + highest_6th + highest_7th + highest_8th + highest_9th + highest_10th\n",
    "\n",
    "    # remove duplicated tickers in the list\n",
    "    highest_corr_index = [i for n, i in enumerate(highest_corr_index) if i not in highest_corr_index[:n]] \n",
    "\n",
    "    with open(\"highest_corr_index.txt\", 'w') as f:\n",
    "        for s in highest_corr_index:\n",
    "            f.write(str(s) + '\\n')     \n",
    "    with open('ex_highest_corr_index.txt', 'r') as f:\n",
    "        ex_highest_corr_index = [line.rstrip('\\n') for line in f]\n",
    "    with open('slist.txt', 'r') as f:\n",
    "        slist = [line.rstrip('\\n') for line in f]\n",
    "    with open('snp_value_tickers.txt', 'r') as f:\n",
    "        snp_value_tickers = [line.rstrip('\\n') for line in f]\n",
    "    with open('etf_portfolio.txt', 'r') as f:\n",
    "        etf_portfolio = [line.rstrip('\\n') for line in f]\n",
    "    with open('rsi_portfolio.txt', 'r') as f:\n",
    "        rsi_portfolio = [line.rstrip('\\n') for line in f]\n",
    "    with open('sentiment_portfolio.txt', 'r') as f:\n",
    "        sentiment_portfolio = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    # top 3 quant score\n",
    "    snp_main = pd.read_csv(f\"snp500 {currentdate}.csv\")\n",
    "    quant_score_top = snp_main.groupby(['Symbol']).mean().reset_index().sort_values('Quant Score', ascending = False).head(3).reset_index(drop = True)['Symbol'].tolist()\n",
    "\n",
    "    # previous day - good value tickers    \n",
    "    good_value = pd.read_csv('predict_valuation.csv')\n",
    "    good_value = good_value.groupby(['date', 'symbol']).mean().reset_index()\n",
    "    good_value = good_value[good_value['date'] == mostrecentdate] \n",
    "    good_value = good_value.query('valuation > 0.5 and up_down == 1')\n",
    "    good_value = good_value['symbol'].values.tolist()\n",
    "    non_timeout_tickers = ['XLE', 'XLF', 'XLRE', 'XLB', 'XLK', 'XLP', 'XLV', 'XLI', 'XLY', 'XLU', 'XLC',\n",
    "                           '^GSPC', 'BTC-USD', 'GLD', 'USO', '^TNX', '^VIX', 'GC=F', 'CL=F', 'DX-Y.NYB']\n",
    "    good_value_tickers = []\n",
    "    for i in good_value:\n",
    "        if i not in non_timeout_tickers:\n",
    "            good_value_tickers.append(i)    \n",
    "    with open(\"good_value_tickers.txt\", 'w') as f:\n",
    "        for s in good_value_tickers:\n",
    "            f.write(str(s) + '\\n')  \n",
    "\n",
    "    #     print('\\n\\n\\nImport Portfolio Candidates______________________________________________________________')\n",
    "    #     print('\\n'+ f\"Sean Score - Tickers:        \\n\\n{slist}\")\n",
    "    #     print('\\n'+ f\"Ratio analysis - Tickers:    \\n\\n{snp_value_tickers}\")\n",
    "    #     print('\\n'+ f\"Perf. Review Corr. - Tickers:\\n\\n{highest_corr_index}\")\n",
    "    #     print('\\n'+ f\"Radarplot largest - Tickers: \\n\\n{ex_highest_corr_index}\")  \n",
    "    #     print('\\n'+ f\"Previous Good Value - Tickers: \\n\\n{good_value_tickers}\")  \n",
    "    #     print('\\n'+ f\"ETF - Tickers: \\n\\n{etf_portfolio}\") \n",
    "    #     print('\\n'+ f\"RSI - Tickers: \\n\\n{rsi_portfolio}\")\n",
    "    #     print('\\n'+ f\"Quant Score - Tickers: \\n\\n{quant_score_top}\")   \n",
    "\n",
    "    with open(\"tickers_to_sell_big.txt\", 'r') as f:\n",
    "        tickers_to_sell_big = [line.rstrip('\\n') for line in f]      \n",
    "\n",
    "    slist_snpvalue_corr = tickers_to_sell_big + slist + ex_highest_corr_index + quant_score_top + rsi_portfolio\n",
    "    slist_snpvalue_corr = [i for n, i in enumerate(slist_snpvalue_corr) if i not in slist_snpvalue_corr[:n]] \n",
    "\n",
    "    # remove tickers\n",
    "    portfolio = [i for i in slist_snpvalue_corr if i not in remove_tickers]\n",
    "    portfolio = sorted(portfolio)\n",
    "    \n",
    "\n",
    "# filtering starts from below\n",
    "\n",
    "\n",
    "#     # buy sign check by volume vs price\n",
    "#     portfolio_after_buysign = []\n",
    "#     print(f\"\\n\\n\\nSrank Filtering ===> F/S Filtering ===> {buy_sign_lookback_days} Days Buy Sign Check:\\n\\n {portfolio}\\n\")\n",
    "#     c = 0 \n",
    "#     for stock in portfolio:\n",
    "#         c+=1\n",
    "#         print(f\"> Ticker ({c}/{len(portfolio)}): {stock}\", end = '                                                                 \\r')\n",
    "#         try:\n",
    "#             if buy_sign_day(stock, buy_sign_lookback_days) == True:\n",
    "#                 portfolio_after_buysign.append(stock)\n",
    "#             else:\n",
    "#                 pass\n",
    "#         except:\n",
    "#             print('\\nException >>> Buy Sign Check: ', stock)\n",
    "#             pass\n",
    "#     portfolio = portfolio_after_buysign \n",
    "\n",
    "\n",
    "    # Volatility test \n",
    "    initial_vola_cut_rate = vola_cut_rate_value\n",
    "    if snptrend > 0:\n",
    "        vola_cut_rate = initial_vola_cut_rate\n",
    "    else:\n",
    "        vola_cut_rate = initial_vola_cut_rate + snptrend/10\n",
    "\n",
    "    portfolio_after_vola = []\n",
    "    print(f\"\\n\\n\\n\\n\\nSrank Filtering ===> F/S Filtering ===> {vola_cut_rate}% Volatility Test Candidates:\\n\\n {portfolio}\\n\")\n",
    "    c = 0 \n",
    "    for stock in portfolio:\n",
    "        c+=1\n",
    "        print(f\"> Ticker ({c}/{len(portfolio)}): {stock}\", end = '                                                                 \\r')\n",
    "        try:\n",
    "            ##############################################################################################\n",
    "            if volatility_check(stock) < volatility_check('^GSPC')*vola_cut_rate: # filter out > S&P500*x\n",
    "            ##############################################################################################    \n",
    "                portfolio_after_vola.append(stock)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('\\nException >>> Volatility Test Candidates: ', stock)\n",
    "            pass\n",
    "    portfolio = portfolio_after_vola\n",
    "    \n",
    "    \n",
    "    # mdd test: get ETF(icld. S&P500) MDD average\n",
    "    avg_etf_mdd = mdd_cal(etf_ticker) * vola_cut_rate\n",
    "\n",
    "    portfolio_after_mdd = []\n",
    "    print(f\"\\n\\n\\nVolatility filtering ===> MDD Test Candidates:\\n\\n {portfolio}\\n\")\n",
    "    c = 0 \n",
    "\n",
    "    for stock in portfolio:\n",
    "        c+=1\n",
    "        print(f\"> Ticker ({c}/{len(portfolio)}): {stock}\", end = '                                                                 \\r')\n",
    "        try:\n",
    "            if avg_etf_mdd > mdd_cal([stock]):\n",
    "                portfolio_after_mdd.append(stock)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('\\nException >>> MDD Test Candidates: ', stock)\n",
    "            pass\n",
    "    portfolio = portfolio_after_mdd\n",
    "\n",
    "    \n",
    "    # add bypassing ticker list\n",
    "    portfolio_superpass = super_pass_filtering(superpass_buysign_lookbackdays)    # Super Pass filtering   \n",
    "    \n",
    "    ################################################\n",
    "    # w/o remove filter: Only look at F/S condition\n",
    "    portfolio.extend(buymonitor_whitelist) \n",
    "    portfolio.extend(Vol_Mul_Price) \n",
    "    portfolio.extend(good_value_tickers)      \n",
    "    portfolio.extend(highest_corr_index)\n",
    "    portfolio.extend(portfolio_superpass)\n",
    "    portfolio.extend(sentiment_portfolio)\n",
    "    portfolio = sorted([i for n, i in enumerate(portfolio) if i not in portfolio[:n]]) #duplicated \n",
    "    ################################################\n",
    "    \n",
    "    \n",
    "    # Moving Average crossover test\n",
    "    snp_sector = pd.read_csv('./snp500/snp_sector_index.csv')\n",
    "    portfolio = [i for i in portfolio if i in snp_sector.Symbol.tolist()] # only include tickers within S&P500.csv\n",
    "    portfolio_after_crossover = []\n",
    "    print(f\"\\n\\n\\n\\n\\nMDD Test ===> Bypass list adding ===> Moving Average Crossover Test Candidates:\\n\\n {portfolio}\\n\")\n",
    "    c = 0 \n",
    "    for stock in portfolio:\n",
    "        c+=1\n",
    "        print(f\"> Ticker ({c}/{len(portfolio)}): {stock}\", end = '                                                                 \\r')\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            analysis = analysis_price(stock)           \n",
    "            if analysis[8] == False:\n",
    "                portfolio_after_crossover.append(stock)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('\\nException >>> Moving Average Test Candidates: ', stock)\n",
    "            pass\n",
    "    portfolio = portfolio_after_crossover\n",
    "        \n",
    "        \n",
    "    # Sector comparison test\n",
    "    portfolio_after_sector_comparison = []\n",
    "    print(f\"\\n\\n\\n\\n\\nMoving Average Crossover Test Candidates ===> Sector Comparison Test Candidates:\\n\\n {portfolio}\\n\")\n",
    "    c = 0 \n",
    "    for stock in portfolio:\n",
    "        c+=1\n",
    "        print(f\"> Ticker ({c}/{len(portfolio)}): {stock}\", end = '                                                                 \\r')\n",
    "        try:\n",
    "            time.sleep(0.5)\n",
    "            analysis = analysis_price(stock)           \n",
    "            if analysis_price(snp_sector[snp_sector['Symbol'] == stock]['Ticker'].values[0])[5]*portfolio_sector_ticker < analysis[5]:\n",
    "                portfolio_after_sector_comparison.append(stock)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('\\nException >>> Sector Comparison Test Candidates: ', stock)\n",
    "            pass\n",
    "    portfolio = portfolio_after_sector_comparison    \n",
    "          \n",
    "        \n",
    "    # Strategy test     \n",
    "    while True:\n",
    "        portfolio, analysis_success_rate = portfolio_strategy_test(portfolio_after_sector_comparison)\n",
    "\n",
    "        # Strategy test success rate check\n",
    "        min_success_rate = 0.3\n",
    "        if analysis_success_rate >= min_success_rate:\n",
    "            print('\\033[1m' + f'\\n\\n\\nStrategy Analysis Success Rate is higher than {min_success_rate*100} %\\n\\n\\n' + '\\033[0m')\n",
    "            break\n",
    "        else:\n",
    "            print('\\033[1m' + f'\\n\\n\\nStrategy Analysis Success Rate is less than {min_success_rate*100} %\\n\\n\\n' + '\\033[0m')\n",
    "            min_SuccessRate -= 1\n",
    "    \n",
    "    if min_SuccessRate < min_SuccessRate_fixed:\n",
    "        min_SuccessRate = min_SuccessRate_fixed            \n",
    "    print('\\033[1m' + f'\\n\\n\\nAdjusted min_success_rate: {min_SuccessRate} %\\n\\n\\n' + '\\033[0m')\n",
    "\n",
    "    \n",
    "    # BBand buy sign check\n",
    "    BBand_Buy_Sign = BBand_Buy_Sign_Count(portfolio)\n",
    "    print('\\033[1m' + f'\\n\\n\\nBollinger Buy Sign Rate: {BBand_Buy_Sign} %\\n\\n\\n' + '\\033[0m')\n",
    "    \n",
    "    # add inv tickers\n",
    "    # w/o remove filter\n",
    "    portfolio.extend(inv_tickers)\n",
    "    \n",
    "    # sort out event-day tickers\n",
    "    portfolio = [i for i in portfolio if i not in event_tickers]\n",
    "    portfolio = [i for n, i in enumerate(portfolio) if i not in portfolio[:n]] #duplicated \n",
    "    portfolio = sorted(portfolio)\n",
    "\n",
    "    # delete previous files\n",
    "    dir_cleaner(mostrecentdate)\n",
    "    \n",
    "    # load and store the previous portfolio\n",
    "    with open(\"portfolio_original.txt\", 'r') as f:\n",
    "        previous_portfolio = [line.rstrip('\\n') for line in f]\n",
    "    with open(\"previous_portfolio.txt\", 'w') as f:\n",
    "        for s in previous_portfolio:\n",
    "            f.write(str(s) + '\\n')\n",
    "    \n",
    "    # the new tickers in the current portfolio\n",
    "    new_portfolio = []\n",
    "    for stock in portfolio:\n",
    "        if stock not in previous_portfolio:\n",
    "            new_portfolio.append(stock)\n",
    "    print(f'\\n\\nNew tickers in the portfolio:\\n{new_portfolio}\\n')\n",
    "    message = f'We have {len(new_portfolio)} new tickers in the new portfolio.'\n",
    "    print('\\n' + '\\033[1m' + message + '\\033[0m' + '\\n\\n')\n",
    "    voice_message(message)\n",
    "    \n",
    "    # save list to portfolio.txt\n",
    "    with open(\"portfolio.txt\", 'w') as f:\n",
    "        for s in portfolio:\n",
    "            f.write(str(s) + '\\n')\n",
    "\n",
    "    with open(\"portfolio_original.txt\", 'w') as f:\n",
    "        for s in portfolio:\n",
    "            f.write(str(s) + '\\n')\n",
    "            \n",
    "    # portfolio plot vs sector etf and moving average\n",
    "    portfolio_plot_list = [i for i in portfolio if i not in inv_tickers]\n",
    "    for stock_ticker in portfolio_plot_list:\n",
    "        sector_stock_plot(stock_ticker)\n",
    "        dmv_plot(stock_ticker)\n",
    "        rsi_plot(stock_ticker)\n",
    "        print('\\n\\n\\n\\n\\n')\n",
    "            \n",
    "    # export portfolio srank to email\n",
    "    srank_portfolio = pd.read_csv('srank.csv')\n",
    "    srank_portfolio = srank_portfolio[srank_portfolio['Symbol'].isin(portfolio)].reset_index(drop = True)\n",
    "    srank_portfolio.to_csv('srank_portfolio.csv', index = False)\n",
    "\n",
    "    # send portfolio to the email list members\n",
    "    slistemail(portfolio) \n",
    "\n",
    "    pd.set_option('display.max_columns', 70)\n",
    "    pd.set_option('display.max_rows', 70)\n",
    "\n",
    "    srank_portfolio_df = srank.loc[srank['Symbol'].isin(portfolio)].sort_values('Upcoming Announce Date').reset_index(drop=True)\n",
    "    srank_portfolio_df.to_csv('portfolio_df.csv', index = False)\n",
    "    display(srank_portfolio_df['GICS Sector'].value_counts())\n",
    "    display(srank_portfolio_df)\n",
    "    \n",
    "    \n",
    "def main_creation():\n",
    "    if str(yfinance_df_setting('^GSPC')[-1:].Date.values[0])[:10] == currentdate:\n",
    "        print('> The last line == currentdate\\n\\n\\n')\n",
    "        \n",
    "#         webbrowser.open(\"Nuclear Launch Detected.mp3\")\n",
    "#         alarm_open(2)\n",
    "        message = \"Portfolio analysis has been launched\"\n",
    "        voice_message(message)\n",
    "        print(message)\n",
    "        creation_start = current_time()\n",
    "        print(f'\\n\\n\\n\\n\\n{creation_start}\\n')\n",
    "       \n",
    "        tickers = pd.read_csv('snp500\\snp_sector_index.csv').sort_values('Symbol')['Symbol'].to_list()\n",
    "        sentiment_portfolio(tickers, 7)\n",
    "        num_new_portfolio = pfo_creation(bluechips)\n",
    "        \n",
    "        with open('portfolio.txt', 'r') as f:\n",
    "            portfolio = [line.rstrip('\\n') for line in f]\n",
    "        with open(\"etf_portfolio.txt\", 'r') as f:\n",
    "            etf_portfolio = [line.rstrip('\\n') for line in f]\n",
    "        print(f\"\\nETF Portfolio: {etf_portfolio}\\n\\n\")\n",
    "        print('\\033[1m' + f\"\\n\\n\\n• Total number of monitoring tickers: {len(portfolio+etf_portfolio)} EA\\n\\n\\n\" + '\\033[0m')\n",
    "        \n",
    "        # back test\n",
    "        portfolio_backtest()\n",
    "        \n",
    "        # time cal\n",
    "        print('\\033[1m' + f'\\n\\n\\n\\n\\n• Creation Start: {creation_start}' + '\\033[0m')\n",
    "        print('\\033[1m' + f'• Creation End:   {current_time()}\\n\\n\\n\\n\\n' + '\\033[0m')\n",
    "        \n",
    "    else:\n",
    "        print('> The last line != currentdate\\n')\n",
    "        if timechecknow() < -30:\n",
    "            print('Stand by Mode', end = '          \\r')\n",
    "            voice_message(\"\"\"\\\n",
    "                          Opening bell stand by for 30 minutes\"\"\")                \n",
    "            time.sleep(1800)\n",
    "        elif timechecknow() < -10:\n",
    "            print('Stand by Mode', end = '          \\r')\n",
    "            voice_message(\"\"\"\\\n",
    "                          Opening bell stand by for 10 minutes\"\"\")                \n",
    "            time.sleep(600)\n",
    "        else:\n",
    "            print('Stand by Mode', end = '          \\r')\n",
    "            voice_message(\"\"\"\\\n",
    "                          Opening bell stand by for 3 minutes\"\"\")      \n",
    "            \n",
    "        main_creation()\n",
    "                \n",
    "    return num_new_portfolio\n",
    "     \n",
    "     \n",
    "# if __name__ == '__main__':\n",
    "#     if day_check() != 'Saturday' and day_check() != 'Sunday' and timechecknow() < -20:\n",
    "#         print(f'\\n\\n{current_time()}  Portfolio Pre-generation\\n\\n')\n",
    "#         main_creation()\n",
    "#     else:\n",
    "#         print('\\n', '\\033[1m' + 'The market is closed or underway.' + '\\033[1m', '\\n')\n",
    "        \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The portfolio creation gets ready.' + '\\033[0m', '\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56364c4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## Portfolio - Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29247804",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def peertickers():\n",
    "    peers = {}\n",
    "\n",
    "#####################################################################################  \n",
    "    \n",
    "    peers['GOOGL'] = ['GOOGL', 'META', 'TCEHY', 'SNAP', 'BIDU', 'MTCH', '^GSPC']\n",
    "    peers['QCOM'] = ['QCOM', 'INTC', 'AMD', 'TXN', 'AVGO', 'MU', '^GSPC']\n",
    "    peers['AAPL'] = ['AAPL', 'DELL', 'HPQ', 'FUJIY', 'CAJPY', 'STX', '^GSPC']\n",
    "    peers['MSFT'] = ['MSFT', 'ORCL', 'NOW', 'FTNT', 'PANW', 'VMW', '^GSPC']\n",
    "    peers['NVDA'] = ['NVDA', 'TSM', 'AVGO', 'INTC', 'AMD', 'QCOM', '^GSPC']\n",
    "    peers['CRM'] = ['CRM', 'ADBE', 'SAP', 'INTU', 'TEAM', 'DASTY', '^GSPC']\n",
    "    peers['TSLA'] = ['TSLA', 'TM', 'GM', 'MBGAF', 'F', 'LCID', '^GSPC']\n",
    "    peers['AMZN'] = ['AMZN', 'BABA', 'JD', 'MELI', 'PDD', 'DASH', '^GSPC']\n",
    "    peers['V'] = ['V', 'MA', 'PYPL', 'ADP', 'SQ', 'FISV', '^GSPC']\n",
    "    peers['JPM'] = ['JPM', 'BAC', 'WFC', 'RY', 'TD', 'HSBC', '^GSPC']\n",
    "    peers['JNJ'] = ['JNJ', 'PFE', 'LLY', 'NVO', 'MRK', 'AZN', '^GSPC']\n",
    "    peers['WMT'] = ['WMT', 'COST', 'WMMVY', 'CRRFY', 'BJ', 'ASAI', '^GSPC']\n",
    "    peers['XOM'] = ['XOM', 'CVX', 'SHEL', 'BP', 'TTE', 'EQNR', '^GSPC']\n",
    "    peers['AMT'] = ['AMT', 'CCI', 'EQIX', 'PSA', 'DLR', 'SBAC', '^GSPC']\n",
    "    peers['DIS'] = ['DIS', 'NFLX', 'SPOT', 'LYV', 'WMG', 'ROKU', '^GSPC']\n",
    "    peers['GIANT'] = ['GOOGL', 'AAPL', 'MSFT', 'TSLA', 'AMZN', 'META', '^GSPC']\n",
    "    \n",
    "#####################################################################################    \n",
    "    \n",
    "    \n",
    "    for key, values in peers.items():\n",
    "        try:\n",
    "            print('\\033[1m' + key + '____________________________________________________________________________' + '\\033[0m', '\\n')\n",
    "\n",
    "            dfpeer = {}\n",
    "\n",
    "            for i in values:\n",
    "                tday_yes = yfinance_df_rsi(i).tail(2)\n",
    "                change_percent = np.round((tday_yes['Adj Close'].values[1]-tday_yes['Adj Close'].values[0])/tday_yes['Adj Close'].values[0]*100, 2)\n",
    "                dfpeer[i] = change_percent\n",
    "            df = pd.DataFrame([dfpeer]).T.rename(columns = {0:'change'}).reset_index()\n",
    "            gain = df[df['change'] > 0].sort_values('change',ascending = False)\n",
    "            loss = df[df['change'] < 0].sort_values('change',ascending = False)\n",
    "\n",
    "            plt.figure(figsize = (10,5))\n",
    "            plt.bar(gain['index'], gain['change'], alpha = 0.95, color = sb.color_palette()[0])\n",
    "            plt.bar(loss['index'], loss['change'], alpha = 0.95, color = sb.color_palette()[3])\n",
    "            plt.grid(axis = 'x')\n",
    "            plt.grid(axis = 'y')\n",
    "            plt.show()\n",
    "        except:\n",
    "            print('Peer valuation error', key, values)\n",
    "            pass\n",
    "\n",
    "\n",
    "def buy_blacklistcheck_norm(portfolio_original):\n",
    "    global purchased # filter out Norm list from monitoring  \n",
    "    \n",
    "    print('\\n\\n\\nPortfolio return updating .....\\n')\n",
    "    # Get change% and normal distribution\n",
    "\n",
    "    dfportfolio = {}\n",
    "    yes_price = []\n",
    "    tod_price = []\n",
    "    average_price = []\n",
    "    standard_deviation = []\n",
    "    normal_distribution = []\n",
    "    c = 1\n",
    "    for i in portfolio_original:\n",
    "        time.sleep(0.3)\n",
    "        print('Analysing: ', c, '/', len(portfolio_original),'- Ticker: ', i, end = '          \\r')\n",
    "        c+=1  \n",
    "        try:\n",
    "            tday_yes = yfinance_df(i).tail(2)\n",
    "            change_percent = np.round((tday_yes['Adj Close'].values[1]-tday_yes['Adj Close'].values[0])/tday_yes['Adj Close'].values[0]*100, 2)\n",
    "            dfportfolio[i] = change_percent\n",
    "            yes_price.append(tday_yes['Adj Close'].values[0])\n",
    "            tod_price.append(tday_yes['Adj Close'].values[1])\n",
    "    # Normal distribution \n",
    "    # range setting: 52 weeks; 263 working days in a year\n",
    "            average = yfinance_df(i).tail(253)['Adj Close'].mean()\n",
    "            average_price.append(average)\n",
    "\n",
    "            std = yfinance_df(i).tail(253)['Adj Close'].std()\n",
    "            standard_deviation.append(std)    \n",
    "    # percentile setting: 10% out of 263 working days\n",
    "            norm = np.percentile(np.random.normal(average, std, 10000000), 10)\n",
    "            normal_distribution.append(norm)\n",
    "        except:\n",
    "            yfinance_df(i).tail(2)\n",
    "            pass\n",
    "    df = pd.DataFrame([dfportfolio]).T.rename(columns = {0:'Change'})\n",
    "\n",
    "    ttoday = date.today()\n",
    "    df['Date'] = ttoday \n",
    "    df['Time'] = timechecknow()\n",
    "    df['Today'] = tod_price   \n",
    "    df['Yesterday'] = yes_price  \n",
    "    df['Average'] = average_price\n",
    "    df['Standard_deviation'] = standard_deviation\n",
    "    df['Normal_distribution'] = normal_distribution\n",
    "    df = df.sort_values('Change', ascending = False).reset_index()\n",
    "    df = df[['Date', 'Time', 'index', 'Change', 'Today', 'Yesterday', 'Average', 'Standard_deviation', 'Normal_distribution']]\n",
    "\n",
    "    df_accu = pd.read_csv('daily_summary_target_tickers.csv')\n",
    "    df_merged = pd.concat([df_accu, df])\n",
    "    df_merged.to_csv('daily_summary_target_tickers.csv', index = False)\n",
    "    print('\\n\\n')\n",
    "    print(df[['Date', 'index', 'Change']],'\\n')\n",
    "\n",
    "#     with open(\"portfolio.txt\", 'r') as f:\n",
    "#         portfolio = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "#     for stock in portfolio:\n",
    "#     # S&P 500: Do not buy monitor and put Blacklist condition \n",
    "#         if stock != \"GLD\" and stock != \"USO\" and stock != \"BTC-USD\" and analysis_price(stock)[4] < df[df['index'] == stock]['Normal_distribution'].values[0]:\n",
    "#             for i in range(1):\n",
    "#                 try:\n",
    "#                     purchased.append(stock)        \n",
    "#                 except:\n",
    "#                     print('>>> Norm Blacklist has not filtered out. Ticker: ', stock)\n",
    "#                     alarm()\n",
    "#                     pass\n",
    "\n",
    "#             message = \"\"\"\\\n",
    "# [{}] Ticker has been placed less than P-value.\"\"\".format(stock)\n",
    "#             print('\\n')\n",
    "#             print(message)\n",
    "\n",
    "#         else:\n",
    "#             pass          \n",
    "        \n",
    "\n",
    "def performance_review():\n",
    "    # only execute once a day\n",
    "    from os.path import exists\n",
    "    asset_review_exists = exists('asset_review %s.csv' %currentdate)\n",
    "    \n",
    "    if asset_review_exists == False:\n",
    "        \n",
    "    # robinhood_gain&loss\n",
    "        robin = robin_login()\n",
    "        robin['date'] = today_to_add\n",
    "\n",
    "        robin = robin[['date', 'index', 'name', 'price', 'quantity', 'average_buy_price', 'equity',\n",
    "               'percent_change', 'equity_change']]\n",
    "        numer = ['price','quantity','average_buy_price','equity','percent_change','equity_change']\n",
    "        for i in numer:\n",
    "            robin[i] = pd.to_numeric(robin[i])\n",
    "\n",
    "        # samsung info\n",
    "        krw = exchange_check()\n",
    "        samsung = {}\n",
    "        samsung['index'] = '005930.KS'\n",
    "        samsung['name'] = 'SamsungElec'\n",
    "        samsung['price'] = yfinance_df_min('005930.KS')['Adj Close'].values[0]\n",
    "        samsung['quantity'] = 5000\n",
    "        samsung['average_buy_price'] = 78325\n",
    "        samsung = pd.DataFrame([samsung])\n",
    "        samsung['equity'] = samsung.price * samsung.quantity / krw\n",
    "        samsung['percent_change'] = ((samsung.price - samsung.average_buy_price) / samsung.average_buy_price)*100\n",
    "        samsung['equity_change'] = (samsung.price - samsung.average_buy_price) * samsung.quantity / krw\n",
    "     \n",
    "        # merge\n",
    "        performance = pd.concat([robin, samsung]).reset_index(drop=True)\n",
    "        performance['date'] = today_to_add\n",
    "\n",
    "        # target sell price\n",
    "\n",
    "        with open(\"realgainrate_big.txt\", 'r') as f:\n",
    "            realgainrate_big = [line.rstrip('\\n') for line in f]    \n",
    "            realgainrate_big = float(realgainrate_big[0])\n",
    "\n",
    "        with open(\"realgainrate_mid.txt\", 'r') as f:\n",
    "            realgainrate_mid = [line.rstrip('\\n') for line in f]    \n",
    "            realgainrate_mid = float(realgainrate_mid[0])\n",
    "\n",
    "        #     target_percentage = 1 + ((realgainrate_big/100)+(realgainrate_mid/100))/2\n",
    "        target_percentage = 1 + (realgainrate_mid/100)\n",
    "\n",
    "        ptg = []\n",
    "        for i in performance.average_buy_price:\n",
    "            ptg.append(i*target_percentage)\n",
    "        performance['target_sell_price'] = ptg   \n",
    "\n",
    "        # Short-term: 10% <= Up to $80,800\n",
    "        # Long-term: 0% <= Up to $19,900\n",
    "        tax = []\n",
    "        for i in performance.price:\n",
    "            tax.append(i*0.1) # \n",
    "        performance['tax_shorterm_share'] = tax \n",
    "        tax = []\n",
    "        for i in performance.equity:\n",
    "            tax.append(i*0.1) # \n",
    "        performance['tax_shorterm'] = tax \n",
    "\n",
    "        # portion%\n",
    "        ptg = []\n",
    "        for i in performance.equity:\n",
    "            ptg.append(i/performance.equity.sum()*100)\n",
    "        performance['percentage'] = ptg   \n",
    "\n",
    "    # asset_review: save, and read & combine \n",
    "        asset_review = performance.copy() # for asset review\n",
    "        asset_review['date'] = pd.to_datetime(asset_review['date'])\n",
    "        asset_review.to_csv('asset_review %s.csv' %currentdate, index = False)\n",
    "        asset_review = pd.concat(map(pd.read_csv, ['asset_review %s.csv' %mostrecentdate, 'asset_review %s.csv' %currentdate]), ignore_index=True)\n",
    "        asset_review.to_csv('asset_review %s.csv' %currentdate, index = False) \n",
    "        asset_review.to_csv('./Backup_asset_review/asset_review %s.csv' %currentdate, index = False) \n",
    "\n",
    "        import os\n",
    "        os.remove('asset_review %s.csv' %mostrecentdate)  \n",
    "\n",
    "        # add error from predict_valuation.csv\n",
    "        predict = pd.read_csv('predict_valuation.csv')\n",
    "        predict_error = pd.DataFrame(predict.groupby('symbol')[['error', 'compensation_corr']].mean()).reset_index()\n",
    "        performance = performance.merge(predict_error, how = 'inner', left_on = 'index', right_on = 'symbol').sort_values('percent_change', ascending = False)\n",
    "\n",
    "    # perf_review for 'srank.csv': save, and read & combine \n",
    "        seansc = pd.read_csv('srank.csv')[['Symbol','sean_score']]\n",
    "        seansc.rename(columns = {'Symbol':'index'}, inplace = True)\n",
    "        perf_review = performance.merge(seansc, how = 'inner', left_on = 'index', right_on = 'index').sort_values('percentage')\n",
    "        perf_review = perf_review[['date', 'index','percent_change', 'error', 'compensation_corr']].sort_values('percent_change', ascending = False).reset_index(drop=True)\n",
    "\n",
    "        srank = pd.read_csv('srank.csv')    \n",
    "        perf_review = perf_review.merge(srank, how = 'inner', left_on = 'index', right_on = 'Symbol').sort_values('index').reset_index(drop = True)\n",
    "        perf_review.to_csv('perf_review %s.csv' %currentdate, index = False)\n",
    "        perf_review = pd.concat(map(pd.read_csv, ['perf_review %s.csv' %mostrecentdate, 'perf_review %s.csv' %currentdate]), ignore_index=True)\n",
    "        perf_review = perf_review.sort_values(['index', 'date']).reset_index(drop = True)\n",
    "\n",
    "        day_change = []\n",
    "        for i in range(len(perf_review.index)):\n",
    "            if i == 0:\n",
    "                day_change.append(0)\n",
    "            elif perf_review['index'][i-1] == perf_review['index'][i]:\n",
    "                daychange = perf_review['percent_change'][i] - perf_review['percent_change'][i-1]\n",
    "                day_change.append(daychange)\n",
    "            elif perf_review['index'][i-1] != perf_review['index'][i]:\n",
    "                day_change.append(0)\n",
    "            else:\n",
    "                pass\n",
    "        perf_review['day_change'] = day_change\n",
    "        perf_review['date'] = perf_review['date'].astype('datetime64[ns]')\n",
    "\n",
    "        perf_review.to_csv('perf_review %s.csv' %currentdate, index = False) \n",
    "        perf_review.to_csv('./Backup_perf_review/perf_review %s.csv' %currentdate, index = False) \n",
    "\n",
    "        import os\n",
    "        os.remove('perf_review %s.csv' %mostrecentdate)  \n",
    "        print('\\n\\nPerformance Review for Today: Done\\n\\n')\n",
    "    else:\n",
    "        print('\\n\\nPerformance Review for today has already executed.\\n\\n')\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The performance review gets ready.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a598298",
   "metadata": {
    "deletable": false,
    "editable": false,
    "heading_collapsed": true,
    "hide_input": false
   },
   "source": [
    "# Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b1fa0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Engine #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2117de65",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def macrosnp(): \n",
    "    global proba_initial_value\n",
    "    global predict_proba_value\n",
    "    \n",
    "    # check S&P 500\n",
    "    snp_dropped_percentage = analysis_price('^GSPC')[2]\n",
    "    snp_dropped_limit = sum(check_snp_t_y)\n",
    "    snp_surged_limit = -sum(check_snp_t_y)/2\n",
    "    \n",
    "    if snp_dropped_percentage > snp_surged_limit and proba_initial_value != proba_initial_value_original:\n",
    "        # proba value reset by market trend\n",
    "        proba_initial_value = proba_initial_value_original\n",
    "        print(f'\\n\\n\\nImportant >>> S&P 500 has surged by {np.round(snp_dropped_percentage, 2)} %, Initial Proba Value resets at {proba_initial_value_original}\\n\\n\\n')\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    if snp_dropped_percentage < snp_dropped_limit:\n",
    "        print('\\033[1m' + '\\nS&P 500 change(%) is less than the limit.\\n\\n\\n' + '\\033[0m')\n",
    "        \n",
    "        # proba value reset by market trend   \n",
    "        proba_initial_value = proba_initial_value + 0.02\n",
    "        print(f'\\n\\n\\nImportant >>> S&P 500 has dropped by {np.round(snp_dropped_percentage, 2)} %, Initial Proba Value increases by 2 %\\n\\n\\n')\n",
    "        \n",
    "        print(f'\\n\\n{current_time()} - Market Sentiment Reflection: Proba Value adjusting\\n\\n')\n",
    "        proba_value_reset()\n",
    "        \n",
    "        # decrease the monitoring threshold \n",
    "        check_snp_t_y.append(-0.3)\n",
    "        snp_dropped_limit = sum(check_snp_t_y)\n",
    "        snp_dropped_percentage = analysis_price('^GSPC')[2]\n",
    "        print(f'\\n\\n\\nS&P 500 is at {np.round(snp_dropped_percentage, 2)} % | New Monitoring Limit: {np.round(snp_dropped_limit, 2)} %')\n",
    "        \n",
    "        # email\n",
    "        message = \"\"\"\\\n",
    "Subject: [{}] dropped: {}%\"\"\".format('S&P 500', '%.2f' %snp_dropped_percentage)\n",
    "        voice_message(message)\n",
    "#         emailsend_to_server(message)\n",
    "        \n",
    "    else:\n",
    "        print('\\nS&P 500 change(%) is higher than the limit.\\n\\n\\n')     \n",
    "\n",
    "        # increase the monitoring threshold\n",
    "        if sum(check_snp_t_y) < sum(check_snp_t_y_original):\n",
    "            check_snp_t_y.append(0.3)\n",
    "            snp_dropped_limit = sum(check_snp_t_y)\n",
    "        \n",
    "        # limit the max. value when reached the limit\n",
    "        if snp_dropped_limit > sum(check_snp_t_y_original):\n",
    "            snp_dropped_limit = sum(check_snp_t_y_original)\n",
    "        \n",
    "        print(f'\\n\\n\\nS&P 500 is at {np.round(snp_dropped_percentage, 2)} % | New Monitoring Limit: {np.round(snp_dropped_limit, 2)} %')\n",
    "      \n",
    "        pass\n",
    "\n",
    "\n",
    "def sectorcheck_main(val_times):      \n",
    "    voice_message(\"\"\"\\\n",
    "        Sector valuation starts\"\"\")\n",
    "    \n",
    "# Macro: S&P500 and Sectors    \n",
    "# Sector SPDR ETF: https://seekingalpha.com/etfs-and-funds/etf-tables/sectors\n",
    "\n",
    "    # read S&P 500 to get 'GICS Sector' column    \n",
    "    snp_sector = pd.read_csv('./snp500/snp500.csv')\n",
    "    sector = snp_sector[['Symbol', 'GICS Sector']]\n",
    "\n",
    "    # read 'portfolio.txt' to check each sector' from snp    \n",
    "    with open(\"portfolio_original.txt\", 'r') as f:\n",
    "        portfolio_original = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    with open(\"portfolio_superpass.txt\", 'r') as f:\n",
    "        portfolio_superpass = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    portfolio_original = [i for i in portfolio_original if i not in inv_tickers and i not in portfolio_superpass]\n",
    "    \n",
    "    \n",
    "    # Sector SPDR ETF: https://seekingalpha.com/etfs-and-funds/etf-tables/sectors   \n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df['Symbol'] = portfolio_original\n",
    "    df['Date'] = currentdate\n",
    "    df['Time'] = timechecknow()\n",
    "    df = pd.merge(df, sector, how = 'inner', on = 'Symbol')\n",
    "    df.loc[df['GICS Sector'] == 'Energy', 'Ticker'] =                'XLE'\n",
    "    df.loc[df['GICS Sector'] == 'Financials', 'Ticker'] =            'XLF'\n",
    "    df.loc[df['GICS Sector'] == 'Real Estate', 'Ticker'] =           'XLRE'\n",
    "    df.loc[df['GICS Sector'] == 'Materials', 'Ticker'] =             'XLB'\n",
    "    df.loc[df['GICS Sector'] == 'Information Technology', 'Ticker'] ='XLK'\n",
    "    df.loc[df['GICS Sector'] == 'Consumer Staples', 'Ticker'] =      'XLP'\n",
    "    df.loc[df['GICS Sector'] == 'Health Care', 'Ticker'] =           'XLV'\n",
    "    df.loc[df['GICS Sector'] == 'Industrials', 'Ticker'] =           'XLI'\n",
    "    df.loc[df['GICS Sector'] == 'Consumer Discretionary', 'Ticker'] ='XLY'\n",
    "    df.loc[df['GICS Sector'] == 'Utilities', 'Ticker'] =             'XLU'\n",
    "    df.loc[df['GICS Sector'] == 'Communication Services', 'Ticker'] ='XLC'\n",
    "    \n",
    "    df = df.sort_values('Ticker').reset_index(drop=True)\n",
    "    print('\\n\\n\\n', '\\033[1m' + f\">>> Sector Price Trend Valuation .....\" + '\\033[0m', '\\n')\n",
    "    valuation_mean = []\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(df.Ticker))):\n",
    "        analysis = analysis_price(df.Ticker[i])\n",
    "        try:\n",
    "            print('Sector valuating: ', df['GICS Sector'][i], end = '                                                               \\r')\n",
    "            if i == 0:\n",
    "                valuation_mean.append(analysis[7] or analysis[10] or analysis[11] or analysis[12])\n",
    "            elif df.Ticker[i] == df.Ticker[i-1]:\n",
    "                valuation_mean.append(0.3333333)\n",
    "            else:\n",
    "                valuation_mean.append(analysis[7] or analysis[10] or analysis[11] or analysis[12])\n",
    "        except:\n",
    "            valuation_mean.append(analysis[7] or analysis[10] or analysis[11] or analysis[12])\n",
    "            print('Re-excuted >>> Sector check - moving average: ', df.Ticker[i])\n",
    "            pass\n",
    "            \n",
    "    df['valuation_mean'] = valuation_mean        \n",
    "    for i in range(len(df['valuation_mean'])):\n",
    "        if df['valuation_mean'][i] == 0.3333333:\n",
    "            df['valuation_mean'][i] = df['valuation_mean'][i-1]  \n",
    "        else:\n",
    "            pass\n",
    "              \n",
    "    print('\\n\\n\\n', '\\033[1m' + f\">>> Sector vs S&P 500 - {averageline} days average price comparison .....\" + '\\033[0m', '\\n')\n",
    "    valuation_snp = []\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(df.Ticker))):\n",
    "        try:\n",
    "            print('Sector valuating: ', df['GICS Sector'][i], end = '                                                               \\r')\n",
    "            if i == 0:\n",
    "                valuation_snp.append(analysis_price('^GSPC')[5] < analysis_price(df.Ticker[i])[5])\n",
    "            elif df.Ticker[i] == df.Ticker[i-1]:\n",
    "                valuation_snp.append(0.3333333)\n",
    "            else:\n",
    "                valuation_snp.append(analysis_price('^GSPC')[5] < analysis_price(df.Ticker[i])[5]) \n",
    "        except:\n",
    "            valuation_snp.append(analysis_price('^GSPC')[5] < analysis_price(df.Ticker[i])[5])\n",
    "            print(f\">>> Re-excuted: Sector {averageline} vs {averageline-50} days S&P 500 change(%) comparison\")\n",
    "            pass\n",
    "            \n",
    "    df['valuation_snp'] = valuation_snp        \n",
    "    for i in range(len(df['valuation_snp'])):\n",
    "        if df['valuation_snp'][i] == 0.3333333:\n",
    "            df['valuation_snp'][i] = df['valuation_snp'][i-1]  \n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    vc = pd.read_csv('sector_valuation.csv')\n",
    "    for valuecheck_count in range(0, val_times):\n",
    "        time.sleep(1)\n",
    "        print('\\n\\n\\n', '\\033[1m' + f'>>> Sector R-squared valuation ..... {valuecheck_count+1}th' + '\\033[0m')        \n",
    "        valuation_rsquared = []\n",
    "        from tqdm.notebook import tqdm\n",
    "        for i in tqdm(range(len(df.Ticker))):\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                print('Sector valuating: ', df['GICS Sector'][i], end = '                                                               \\r')\n",
    "                filter_mask = (vc['Date'] == currentdate) & (vc['Ticker'] == df.Ticker[i])\n",
    "                if vc[filter_mask].Time.nunique() >= 3:\n",
    "                    valuation_rsquared.append(vc[filter_mask].groupby('Time').mean().reset_index().valuation_rsq.max())\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        valuation_rsquared.append(valuecheck(df.Ticker[i])[1])    \n",
    "                    elif df.Ticker[i] == df.Ticker[i-1]:\n",
    "                        valuation_rsquared.append(0.3333333)  #same ticker as above\n",
    "                    else:\n",
    "                        valuation_rsquared.append(valuecheck(df.Ticker[i])[1])\n",
    "            except: \n",
    "                valuation_rsquared.append(valuecheck(df.Ticker[i])[1])\n",
    "                print('Re-excuted >>> Sector check - R-squared: ', df.Ticker[i])\n",
    "                pass\n",
    "            \n",
    "        for i in valuation_rsquared:\n",
    "            if type(i) != float and type(i) != int:\n",
    "                i = 0.3333333\n",
    "            else:\n",
    "                pass\n",
    "        df['valuation_rsq'] = valuation_rsquared\n",
    "\n",
    "        for i in range(len(df['valuation_rsq'])):\n",
    "            try:\n",
    "                df = df.fillna(0)      \n",
    "                if df['valuation_rsq'][i] == 0.3333333:\n",
    "                    df['valuation_rsq'][i] = df['valuation_rsq'][i-1]  \n",
    "                else:\n",
    "                    pass\n",
    "            except KeyError:\n",
    "                sectorcheck_main(1)\n",
    "                pass\n",
    "            \n",
    "        df['Time'] = timechecknow()\n",
    "        df = df.sort_values(['valuation_mean', 'valuation_snp', 'valuation_rsq'], ascending = False)\n",
    "        df = df[['Date', 'Time', 'Symbol', 'GICS Sector', 'Ticker', 'valuation_mean', 'valuation_snp', 'valuation_rsq']]\n",
    "\n",
    "        df_accu = pd.read_csv('sector_valuation.csv')\n",
    "        df_merged = pd.concat([df_accu, df])\n",
    "        df_merged.to_csv('sector_valuation.csv', index = False)    \n",
    "    #     df.to_csv('sector_valuation.csv', index = False)  # for initial draft\n",
    "\n",
    "#         print(df.tail(df['Symbol'].nunique())[['Symbol', 'valuation_mean', 'valuation_snp', 'valuation_rsq']].reset_index(drop=True))\n",
    "    \n",
    "    portfolio_original.extend(inv_tickers)\n",
    "\n",
    "    \n",
    "def predict(target, ticker):    \n",
    "    target = None\n",
    "    \n",
    "    if ticker == 'GLD':\n",
    "        target = 'GLD'\n",
    "    elif ticker == 'USO':\n",
    "        target = 'USO'       \n",
    "    elif ticker == 'BTC-USD':\n",
    "        target = 'BTC-USD'\n",
    "    else:\n",
    "        target = '^GSPC'    \n",
    "\n",
    "    # target to get corr\n",
    "    target_index = yfinance_df(target)  \n",
    "    target_index = target_index[['Date', 'Adj Close']]\n",
    "    # ticker to get corr\n",
    "    ticker_index = yfinance_df(ticker)   \n",
    "    # corr\n",
    "    cor = pd.merge(target_index, ticker_index, how = 'inner', on = 'Date')\n",
    "\n",
    "    import statsmodels.api as sm\n",
    "    cor['intercept'] = 1\n",
    "    lm = sm.OLS(cor['Adj Close_y'], cor[['intercept', 'Adj Close_x']])\n",
    "    results = lm.fit()\n",
    "#     compensation_cor = results.rsquared * results.params[1]    \n",
    "#     compensation_cor = results.params[1] * np.absolute(cor.corr()['Adj Close_x'][1])\n",
    "    compensation_cor = np.absolute(cor.corr()['Adj Close_x'][1])\n",
    "    \n",
    "# check if exist for today\n",
    "    # if exists, only update the current rows\n",
    "    # remove past data\n",
    "    sourcedata_exists = exists(f\"predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv\")\n",
    "    if sourcedata_exists == True:\n",
    "        import os\n",
    "        os.remove(f\"predict_aim_sourcedata_monitoring_{ticker}_{mostrecentdate}.csv\")\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    sourcedata_exists = exists(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "    if target == '^GSPC' and sourcedata_exists == True:\n",
    "        print(f\"File Exists - Target: {target} & Ticker: {ticker} ..... Updating the current values\", end = '                       \\r')\n",
    "        df = pd.read_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "        realtime_vars = {'Int_rate':'^TNX', \n",
    "                         'Stock_price':ticker, \n",
    "                         'Cboe':'^VIX', \n",
    "                         'Gold':'GC=F', \n",
    "                         'Oil':'CL=F', \n",
    "                         'Bitcoin':'BTC-USD', \n",
    "                         'Dollar':'DX-Y.NYB'}\n",
    "        for i, v in realtime_vars.items():\n",
    "            df.loc[df.shape[0]-1, [i]] = yfinance_df_min(v)['Adj Close'].values[0]\n",
    "            \n",
    "    elif target == 'GLD' and sourcedata_exists == True:\n",
    "        print(f\"File Exists - Target: {target} & Ticker: {ticker} ..... Updating the current values\", end = '                       \\r')\n",
    "        df = pd.read_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "        realtime_vars = {'Int_rate':'^TNX', \n",
    "                         'Stock_price':ticker, \n",
    "                         'Cboe':'^VIX', \n",
    "                         'snp':'^GSPC', \n",
    "                         'Oil':'CL=F', \n",
    "                         'Bitcoin':'BTC-USD', \n",
    "                         'Dollar':'DX-Y.NYB'}\n",
    "        for i, v in realtime_vars.items():\n",
    "            df.loc[df.shape[0]-1, [i]] = yfinance_df_min(v)['Adj Close'].values[0]\n",
    "    \n",
    "    elif target == 'USO' and sourcedata_exists == True:\n",
    "        print(f\"File Exists - Target: {target} & Ticker: {ticker} ..... Updating the current values\", end = '                       \\r')\n",
    "        df = pd.read_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "        realtime_vars = {'Int_rate':'^TNX', \n",
    "                         'Stock_price':ticker, \n",
    "                         'Cboe':'^VIX', \n",
    "                         'snp':'^GSPC', \n",
    "                         'Oil':'CL=F', \n",
    "                         'Bitcoin':'BTC-USD', \n",
    "                         'Dollar':'DX-Y.NYB'}\n",
    "        for i, v in realtime_vars.items():\n",
    "            df.loc[df.shape[0]-1, [i]] = yfinance_df_min(v)['Adj Close'].values[0]\n",
    "            \n",
    "    elif target == 'BTC-USD' and sourcedata_exists == True:\n",
    "        print(f\"File Exists - Target: {target} & Ticker: {ticker} ..... Updating the current values\", end = '                       \\r')\n",
    "        df = pd.read_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\")\n",
    "        realtime_vars = {'Int_rate':'^TNX', \n",
    "                         'Stock_price':ticker, \n",
    "                         'Cboe':'^VIX', \n",
    "                         'snp':'^GSPC', \n",
    "                         'Oil':'CL=F', \n",
    "                         'Gold':'GC=F', \n",
    "                         'Dollar':'DX-Y.NYB'}\n",
    "        for i, v in realtime_vars.items():\n",
    "            df.loc[df.shape[0]-1, [i]] = yfinance_df_min(v)['Adj Close'].values[0]\n",
    "            \n",
    "    else:\n",
    "        print(f\"Initial downloading - Target: {target} & Ticker: {ticker}\", end = '                                                        \\r')\n",
    "    # if not exists, gather data from the beginning\n",
    "    # 10 year treasery bold yield\n",
    "        rate = yfinance_df('^TNX')  \n",
    "        irate = rate[['Date', 'Adj Close']]\n",
    "\n",
    "        # CBOE Volatility Index\n",
    "        cboe = yfinance_df('^VIX')\n",
    "        cboe = cboe[['Date', 'Adj Close']]\n",
    "\n",
    "        if target == '^GSPC':\n",
    "            # Gold Feb 22 (GC=F)\n",
    "            gold = yfinance_df('GC=F')  \n",
    "            gold = gold[['Date', 'Adj Close']]    \n",
    "\n",
    "        elif target == 'GLD' or target == 'USO' or target == 'BTC-USD':\n",
    "            # S&P 500\n",
    "            snp = yfinance_df('^GSPC')\n",
    "            snp = snp[['Date', 'Adj Close']]\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Crude Oil\n",
    "        mar = yfinance_df('CL=F')  \n",
    "        mar = mar[['Date', 'Adj Close']]\n",
    "\n",
    "        if target == '^GSPC' or target == 'GLD' or target == 'USO':\n",
    "            # Bitcoin (*last)\n",
    "            bit = yfinance_df('BTC-USD')   \n",
    "            bit = bit[['Date', 'Adj Close']]\n",
    "        elif target == 'BTC-USD':\n",
    "            # Gold\n",
    "            gold = yfinance_df('GC=F')   \n",
    "            gold = gold[['Date', 'Adj Close']]\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # Dollar Index\n",
    "        dollar = yfinance_df('DX-Y.NYB')\n",
    "        dollar = dollar[['Date', 'Adj Close']]\n",
    "\n",
    "        # cpi index\n",
    "        url = 'https://fred.stlouisfed.org/data/CPIAUCSL.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        cpi = webContent[4075:]\n",
    "        cpi = cpi.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(cpi)):\n",
    "            if i%2 == 0:\n",
    "                date.append(cpi[i])\n",
    "            else:\n",
    "                number.append(cpi[i])\n",
    "        cpidx = pd.DataFrame({'Date': date, 'Cpi': number})\n",
    "        cpidx['Date'] = cpidx['Date'].astype('datetime64[ns]')\n",
    "        cpidx['Cpi'] = pd.to_numeric(cpidx['Cpi'])\n",
    "\n",
    "        # jobless claim index\n",
    "        url = 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        joblessclaim = webContent[710:]\n",
    "        joblessclaim = joblessclaim.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(joblessclaim)):\n",
    "            if i%2 == 0:\n",
    "                date.append(joblessclaim[i])\n",
    "            else:\n",
    "                number.append(joblessclaim[i])\n",
    "        jls = pd.DataFrame({'Date': date, 'Jclaim': number})\n",
    "        \n",
    "        # fix error from 'https://fred.stlouisfed.org/data/ICSA.txt'\n",
    "        jls.Jclaim.replace('.', np.NaN, inplace = True)\n",
    "        jls.fillna(method = 'ffill', inplace = True)\n",
    "        jls = jls[jls['Date'] <= currentdate]\n",
    "        \n",
    "        jls['Date'] = jls['Date'].astype('datetime64[ns]')\n",
    "        jls['Jclaim'] = pd.to_numeric(jls['Jclaim'])\n",
    "        jls = jls[jls['Date'] > '1970-1-1'].reset_index(drop=True)\n",
    "        cpidx = cpidx[cpidx['Date'] > '1969-12-1'].reset_index(drop=True)\n",
    "\n",
    "        # inflation index\n",
    "        url = 'https://fred.stlouisfed.org/data/T10YIE.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        inf = webContent[1222:]\n",
    "        inf = inf.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(inf)):\n",
    "            if i%2 == 0:\n",
    "                date.append(inf[i])\n",
    "            else:\n",
    "                number.append(inf[i])\n",
    "        inf = pd.DataFrame({'Date': date, 'Inf': number})\n",
    "        inf.Inf.replace('.', np.NaN, inplace = True)\n",
    "        inf.fillna(method = 'ffill', inplace = True)\n",
    "        inf['Date'] = inf['Date'].astype('datetime64[ns]')\n",
    "        inf['Inf'] = pd.to_numeric(inf['Inf'])\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        # combine & cleaning\n",
    "        df = cpidx.merge(jls, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.merge(irate, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.merge(ticker_index, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close_x':'Int_rate', 'Adj Close_y':'Stock_price'}).reset_index(drop=True)\n",
    "        df = df.fillna(0)\n",
    "        df = df[df['Date'] >= '1970-01-01'].reset_index(drop=True) # since Jclaim\n",
    "\n",
    "        # cleaning - cpi and Jclaim \n",
    "        for i in range(len(df.Cpi)):\n",
    "            if df.Cpi[i] == 0:\n",
    "                df.Cpi[i] = df.Cpi[i-1]    \n",
    "        df = df[df['Date'] > '1970-01-02'].reset_index(drop=True)\n",
    "        for i in range(len(df.Jclaim)):\n",
    "            if df.Jclaim[i] == 0.0:\n",
    "                df.Jclaim[i] = df.Jclaim[i-1]\n",
    "#         drp = df[df['Int_rate'] == 0].index.values\n",
    "#         df = df.drop(index=drp)\n",
    "        df['Int_rate'] = df['Int_rate'].replace(to_replace=0, method='ffill')\n",
    "\n",
    "        # cleaning - inflation rate since 1990\n",
    "        df = df.merge(inf, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df[df['Date'] > '2003-01-01']\n",
    "        df = df.fillna(0)\n",
    "        if df.tail(1).Inf.values[0] == 0:\n",
    "            df.tail(1).Inf = df.tail(2).Inf.values[0]\n",
    "\n",
    "        # merge - cboe     \n",
    "        df = df.merge(cboe, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close':'Cboe'}).reset_index(drop=True)\n",
    "        df = df.fillna(0)\n",
    "\n",
    "        if target == '^GSPC':  \n",
    "            # merge - gold\n",
    "            df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "            df = df.fillna(0)\n",
    "\n",
    "        elif target == 'GLD' or target == 'USO' or target == 'BTC-USD':  \n",
    "            # merge - snp\n",
    "            df = df.merge(snp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'snp'}).reset_index(drop=True)\n",
    "            df = df.fillna(0)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # merge - Crude Oil Mar 22 \n",
    "        df = df.merge(mar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close':'Oil'}).reset_index(drop=True)\n",
    "\n",
    "        if target == 'BTC-USD':\n",
    "            # merge - gold\n",
    "            df = df.merge(gold, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'Gold'}).reset_index(drop=True)\n",
    "            df = df.fillna(0)\n",
    "        elif target == '^GSPC' or target == 'GLD' or target == 'USO':\n",
    "            # merge - bitcoin since 2014 (test rsquared for both w aad w/t this variable)\n",
    "            df = df.merge(bit, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "            df = df.rename(columns = {'Adj Close':'Bitcoin'}).reset_index(drop=True)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        # merge - Dollar Index \n",
    "        df = df.merge(dollar, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date')\n",
    "        df = df.rename(columns = {'Adj Close':'Dollar'}).reset_index(drop=True)\n",
    "\n",
    "        # dropna and save the source date\n",
    "        # fill ZERO value with the previous value\n",
    "        for i in df.all().reset_index().values:\n",
    "            if i[1] == False:\n",
    "                df[i[0]] = df[i[0]].replace(to_replace=0, method='ffill')\n",
    "        df = df.dropna()\n",
    "\n",
    "        # fillavg = df[df['Inf'] == 0].index.values\n",
    "        # df['Inf'].loc[fillavg] = df['Inf'].mean()   # Don't fill zero rows as average value\n",
    "\n",
    "        dfdrop = df[df['Stock_price'] == 0].index.values\n",
    "        df = df.drop(index = dfdrop)   \n",
    "\n",
    "        # dfdrop = df[df['Inf'] == 0].index.values\n",
    "        # df = df.drop(index = dfdrop)\n",
    "\n",
    "        fillavg = df[df['Inf'] == 0].index.values\n",
    "        df['Inf'].loc[fillavg] = df['Inf'].tail(5).max()             \n",
    "\n",
    "        # Fillna Cboe with the highest value for the past 20 working days. \n",
    "        cboe_fillna = df[df['Cboe'] == 0].index.values\n",
    "        df['Cboe'].loc[cboe_fillna] = df.tail(20)['Cboe'].max() \n",
    "        \n",
    "        # Real GDP\n",
    "        url = 'https://fred.stlouisfed.org/data/GDPC1.txt'\n",
    "        response = urllib.request.urlopen(url)\n",
    "        webContent = response.read().decode('UTF-8')\n",
    "        Rgdp = webContent[1005:]\n",
    "        Rgdp = Rgdp.split()\n",
    "        date = []\n",
    "        number = []\n",
    "        for i in range(len(Rgdp)):\n",
    "            if i%2 == 0:\n",
    "                date.append(Rgdp[i])\n",
    "            else:\n",
    "                number.append(Rgdp[i])\n",
    "        Rgdp = pd.DataFrame({'Date': date, 'Rgdp': number})\n",
    "        Rgdp['Date'] = Rgdp['Date'].astype('datetime64[ns]')\n",
    "        Rgdp['Rgdp'] = pd.to_numeric(Rgdp['Rgdp'])\n",
    "\n",
    "        df = df.merge(Rgdp, how = 'outer', left_on = 'Date', right_on = 'Date').sort_values('Date').reset_index(drop = True)\n",
    "        df.Rgdp.replace(np.NaN, 0, inplace = True)\n",
    "        for i in range(len(df.Rgdp)):\n",
    "            if df.Rgdp[i] == 0:\n",
    "                df.Rgdp[i] = df.Rgdp[i-1]\n",
    "        # fill ZERO value with the previous value\n",
    "        for i in df.all().reset_index().values:\n",
    "            if i[1] == False:\n",
    "                df[i[0]] = df[i[0]].replace(to_replace=0, method='ffill')\n",
    "        df.dropna(inplace = True)       \n",
    "        \n",
    "        if target == '^GSPC':\n",
    "            dfdrop = df[df['Gold'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "\n",
    "        elif target == 'BTC-USD':\n",
    "            dfdrop = df[df['Gold'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "            dfdrop = df[df['snp'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "\n",
    "        elif target == 'GLD' or target == 'USO':\n",
    "            dfdrop = df[df['snp'] == 0].index.values\n",
    "            df = df.drop(index=dfdrop)\n",
    "\n",
    "        else:\n",
    "            pass            \n",
    "\n",
    "        dfdrop = df[df['Int_rate'] == 0].index.values\n",
    "        df = df.drop(index=dfdrop)\n",
    "        df = df.drop(columns = ['Close'])\n",
    "        ### end of new gathering ####\n",
    "\n",
    "    # data verification: zero values\n",
    "    if df.all().sum() != len(df.columns):\n",
    "        print(f'\\nZero value checked - Filling with Mean() ===> Ticker: {ticker}\\n')\n",
    "\n",
    "        fill_zero_mean = df.all().reset_index().rename(columns = {0:'zero_var'})\n",
    "        zero_var_num = fill_zero_mean[fill_zero_mean['zero_var'] == False]['index'].count() \n",
    "\n",
    "        for i in range(zero_var_num):\n",
    "            zero_col = fill_zero_mean[fill_zero_mean['zero_var'] == False]['index'].values[i]\n",
    "            df[zero_col] = df[zero_col].replace(0, df[zero_col].mean()) # fill zero with average\n",
    "\n",
    "        if df.all().sum() == len(df.columns):\n",
    "            print('> Successful: Non-Zero values')\n",
    "            \n",
    "            last_date = str(df[-1:].Date.values[0])[:10]\n",
    "            if last_date == currentdate or last_date == mostrecentdate:\n",
    "                df.to_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\", index = False)\n",
    "            else:\n",
    "                df.to_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\", index = False)\n",
    "                print('\\033[1m'+\"\\n\\n\\nThe last line of the dateframe does not match with the currentdate.\\n\\nPrediction returns >>> Zero <<<\\n\\n\"+ f'Most recent date: {last_date}\\n\\n\\n'+'\\033[0m')\n",
    "            \n",
    "        else:\n",
    "            df.to_csv('errorlog_prediction_monitoring_%s.csv' %ticker, index = False)\n",
    "            print('> Unsuccessful - Check error log file: ', ticker, '\\n') \n",
    "\n",
    "    else:\n",
    "        last_date = str(df[-1:].Date.values[0])[:10]\n",
    "        if last_date == currentdate or last_date == mostrecentdate:\n",
    "            df.to_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\", index = False)\n",
    "        else:\n",
    "            df.to_csv(f\"predict_aim_sourcedata_monitoring_{ticker}_{currentdate}.csv\", index = False)\n",
    "            print('\\033[1m'+\"\\n\\n\\nThe last line of the dateframe does not match with the currentdate.\\n\\nPrediction returns >>> Zero <<<\\n\\n\"+ f'Most recent date: {last_date}\\n\\n\\n'+'\\033[0m')\n",
    "    ################################################################################### new merge - add above\n",
    "     \n",
    "    with open(\"rday_oil.txt\", 'r') as f:\n",
    "        rday_oil = [line.rstrip('\\n') for line in f][0]\n",
    "    with open(\"rday_gold.txt\", 'r') as f:\n",
    "        rday_gold = [line.rstrip('\\n') for line in f][0]\n",
    "    with open(\"rday_bit.txt\", 'r') as f:\n",
    "        rday_bit = [line.rstrip('\\n') for line in f][0]\n",
    "    with open(\"rday_snp.txt\", 'r') as f:\n",
    "        rday_snp = [line.rstrip('\\n') for line in f][0]\n",
    "\n",
    "    # range selection - COVID19\n",
    "    if target == '^GSPC':\n",
    "        dfc = df[df['Date'] > rday_snp]\n",
    "    elif target == 'GLD':\n",
    "        dfc = df[df['Date'] > rday_gold]\n",
    "    elif target == 'USO':\n",
    "        dfc = df[df['Date'] > rday_oil]\n",
    "    elif target == 'BTC-USD':\n",
    "        dfc = df[df['Date'] > rday_bit]\n",
    "    else:\n",
    "        pass\n",
    "    df_classifier = dfc.copy()\n",
    "\n",
    "    # Regression analysis\n",
    "    import statsmodels.api as sm\n",
    "    dfc['intercept'] = 1\n",
    "\n",
    "    if target == '^GSPC':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'Gold', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']]) \n",
    "    elif target == 'GLD':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'snp', 'Oil', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    elif target == 'BTC-USD':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Oil', 'Dollar', 'Gold', 'Rgdp']])\n",
    "    elif target == 'USO':\n",
    "        lm = sm.OLS(dfc['Stock_price'], dfc[['intercept', 'Cpi', 'Jclaim', 'Int_rate', 'Inf', 'Cboe', 'snp', 'Bitcoin', 'Dollar', 'Rgdp']])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    results = lm.fit()\n",
    "#     print(f'\\n\\n• Value Check: {ticker}\\n')\n",
    "#     print(results.summary())\n",
    "\n",
    "    # linear regression\n",
    "    if target == '^GSPC':\n",
    "        dfl = dfc.drop(columns = ['intercept'])\n",
    "    elif target == 'GLD':\n",
    "        dfl = dfc.drop(columns = ['intercept', 'Cboe'])      \n",
    "    elif target == 'USO':\n",
    "        dfl = dfc.drop(columns = ['intercept', 'Oil']) \n",
    "    elif target == 'BTC-USD':\n",
    "        dfl = dfc.drop(columns = ['intercept'])\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    prediction = pred_lstm(dfl) \n",
    "    err = (prediction - dfc.tail(1).Stock_price.values[0])/dfc.tail(1).Stock_price.values[0]\n",
    "    tprice = yfinance_df(ticker).tail(1)['Adj Close'].values[0]\n",
    "\n",
    "    # classifier\n",
    "    prediction_classifier = classifer_data_input(df_classifier, moving_avg_value) # [0]: precision / [1]: prediction result > pass df w/t r-day considered\n",
    "\n",
    "    # check the most recent date if yFinance includes it    \n",
    "    lastdate = str(df[-1:].Date.values[0])[:10]\n",
    "    if lastdate == currentdate or lastdate == mostrecentdate:\n",
    "        prediction_up_down = prediction_classifier[1]\n",
    "    else:\n",
    "        print(f'\\n\\nExcept >>> Last date error - Most recent date: {lastdate}\\n\\n')\n",
    "        prediction_up_down = 0    \n",
    "    \n",
    "    return [today_to_add, ticker, prediction, tprice, results.rsquared, err, compensation_cor, prediction_up_down]\n",
    "    \n",
    "    \n",
    "def valuecheck(stock):\n",
    "    print(end = '                                                                                    \\r')\n",
    "    print('\\n'+current_time()+'\\n')\n",
    "    print('\\033[1m' + \"Ticker Valuation\\n\" + '\\033[0m')\n",
    "    portfolio_sector_true = [stock]\n",
    "    \n",
    "    pricepredict = {}\n",
    "    date = []\n",
    "    symbol = []\n",
    "    prediction = []\n",
    "    price = []\n",
    "    rsquared = []\n",
    "    error = []\n",
    "    compensation = []\n",
    "    up_down = []\n",
    "    target = None\n",
    "\n",
    "    for i in portfolio_sector_true:\n",
    "        try:\n",
    "            result = predict(target, i)\n",
    "            date.append(result[0])\n",
    "            symbol.append(result[1])\n",
    "            prediction.append(result[2])\n",
    "            price.append(result[3])\n",
    "            rsquared.append(result[4])\n",
    "            error.append(result[5])\n",
    "            compensation.append(result[6])\n",
    "            up_down.append(result[7])            \n",
    "        except KeyError:                    \n",
    "            print('Except: Ticker valuation: ', portfolio_sector_true[i])\n",
    "            alarm()\n",
    "\n",
    "            # email\n",
    "            message = \"\"\"\\\n",
    "Subject: Ticker valuation error: {}\"\"\".format(portfolio_sector_true[i])                               \n",
    "            context = ssl.create_default_context()\n",
    "            with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "                server.login(sender_email, password)\n",
    "                server.sendmail(sender_email, receiver_email, message)                 \n",
    "\n",
    "            # re-try    \n",
    "            print('Ticker re-valuation: ', portfolio_sector_true[i])\n",
    "            result = predict(target, portfolio_sector_true[i])\n",
    "            date.append(result[0])\n",
    "            symbol.append(result[1])\n",
    "            prediction.append(result[2])\n",
    "            price.append(result[3])\n",
    "            rsquared.append(result[4])\n",
    "            error.append(result[5])\n",
    "            compensation.append(result[6])  \n",
    "            up_down.append(result[7])  \n",
    "            print('Re-valuation success: ', portfolio_sector_true[i])\n",
    "            alarm()\n",
    "\n",
    "            # email\n",
    "            message = \"\"\"\\\n",
    "Subject: Ticker re-valuation success: {}\"\"\".format(portfolio_sector_true[i])                               \n",
    "            context = ssl.create_default_context()\n",
    "            with smtplib.SMTP_SSL(smtp_server, port, context=context) as server:\n",
    "                server.login(sender_email, password)\n",
    "                server.sendmail(sender_email, receiver_email, message) \n",
    "            pass\n",
    "\n",
    "    pricepredict['date'] = date\n",
    "    pricepredict['time'] = timechecknow()\n",
    "    pricepredict['symbol'] = symbol\n",
    "    pricepredict['prediction'] = prediction\n",
    "    pricepredict['price'] = price\n",
    "    pricepredict['rsquared'] = rsquared\n",
    "    pricepredict['error'] = np.absolute(error)\n",
    "    pricepredict['compensation_corr'] = compensation\n",
    "    pricepredict['up_down'] = up_down\n",
    "\n",
    "    # to send today_predict\n",
    "    df = pd.DataFrame(pricepredict)  \n",
    "    df = df.fillna(0)\n",
    "\n",
    "    # apply accmulated gap to compensate 'gap'\n",
    "    dfacc = pd.read_csv('predict_valuation.csv').sort_values('symbol')\n",
    "    dfacc = dfacc.dropna()\n",
    "\n",
    "    average_gap = []\n",
    "    for i in df.symbol:\n",
    "        if i in pd.DataFrame(dfacc.groupby('symbol').count()).reset_index().symbol.values:\n",
    "            average_gap.append(dfacc[dfacc['symbol'] == i].gap.min())\n",
    "        else:\n",
    "            average_gap.append(np.absolute(df[df['symbol'] == i].prediction.values[0] - df[df['symbol'] == i].price.values[0]))\n",
    "\n",
    "    df['gap'] = average_gap\n",
    "\n",
    "    # compensate - original predicted price is compensated by multipying coefficient correlation\n",
    "    #              this also partially compensates rsquared per ticker since corr*corr becomes rsquared\n",
    "    # again, df['gap'] here is calculated from accmulated data, 'predict', and save the result to 'predict_valuation_today'\n",
    "    \n",
    "# Compensation    \n",
    "    for i in range(len(df['symbol'])):\n",
    "        if df['symbol'][i] == 'GLD' or df['symbol'][i] == 'USO' or df['symbol'][i] == 'BTC-USD':\n",
    "            pass\n",
    "        else:\n",
    "            if df['prediction'][i] < df['price'][i]:\n",
    "                df['prediction'][i] = df['prediction'][i]# + (df['gap'][i] * df['compensation_corr'][i])\n",
    "            else:\n",
    "                df['prediction'][i] = df['prediction'][i]# - (df['gap'][i] * df['compensation_corr'][i])\n",
    "# Compensation  \n",
    "\n",
    "    # final valuation, and save the result to 'predict_valuation_today'\n",
    "    df.loc[df['prediction'] - df['price'] > 0, 'valuation'] = 1\n",
    "    df.loc[df['prediction'] - df['price'] <= 0, 'valuation'] = 0\n",
    "    df['gap'] = np.absolute(df['prediction'] - df['price'])\n",
    "    df['error'] = np.absolute(df['prediction'] - df['price']) / df['price']\n",
    "\n",
    "    today = df.sort_values('error').reset_index(drop=True)\n",
    "    today.to_csv('predict_valuation_today.csv', index = False)\n",
    "\n",
    "    # update 'predict_valuation_today' into 'predict'\n",
    "    df = pd.read_csv('predict_valuation.csv')\n",
    "    df = pd.concat([df, today])\n",
    "    df.to_csv('predict_valuation.csv', index = False)\n",
    "    df.to_csv('./Backup_predict_valuation/predict_valuation %s.csv' %currentdate, index = False)\n",
    "    \n",
    "    df = pd.read_csv('predict_valuation.csv').sort_values('symbol')\n",
    "    predict_valuation_today = pd.read_csv('predict_valuation_today.csv').sort_values(['up_down', 'valuation', 'error'], ascending = False)\n",
    "#     print(predict_valuation_today[['symbol', 'valuation', 'error']].sort_values(['valuation', 'error'], ascending = False))\n",
    "\n",
    "    # average error check if zero\n",
    "    if predict_valuation_today.price.min() == 0:\n",
    "        print('!!! Null error !!!')\n",
    "        alarm()\n",
    "\n",
    "        # email\n",
    "        message = \"\"\"\\\n",
    "Subject: Average error zero. Check file. today predict!\"\"\"\n",
    "        emailsend_to_server(message)\n",
    "        \n",
    "        drp = predict_valuation_today[predict_valuation_today.price ==0].index\n",
    "        predict_valuation_today.drop(index = drp, inplace = True)   \n",
    "    else: \n",
    "        pass\n",
    "    \n",
    "    df = pd.read_csv('predict_valuation.csv')\n",
    "    df = df[df.date == currentdate]\n",
    "    \n",
    "    return [df[df['symbol'] == stock].tail(3)['valuation'].mean(), df[df['symbol'] == stock].tail(3)['up_down'].mean()] \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The Engine#1 gets ready.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d0e46",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Engine #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8645ffbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def robinExecution_sell(i, q):\n",
    "    global sellmonitor_freq\n",
    "    global num_of_purchased\n",
    "    global day_trading_count\n",
    "    \n",
    "    # only execute during the market hours\n",
    "    if timechecknow() <= 390:\n",
    "        try:\n",
    "            # execue sell order\n",
    "            login = rs.login('aicpasean@gmail.com', sean.robin_api())\n",
    "            order_sell_market(i, q)\n",
    "            rs.logout()\n",
    "            print('\\n'+current_time()+' - The selling order has been executed.\\n')\n",
    "            time.sleep(10)\n",
    "\n",
    "            # sold list update\n",
    "            trade_list('sold', i)\n",
    "            # purchased list update\n",
    "            trade_list('purchased', i)            \n",
    "            print('\\n'+current_time()+' - Trade_List has been updated.\\n')\n",
    "\n",
    "            # update performance\n",
    "            sss_perf(i)\n",
    "            print('\\n'+current_time()+' - SSS_Perf has been updated - sss_perf.csv\\n')\n",
    "            \n",
    "            # count the existing number of stocks for holding period count\n",
    "            onhold_df = robin_login()\n",
    "            if i in onhold_df['index'].tolist():\n",
    "                num_of_stocks_hold = int(float(onhold_df[onhold_df['index'] == i]['quantity'].values[0]))\n",
    "                print(f'• Stock: {i} / {num_of_stocks_hold} shares on hold\\n\\n\\n')\n",
    "            else:\n",
    "                num_of_stocks_hold = 0\n",
    "                print(f'• Stock: {i} / {num_of_stocks_hold} shares on hold\\n\\n\\n')\n",
    "\n",
    "                # remove tickers having no balance from the holding period DataFrame\n",
    "                holding_period = pd.read_csv('holding_period.csv').set_index('index')\n",
    "                if i in holding_period.index.tolist():\n",
    "                    print('\\n\\n\\nTicker exists in the holding period list: ' + '\\033[1m' + f'{i}' + '\\033[0m')\n",
    "                    holding_period = holding_period.drop(index = i)\n",
    "                    holding_period.to_csv('holding_period.csv')\n",
    "                    print('Ticker has been removed from the list: ' + '\\033[1m' + f'{i}\\n\\n\\n' + '\\033[0m')\n",
    "                    print('• Updated Hoding Period\\n\\n' + '\\033[1m' + f'{holding_period}\\n\\n\\n' + '\\033[0m')\n",
    "                else:\n",
    "                    print('\\n\\n\\nTicker does NOT exist in the holding period list: ' + '\\033[1m' + f'{i}\\n\\n\\n' + '\\033[0m')\n",
    "                    \n",
    "        except:\n",
    "            message = 'The selling transaction had one or more than one errors.'\n",
    "            print('Except >>> ' + message)\n",
    "            alarm()\n",
    "            voice_message(message)\n",
    "    else:\n",
    "        message = 'The market has been closed. The sell order has not been executed.'\n",
    "        print('Except >>> ' + message)\n",
    "        alarm()\n",
    "        voice_message(message)\n",
    "  \n",
    "    # sell monitor cycle update\n",
    "    num_of_purchased = len(onhold_df['index'])\n",
    "    sellmonitor_freq = int((sellmonitor_freq_limit) - (num_of_purchased - len(dead_stock))) ### adjust \n",
    "    print('\\n\\nSell Monitor Freq has been updated as every' + '\\033[1m' + f' {sellmonitor_freq} ' + '\\033[0m' + 'cycles.\\n\\n')\n",
    "    \n",
    "    \n",
    "def sss_perf(i):\n",
    "    global day_trading_count\n",
    "    \n",
    "    df = pd.read_csv('robin_all.csv')\n",
    "    percent_change = df[df['index'] == i].percent_change.values[0]\n",
    "    equity_change = df[df['index'] == i].equity_change.values[0]\n",
    "    holding_period = pd.read_csv('holding_period.csv')\n",
    "    try:\n",
    "        holding_period = int(holding_period[holding_period['index'] == i].days.values[0])\n",
    "    except:\n",
    "        day_trading_count+=1\n",
    "        print('\\033[1m' + f'\\n\\n\\n{i} was purchased today. Updated Day-trading count: {day_trading_count}\\n\\n\\n' + '\\033[0m')\n",
    "        holding_period = 0\n",
    "\n",
    "    sss_perf = {'date' : date.today(),\n",
    "                'symbol' : i,\n",
    "                'percent_change': percent_change, \n",
    "                'equity_change' : equity_change, \n",
    "                'holding_period' : holding_period}\n",
    "    sss_perf = pd.DataFrame([sss_perf])\n",
    "\n",
    "    ss_perf = pd.read_csv('sss_perf.csv')\n",
    "\n",
    "    sss_perf = pd.concat([ss_perf, sss_perf], axis = 0)\n",
    "    sss_perf['date'] = sss_perf['date'].astype('datetime64[ns]')\n",
    "    sss_perf.sort_values('date', ascending = False, inplace = True)\n",
    "    sss_perf.to_csv('sss_perf.csv', index = False)\n",
    "\n",
    "    real_performance_selma()\n",
    "    \n",
    "    \n",
    "def split_qty_finder(i, q):\n",
    "    global split_condition_dollar\n",
    "    split_condition_dollar_original = split_condition_dollar\n",
    "    \n",
    "    current_price = analysis_price(i)[4]\n",
    "    current_value = current_price * q\n",
    "    \n",
    "    # reset split_condition_dollar at 1/3 of dollar_limit_order_dollar\n",
    "    if (dollar_limit_order_dollar/3) > split_condition_dollar:\n",
    "        split_condition_dollar = (dollar_limit_order_dollar/3)\n",
    "        print('\\033[1m' + f'\\n\\n\\nAdjusted Split Condition Dollar at ${split_condition_dollar}\\n\\n\\n' + '\\033[0m')\n",
    "    else:\n",
    "        print('\\033[1m' + f'\\n\\n\\nSplit Condition Dollar at ${split_condition_dollar}\\n\\n\\n' + '\\033[0m')\n",
    "    \n",
    "    # get the number of tickers to split-sell \n",
    "    if current_value > split_condition_dollar:\n",
    "        q = math.ceil((current_value/sale_split)/current_price)\n",
    "        \n",
    "    split_condition_dollar = split_condition_dollar_original\n",
    "    return q   \n",
    "\n",
    "\n",
    "def stg_loss_rate(ticker):\n",
    "    global min_SuccessRate\n",
    "    min_SuccessRate_original = min_SuccessRate\n",
    "    min_SuccessRate = int(min_SuccessRate * 0.8)\n",
    "    print('\\033[1m' + f'\\n==========[ Loss Cut Rate Searching by Strategy - Ticker: {ticker} ]==========' + '\\033[0m')\n",
    "    loss_rate = []\n",
    "    for i in range(5):    \n",
    "        stg_loss = strategy_analysis_main(ticker)\n",
    "        if stg_loss[2] != 0:\n",
    "            loss_rate.append(stg_loss[2])\n",
    "        min_SuccessRate = int(min_SuccessRate * 1.1)\n",
    "        if stg_loss[0] == -10:\n",
    "            break\n",
    "    if len(loss_rate) != 0:\n",
    "        stg_loss_rate = max(loss_rate)\n",
    "    else:\n",
    "        stg_loss_rate = -5\n",
    "    min_SuccessRate = min_SuccessRate_original\n",
    "    return stg_loss_rate *100\n",
    "    \n",
    "    \n",
    "def sellmonitor():\n",
    "    with open(f'sold_{currentdate}.txt', 'r') as f:\n",
    "        sold = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    with open(f'purchased_{currentdate}.txt', 'r') as f:\n",
    "        purchased = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    # import sector valuation record\n",
    "    sector_valuation_ticker = pd.read_csv('sector_valuation.csv')\n",
    "    robin = robin_equity_check()\n",
    "    robin = robin.loc[-robin['index'].isin(sold)] # exclude [Sold]\n",
    "    robin.reset_index(drop = True, inplace = True)\n",
    "    current_gl = pd.read_csv('robin_all.csv')\n",
    "    \n",
    "# Sell monitoring start\n",
    "    # Sell threshold - all portfolio  \n",
    "    print('\\n\\n' + '\\033[1m' + '>>> 52-week high monitoring for all tickers .....' + '\\033[0m', '\\n')\n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(robin['index'].values))):     \n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            time.sleep(0.3)\n",
    "            print('52-weeks high checking: ', robin['index'][i], end = '                    \\r')\n",
    "            high_52_price = yfinance_df(robin['index'].values[i]).tail(260)['Adj Close'].max()\n",
    "            current_price = yfinance_df(robin['index'].values[i]).tail(1)['Adj Close'].max()\n",
    "            \n",
    "            if current_price > high_52_price:\n",
    "                # include into purchased list\n",
    "                # purchased list update\n",
    "                print('\\n'+current_time()+'\\n')\n",
    "                purchased = trade_list('purchased', robin['index'].values[i])\n",
    "                \n",
    "#             # open the lost-cut list and add the ticker into the list\n",
    "#                 with open(\"tickers_to_sell_small.txt\", 'r') as f:\n",
    "#                     tickers_to_sell_small = [line.rstrip('\\n') for line in f]                       \n",
    "#                 tickers_to_sell_small.append(robin['index'].values[i])\n",
    "#             # open the long-term list and exclude the lost-cut list from the long-term list  \n",
    "#                 with open(\"tickers_to_sell_big.txt\", 'r') as f:\n",
    "#                     tickers_to_sell_big = [line.rstrip('\\n') for line in f]\n",
    "#                 tickers_to_sell_small = [i for i in tickers_to_sell_small if i not in tickers_to_sell_big]\n",
    "#             # save the updated loss-cut list        \n",
    "#                 with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "#                     for s in tickers_to_sell_small:  \n",
    "#                         f.write(str(s) + '\\n') \n",
    "                \n",
    "                # email only once if already included the purchased list\n",
    "                # remove the following three lines if including the ticker into the loss-cut list\n",
    "                with open(f'purchased_{currentdate}.txt', 'r') as f:\n",
    "                    purchased = [line.rstrip('\\n') for line in f]\n",
    "                if robin['index'].values[i] in purchased == False:    \n",
    "                    # alarm\n",
    "                    webbrowser.open(\"Nuclear Launch Detected.mp3\")\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Alarm: ${} <= 52-week high! The ticker has been included into the purchased list.\"\"\".format(robin['index'].values[i], '%.2f' %high_52_price)\n",
    "                    print(message)\n",
    "                    voice_message(message)\n",
    "                    emailsend_to_server(message)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> 52-week high: ', robin['index'].values[i])\n",
    "            alarm()\n",
    "            pass\n",
    "            \n",
    "    print('\\n')      \n",
    "    \n",
    "# Volume-speed Sell Alert\n",
    "    print('\\n' + '\\033[1m' + '>>> Trend and Volume Drop monitoring for tickers (excl. Big) .....' + '\\033[0m', '\\n')\n",
    "  \n",
    "    from tqdm.notebook import tqdm\n",
    "    tickers = robin['index'].tolist()\n",
    "    # open the long-term list and exclude the lost-cut list from the long-term list  \n",
    "    with open(\"tickers_to_sell_big.txt\", 'r') as f:\n",
    "        tickers_to_sell_big = [line.rstrip('\\n') for line in f]\n",
    "    tickers = [i for i in tickers if i not in tickers_to_sell_big]\n",
    "    \n",
    "    for i in tqdm(range(len(tickers))):    \n",
    "    \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            time.sleep(0.3)\n",
    "            print('Candle Analysing Ticker: ', tickers[i], end = '                    \\r')\n",
    "            analysis = analysis_price(tickers[i])\n",
    "            \n",
    "            if analysis_price('^GSPC')[5] > analysis[5] and analysis[8] == True and analysis[6] == False:\n",
    "                candle_result = volume_vs_sign(tickers[i])\n",
    "                \n",
    "                if candle_result == -3:\n",
    "                # open the lost-cut list and add the ticker into the list\n",
    "                    with open(\"tickers_to_sell_small.txt\", 'r') as f:\n",
    "                        tickers_to_sell_small = [line.rstrip('\\n') for line in f]                       \n",
    "                    tickers_to_sell_small.append(tickers[i])\n",
    "                    tickers_to_sell_small = [i for i in tickers_to_sell_small if i not in tickers_to_sell_big]\n",
    "                # save the updated loss-cut list        \n",
    "                    with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_small:  \n",
    "                            f.write(str(s) + '\\n') \n",
    "                    \n",
    "                    # alarm\n",
    "                    webbrowser.open(\"Nuclear Launch Detected.mp3\")\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sell Alert: Candle Analysis Sell. The ticker has been included into the loss-cut list.\"\"\".format(tickers[i])\n",
    "                    print(message)\n",
    "                    voice_message(message)\n",
    "                    emailsend_to_server(message)\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Volume Drop Sell: ', tickers[i])\n",
    "            alarm()\n",
    "            pass\n",
    "            \n",
    "    print('\\n')    \n",
    "    \n",
    "# Strategy analysis if having 'Sell' sign\n",
    "    print('\\n' + '\\033[1m' + '>>> Strategy Analysis for all tickers (excl. Big) .....' + '\\033[0m', '\\n')\n",
    "  \n",
    "    from tqdm.notebook import tqdm\n",
    "    for i in tqdm(range(len(tickers))):  \n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            time.sleep(0.3)\n",
    "            print('Strategy Analysis Ticker: ', tickers[i], end = '                    \\r')\n",
    "            if strategy_analysis_main(tickers[i], 'off', 'off')[0] + strategy_analysis_main(tickers[i], 'off', 'off')[0] == -2:\n",
    "  \n",
    "            # open the lost-cut list and add the ticker into the list\n",
    "                with open(\"tickers_to_sell_small.txt\", 'r') as f:\n",
    "                    tickers_to_sell_small = [line.rstrip('\\n') for line in f]                       \n",
    "                tickers_to_sell_small.append(tickers[i])\n",
    "                tickers_to_sell_small = [i for i in tickers_to_sell_small if i not in tickers_to_sell_big]\n",
    "            # save the updated loss-cut list        \n",
    "                with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                    for s in tickers_to_sell_small:  \n",
    "                        f.write(str(s) + '\\n')      \n",
    "                # alarm\n",
    "                webbrowser.open(\"Nuclear Launch Detected.mp3\")\n",
    "                # email\n",
    "                message = \"\"\"\\\n",
    "Subject: QED [{}] Sell Alert: Strategy Analysis Sell. The ticker has been included into the loss-cut list.\"\"\".format(tickers[i])\n",
    "                print(message)\n",
    "                voice_message(message)\n",
    "                emailsend_to_server(message)\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Strategy Analysis Sell: ', tickers[i])\n",
    "            alarm()\n",
    "            pass\n",
    "        \n",
    "    print('\\n')  \n",
    "\n",
    "    \n",
    "# Sell threshold - Selected portfolios to realize gain ####################################################################################\n",
    "# Sell_big list update   \n",
    "    with open(\"tickers_to_sell_big.txt\", 'r') as f:\n",
    "        tickers_to_sell_big = [line.rstrip('\\n') for line in f]   \n",
    "    with open(\"realgainrate_big.txt\", 'r') as f:\n",
    "        realgainrate_big = [line.rstrip('\\n') for line in f]    \n",
    "        realgainrate_big = np.round(float(realgainrate_big[0]), 2)\n",
    "\n",
    "    dfb = robin.loc[robin['index'].isin(tickers_to_sell_big)]\n",
    "    print('\\n\\n\\n\\n' + '\\033[1m' + f\">>> {realgainrate_big}\" + '\\033[0m' + '% Max. Sell Monitoring for Long-Term Sell: ' + '\\033[1m' + format(dfb['index'].values) + '\\033[0m', '\\n')\n",
    "\n",
    "    for i, c, q in zip(dfb['index'], dfb['percent_change'], dfb['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        print('Analysing Ticker: ', i, end = '                    \\r')\n",
    "        try:\n",
    "            realgainrate_big_adj = realgainrate_big*(1-(volatility_check(i) - volatility_check(sector_valuation_ticker[sector_valuation_ticker['Symbol'] == i]['Ticker'].values[0])))\n",
    "         \n",
    "            # apply event day risk ratio\n",
    "            if i in ticker_eventday_check():\n",
    "                realgainrate_big_adj*= event_risk_ratio    \n",
    "                print('\\n\\n' + '\\033[1m' + f' {i} ' + '\\033[0m' + 'Event Risk Raio' + '\\033[1m' + f' {event_risk_ratio*100} % ' + '\\033[0m' + 'applied.')\n",
    "\n",
    "            # safe the margin error\n",
    "            if realgainrate_big_adj < 0:\n",
    "                realgainrate_big_adj = 0.01\n",
    "        \n",
    "            print('\\n\\n' + '\\033[1m' + f\"• {i}: \" + '\\033[0m' + '\\033[1m' + f\"{'%.2f' %realgainrate_big_adj}% <--- {'%.2f' %current_gl[current_gl['index'] == i]['percent_change'].values[0]}%\" + '\\033[0m', '\\n')\n",
    "\n",
    "            if c >= realgainrate_big_adj:\n",
    "                analysis_snp = analysis_price('^GSPC')\n",
    "                analysis_sector = analysis_price(sector_valuation_ticker[sector_valuation_ticker['Symbol'] == i]['Ticker'].values[0])\n",
    "                analysis_ticker = analysis_price(i)  \n",
    "                strategy_analysis = strategy_analysis_main(i, 'off', 'off')[0] + strategy_analysis_main(i, 'off', 'off')[0]\n",
    "            \n",
    "                valuecheck(i)\n",
    "                if strategy_analysis == -2 or analysis_snp[5] > analysis_sector[5] or (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_sell:\n",
    "\n",
    "                    print(f\"\"\"\\\n",
    "                    \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                    \"\"\")\n",
    "                    voice_message(f\"\"\"\\\n",
    "                    Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                    \"\"\")\n",
    "                    time.sleep(10)\n",
    "                    robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>\n",
    "\n",
    "                    tickers_to_sell_big.remove(i)\n",
    "                    with open(\"tickers_to_sell_big.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_big:  \n",
    "                            f.write(str(s) + '\\n')    \n",
    "\n",
    "                    # alarm\n",
    "                    webbrowser.open(\"soldsound.mp3\")\n",
    "\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% realized\"\"\".format(i, '%.2f' %c)\n",
    "                    emailsend_to_server(message)\n",
    "                    \n",
    "                else:\n",
    "                    print(f'\\n\\n$$$$$ Realize gain test all passed - Big Ticker: {i} $$$$$\\n\\n')\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Sell Monitoring for Long-Term Sell: ', i)\n",
    "            alarm()\n",
    "            pass\n",
    "\n",
    "        \n",
    "# Sell_mid list update    \n",
    "    with open(\"tickers_to_sell_mid.txt\", 'r') as f:\n",
    "        tickers_to_sell_mid = [line.rstrip('\\n') for line in f] \n",
    "    with open(\"realgainrate_mid.txt\", 'r') as f:\n",
    "        realgainrate_mid = [line.rstrip('\\n') for line in f]    \n",
    "        realgainrate_mid = np.round(float(realgainrate_mid[0]), 2)     \n",
    "            \n",
    "    dfm = robin.loc[robin['index'].isin(tickers_to_sell_mid)]\n",
    "    print('\\n\\n\\n\\n' + '\\033[1m' + f\">>> {realgainrate_mid}\" + '\\033[0m' + '% Max. Sell Monitoring for Mid-Term Sell: ' + '\\033[1m' + format(dfm['index'].values) + '\\033[0m', '\\n')\n",
    " \n",
    "    for i, c, q in zip(dfm['index'], dfm['percent_change'], dfm['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "        \n",
    "        print('Analysing Ticker: ', i, end = '                    \\r')\n",
    "        try:\n",
    "            \n",
    "            ## calculated an adjusted mid-gain% \n",
    "            # holding period premium\n",
    "            holding_period['days_left_percent'] = ((min_holding_period/2)-holding_period['days']) / (min_holding_period/2) # set the date in the middle\n",
    "            if i in holding_period['index'].tolist():\n",
    "                holding_period_premium_discount = holding_period[holding_period['index'] == i]['days_left_percent'].values[0]\n",
    "            else:\n",
    "                holding_period_premium_discount = 0\n",
    "            \n",
    "            # net volatility\n",
    "            if holding_period_premium_discount < 0:\n",
    "                realgainrate_mid_adj = holding_period_premium_discount*2 + realgainrate_mid*(1-(volatility_check(i) - volatility_check(sector_valuation_ticker[sector_valuation_ticker['Symbol'] == i]['Ticker'].values[0])))\n",
    "            else:\n",
    "                realgainrate_mid_adj = holding_period_premium_discount*0.1 + realgainrate_mid*(1-(volatility_check(i) - volatility_check(sector_valuation_ticker[sector_valuation_ticker['Symbol'] == i]['Ticker'].values[0])))\n",
    " \n",
    "            # apply event day risk ratio\n",
    "            if i in ticker_eventday_check():\n",
    "                realgainrate_mid_adj*= event_risk_ratio    \n",
    "                print('\\n\\n' + '\\033[1m' + f' {i} ' + '\\033[0m' + 'Event Risk Raio' + '\\033[1m' + f' {event_risk_ratio*100} % ' + '\\033[0m' + 'applied.\\n\\n')\n",
    "        \n",
    "            # safe the margin error\n",
    "            if realgainrate_mid_adj < 0:\n",
    "                realgainrate_mid_adj = 0.01\n",
    "\n",
    "            print('\\n\\n' + '\\033[1m' + f\"• {i}: \" + '\\033[0m' + '\\033[1m' + f\"{'%.2f' %realgainrate_mid_adj}% <--- {'%.2f' %current_gl[current_gl['index'] == i]['percent_change'].values[0]}%\" + '\\033[0m', '\\n')\n",
    "       \n",
    "            if c >= realgainrate_mid_adj:   \n",
    "                analysis_snp = analysis_price('^GSPC')\n",
    "                analysis_sector = analysis_price(sector_valuation_ticker[sector_valuation_ticker['Symbol'] == i]['Ticker'].values[0])\n",
    "                analysis_ticker = analysis_price(i)\n",
    "                strategy_analysis = strategy_analysis_main(i, 'off', 'off')[0] + strategy_analysis_main(i, 'off', 'off')[0]\n",
    "                \n",
    "                valuecheck(i)\n",
    "                if strategy_analysis == -2 or analysis_snp[5] > analysis_sector[5] or (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_sell:\n",
    "                    \n",
    "                    print(f\"\"\"\\\n",
    "                    \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                    \"\"\")\n",
    "                    voice_message(f\"\"\"\\\n",
    "                    Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                    \"\"\")\n",
    "                    time.sleep(10)\n",
    "                    robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>\n",
    "                    \n",
    "                    tickers_to_sell_mid.remove(i)\n",
    "                    with open(\"tickers_to_sell_mid.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_mid:  \n",
    "                            f.write(str(s) + '\\n')\n",
    "\n",
    "                    # alarm\n",
    "                    webbrowser.open(\"soldsound.mp3\")\n",
    "\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% realized\"\"\".format(i, '%.2f' %c)\n",
    "                    emailsend_to_server(message)\n",
    "                    \n",
    "                else:\n",
    "                    print(f'\\n\\n$$$$$ Realize gain test all passed - Mid Ticker: {i} $$$$$\\n\\n')           \n",
    "        except:\n",
    "            print('Except >>> Sell Monitoring for Mid-Term Sell: ', i)\n",
    "            alarm()\n",
    "            pass\n",
    "\n",
    "        \n",
    "# Sell_inv list update    \n",
    "    with open(\"tickers_to_sell_inv.txt\", 'r') as f:\n",
    "        tickers_to_sell_inv = [line.rstrip('\\n') for line in f] \n",
    "    with open(\"realgainrate_inv.txt\", 'r') as f:\n",
    "        realgainrate_inv = [line.rstrip('\\n') for line in f]    \n",
    "        realgainrate_inv = np.round(float(realgainrate_inv[0]), 2)\n",
    "\n",
    "    dfi = robin.loc[robin['index'].isin(tickers_to_sell_inv)]\n",
    "    print('\\n\\n\\n\\n' + '\\033[1m' + f\">>> {realgainrate_inv}\" + '\\033[0m' + '% Sell Monitoring for Inverse Tickers: ' + '\\033[1m' + format(dfi['index'].values) + '\\033[0m', '\\n')\n",
    "\n",
    "    for i, c, q in zip(dfi['index'], dfi['percent_change'], dfi['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        print('Analysing Ticker: ', i, end = '                    \\r')\n",
    "        try:\n",
    "            realgainrate_inv_adj = realgainrate_inv + realgainrate_inv*(volatility_check('^GSPC') - volatility_check(i))\n",
    "            print('\\n\\n' + '\\033[1m' + f\"• {i}: \" + '\\033[0m' + '\\033[1m' + f\"{'%.2f' %realgainrate_inv_adj}% <--- {'%.2f' %current_gl[current_gl['index'] == i]['percent_change'].values[0]}%\" + '\\033[0m', '\\n')\n",
    "\n",
    "            if c >= realgainrate_inv_adj:\n",
    "                analysis = analysis_price(i)\n",
    "                strategy_analysis = strategy_analysis_main(i, 'off', 'off')[0] + strategy_analysis_main(i, 'off', 'off')[0]\n",
    "\n",
    "                valuecheck(i)\n",
    "                if strategy_analysis == -2 or (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_sell or analysis[3] > analysis[4]:\n",
    "\n",
    "                    print(f\"\"\"\\\n",
    "                    \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                    \"\"\")\n",
    "                    voice_message(f\"\"\"\\\n",
    "                    Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                    \"\"\")\n",
    "                    time.sleep(10)\n",
    "                    robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>> \n",
    "\n",
    "                    tickers_to_sell_inv.remove(i)\n",
    "                    with open(\"tickers_to_sell_inv.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_inv:  \n",
    "                            f.write(str(s) + '\\n')    \n",
    "\n",
    "                    # alarm\n",
    "                    webbrowser.open(\"soldsound.mp3\")\n",
    "\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% realized, Inverse list has been updated.\"\"\".format(i, '%.2f' %c)\n",
    "                    emailsend_to_server(message)\n",
    "                    \n",
    "                else:\n",
    "                    print(f'\\n\\n$$$$$ Realize gain test all passed - INV Ticker: {i} $$$$$\\n\\n')\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Sell Monitoring for tickers_to_sell_inv: \\n', i)   \n",
    "            alarm()\n",
    "            pass\n",
    "        \n",
    "        \n",
    "# Sell_ETF list update    \n",
    "    with open(\"tickers_to_sell_etf.txt\", 'r') as f:\n",
    "        tickers_to_sell_etf = [line.rstrip('\\n') for line in f] \n",
    "    with open(\"realgainrate_etf.txt\", 'r') as f:\n",
    "        realgainrate_etf = [line.rstrip('\\n') for line in f]    \n",
    "        realgainrate_etf = np.round(float(realgainrate_etf[0]), 2)  \n",
    "\n",
    "    dfe = robin.loc[robin['index'].isin(tickers_to_sell_etf)]\n",
    "    print('\\n\\n\\n\\n' + '\\033[1m' + f\">>> {realgainrate_etf}\" + '\\033[0m' + '% Sell Monitoring for ETF Tickers: ' + '\\033[1m' + format(dfe['index'].values) + '\\033[0m', '\\n')\n",
    "\n",
    "    for i, c, q in zip(dfe['index'], dfe['percent_change'], dfe['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "        \n",
    "        print('Analysing Ticker: ', i, end = '                    \\r')\n",
    "        try:\n",
    "            realgainrate_etf_adj = realgainrate_etf + realgainrate_etf*(volatility_check('^GSPC') - volatility_check(i))\n",
    "            print('\\n\\n' + '\\033[1m' + f\"• {i}: \" + '\\033[0m' + '\\033[1m' + f\"{'%.2f' %realgainrate_etf_adj}% <--- {'%.2f' %current_gl[current_gl['index'] == i]['percent_change'].values[0]}%\" + '\\033[0m', '\\n')\n",
    "\n",
    "            if c >= realgainrate_etf_adj:\n",
    "                analysis = analysis_price(i)\n",
    "                strategy_analysis = strategy_analysis_main(i, 'off', 'off')[0] + strategy_analysis_main(i, 'off', 'off')[0]\n",
    " \n",
    "                valuecheck(i)\n",
    "                if strategy_analysis == -2 or (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_sell or analysis[3] > analysis[4]:\n",
    "                   \n",
    "                    print(f\"\"\"\\\n",
    "                    \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                    \"\"\")\n",
    "                    voice_message(f\"\"\"\\\n",
    "                    Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                    \"\"\")\n",
    "                    time.sleep(10)\n",
    "                    robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>  \n",
    "\n",
    "                    tickers_to_sell_etf.remove(i)\n",
    "                    with open(\"tickers_to_sell_etf.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_etf:  \n",
    "                            f.write(str(s) + '\\n')    \n",
    "\n",
    "                    # alarm\n",
    "                    webbrowser.open(\"soldsound.mp3\")\n",
    "\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% realized, ETF list has been updated.\"\"\".format(i, '%.2f' %c)\n",
    "                    emailsend_to_server(message)\n",
    "                    \n",
    "                else:\n",
    "                    print(f'\\n\\n$$$$$ Realize gain test all passed - ETF Ticker: {i} $$$$$\\n\\n')              \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Sell Monitoring for tickers_to_sell_etf: \\n', i)   \n",
    "            alarm()\n",
    "            pass\n",
    "        \n",
    " \n",
    "# Loss-Cut (ticker to sell small <= Excluding BIG Tickers) ############################################################################################################################################\n",
    "    with open(\"tickers_to_sell_small.txt\", 'r') as f:\n",
    "        tickers_to_sell_small = [line.rstrip('\\n') for line in f]  \n",
    "    \n",
    "    tickers_to_sell_small = [i for i in tickers_to_sell_small if i not in sold]\n",
    "        \n",
    "    with open(\"realgainrate_small.txt\", 'r') as f:\n",
    "        realgainrate_small = [line.rstrip('\\n') for line in f]    \n",
    "        realgainrate_small = np.round(float(realgainrate_small[0]), 2)\n",
    "   \n",
    "  # stocks on hold except Big\n",
    "    dfa = robin.loc[-robin['index'].isin(tickers_to_sell_big)]\n",
    "    dfa_original = dfa # to plot the candle chart\n",
    "    \n",
    "  # stocks on hold except purchased on the same date  \n",
    "    dfa = robin.loc[-robin['index'].isin(tickers_to_sell_big)]\n",
    "#     dfa = dfa.loc[-dfa['index'].isin(purchased)]\n",
    "    \n",
    "  # stocks on hold exceed the min. holding peried within the lost-cut threshold\n",
    "    dfs = robin.loc[robin['index'].isin(tickers_to_sell_small)]\n",
    "#     dfs = dfs.loc[-dfs['index'].isin(purchased)]\n",
    "    \n",
    "    print('\\n\\n\\n' + '\\033[1m' + '=====================================< Loss-cut Start >====================================' + '\\033[0m')\n",
    "    print('\\n\\n\\n' + '\\033[1m' + f\">>> {realgainrate_small}\" + '\\033[0m' + '% Base Loss-Cut calculating (Small tickers: -Realize Gain Mid% x 2) => ' + '\\033[1m' + format(dfs['index'].values) + '\\033[0m', '\\n')\n",
    "    \n",
    "# stocks on hold exceed the min. holding peried within the lost-cut threshold  \n",
    "    # Loss-cut execution by the adj_losscut\n",
    "    for i, c, q in zip(dfs['index'], dfs['percent_change'], dfs['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        print('Analysing Ticker: ', i)\n",
    "        try:\n",
    "            # loss cut percentage adjusting by the volatility \n",
    "            if i == 'USO' or i == 'GLD' or i == 'BTC-USD' or i in etf_ticker:\n",
    "                adj_losscut = np.round(realgainrate_small * (1-(volatility_check(i) - volatility_check('^GSPC'))),2)\n",
    "            else:\n",
    "                adj_losscut = np.round(realgainrate_small * (1-(volatility_check(i) - volatility_check(sector_valuation_ticker[sector_valuation_ticker['Symbol'] == i]['Ticker'].values[0]))),2)\n",
    "            print('\\n\\n' + '\\033[1m' + format(adj_losscut) + '\\033[0m' + '% adjusted loss cut monitoring: ' + '\\033[1m' + format(i) + '\\033[0m', f' <=== {c}%')\n",
    "                \n",
    "            if c < adj_losscut:\n",
    "                \n",
    "                q = split_qty_finder(i, q)                \n",
    "                \n",
    "                print(f\"\"\"\\\n",
    "                \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                \"\"\")\n",
    "                voice_message(f\"\"\"\\\n",
    "                Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                \"\"\")\n",
    "                time.sleep(10)                    \n",
    "                    \n",
    "                robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>  \n",
    "\n",
    "                tickers_to_sell_small.remove(i)\n",
    "                with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                    for s in tickers_to_sell_small:  \n",
    "                        f.write(str(s) + '\\n')                    \n",
    "\n",
    "                # email\n",
    "                message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% loss cut executed\"\"\".format(i, '%.2f' %c)\n",
    "                emailsend_to_server(message)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        except:\n",
    "            print('Except >>> Sell Monitoring for Loss Cut Sell: ', i)    \n",
    "            alarm()\n",
    "            sellmonitor()\n",
    "        print('\\n===> Passed\\n\\n')\n",
    "            \n",
    "     \n",
    "    # Loss-cut execution by Strategy Analysis\n",
    "    print('\\n\\n\\n' + '\\033[1m' + '>>> Strategy Analysis ' + '\\033[0m' + 'Loss-Cut Execution (Small tickers excl. BIG & Sold) => ' + '\\033[1m' + format(dfs['index'].values) + '\\033[0m', '\\n')\n",
    "\n",
    "    for i, c, q in zip(dfs['index'], dfs['percent_change'], dfs['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "            \n",
    "        print('Analysing Ticker: ', i)\n",
    "        try:\n",
    "            if strategy_analysis_main(i, 'off', 'off')[0] + strategy_analysis_main(i, 'off', 'off')[0] == -2:\n",
    "                    \n",
    "                q = split_qty_finder(i, q)\n",
    "                    \n",
    "                print(f\"\"\"\\\n",
    "                \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                \"\"\")\n",
    "                voice_message(f\"\"\"\\\n",
    "                Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                \"\"\")\n",
    "                time.sleep(10)                    \n",
    "                    \n",
    "                robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>\n",
    "                 \n",
    "                if i in tickers_to_sell_small:\n",
    "                    tickers_to_sell_small.remove(i)\n",
    "                    with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_small:  \n",
    "                            f.write(str(s) + '\\n')                    \n",
    "\n",
    "                # email\n",
    "                message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% Strategy Analysis loss cut executed\"\"\".format(i, '%.2f' %c)\n",
    "                emailsend_to_server(message)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Strategy Analysis Loss-Cut Execution: ', i)\n",
    "            pass\n",
    "        print('\\n===> Passed\\n\\n')\n",
    "        \n",
    "        \n",
    "    # Loss-cut execution by ValueCheck\n",
    "    print('\\n\\n\\n' + '\\033[1m' + '>>> ValueCheck ' + '\\033[0m' + 'Loss-Cut Execution (Small tickers excl. BIG & Sold) => ' + '\\033[1m' + format(dfs['index'].values) + '\\033[0m', '\\n')\n",
    "\n",
    "    for i, c, q in zip(dfs['index'], dfs['percent_change'], dfs['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "        \n",
    "        print('Analysing Ticker: ', i)\n",
    "        try:\n",
    "            valuecheck(i)\n",
    "            if (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_losscut:\n",
    "\n",
    "                q = split_qty_finder(i, q)\n",
    "                \n",
    "                print(f\"\"\"\\\n",
    "                \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                \"\"\")\n",
    "                voice_message(f\"\"\"\\\n",
    "                Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                \"\"\")\n",
    "                time.sleep(10)                  \n",
    "                \n",
    "                robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>   \n",
    "\n",
    "                if i in tickers_to_sell_small:                    \n",
    "                    tickers_to_sell_small.remove(i)\n",
    "                    with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                        for s in tickers_to_sell_small:  \n",
    "                            f.write(str(s) + '\\n')                    \n",
    "\n",
    "                # email\n",
    "                message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% ValueCheck loss cut executed\"\"\".format(i, '%.2f' %c)\n",
    "                emailsend_to_server(message)\n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> ValueCheck Loss-Cut Execution: ', i)\n",
    "            pass\n",
    "        print('\\n===> Passed\\n\\n')\n",
    "              \n",
    "        \n",
    "    # Loss-cut execution by moving average lines for all tickers except for Big tickers\n",
    "    print('\\n\\n\\n' + '\\033[1m' + '>>> Moving Average ' + '\\033[0m' + 'Loss-Cut Execution (All tickers excl. BIG & Purchased/Sold) => ' + '\\033[1m' + format(dfa['index'].values) + '\\033[0m', '\\n')\n",
    "\n",
    "    for i, c, q in zip(dfa['index'], dfa['percent_change'], dfa['quantity']):\n",
    "        \n",
    "        if timechecknow() > 390:\n",
    "            break\n",
    "        \n",
    "        print('Analysing Ticker: ', i)\n",
    "        try:\n",
    "            dmv_plot(i)\n",
    "            analysis_ticker = analysis_price(i)\n",
    "            \n",
    "    # crossover losscut condition #1\n",
    "            if analysis_ticker[8] == True and bollinger_sign_gen(i)[1] == 1: \n",
    "                \n",
    "                valuecheck(i)\n",
    "                if (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_losscut_all:\n",
    "                \n",
    "                    q = split_qty_finder(i, q)\n",
    "\n",
    "                    print(f\"\"\"\\\n",
    "                    \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                    \"\"\")\n",
    "                    voice_message(f\"\"\"\\\n",
    "                    Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                    \"\"\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "                    robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>   \n",
    "\n",
    "                    if i in tickers_to_sell_small:                    \n",
    "                        tickers_to_sell_small.remove(i)\n",
    "                        with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                            for s in tickers_to_sell_small:  \n",
    "                                f.write(str(s) + '\\n')                    \n",
    "\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% Moving Average loss cut executed\"\"\".format(i, '%.2f' %c)\n",
    "                    emailsend_to_server(message)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "    # crossover losscut condition #2 \n",
    "            elif analysis_ticker[8] == True and c < stg_loss_rate(i): \n",
    "                \n",
    "                valuecheck(i)\n",
    "                if (valuecheck(i)[0]+valuecheck(i)[1]*3) < valuation_conversion_value_losscut_all:\n",
    "                \n",
    "                    q = split_qty_finder(i, q)\n",
    "\n",
    "                    print(f\"\"\"\\\n",
    "                    \\n\\n\\n$$$$$ Hi Sean. We are selling {q} stocks of {i} at {c} percent return $$$$$\\n\\n\\n\n",
    "                    \"\"\")\n",
    "                    voice_message(f\"\"\"\\\n",
    "                    Hi Sean. We are selling {q} stocks of {i} at {c} percent return\n",
    "                    \"\"\")\n",
    "                    time.sleep(10)\n",
    "\n",
    "                    robinExecution_sell(i, q) #<<<<<<< Sell order >>>>>>>   \n",
    "\n",
    "                    if i in tickers_to_sell_small:                    \n",
    "                        tickers_to_sell_small.remove(i)\n",
    "                        with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "                            for s in tickers_to_sell_small:  \n",
    "                                f.write(str(s) + '\\n')                    \n",
    "\n",
    "                    # email\n",
    "                    message = \"\"\"\\\n",
    "Subject: QED [{}] Sold: {}% Moving Average loss cut executed\"\"\".format(i, '%.2f' %c)\n",
    "                    emailsend_to_server(message)\n",
    "                else:\n",
    "                    pass \n",
    "                \n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print('Except >>> Moving Average Loss-Cut Execution: ', i)\n",
    "            pass\n",
    "        print('\\n===> Passed\\n\\n')\n",
    "              \n",
    "        \n",
    "    print('\\n\\n' + '\\033[1m' + '======================================< Loss-cut End >=====================================' + '\\033[0m' + '\\n\\n')\n",
    "        \n",
    "    if cyclecount%10 == 0: \n",
    "        print('\\n\\n' + '\\033[1m' + '< Candle Check >' + '\\033[0m' + '\\n\\n')\n",
    "        for i in dfa_original['index'].to_list():\n",
    "            print(f'• Ticker: ' + '\\033[1m' + f'{i}' + '\\033[0m')\n",
    "            try:\n",
    "                candle_review(i)\n",
    "            except:\n",
    "                pass\n",
    "            print('\\n\\n')\n",
    "    \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The Engine#2 gets ready.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e044cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true
   },
   "source": [
    "## Engine #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd524814",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def robinExecution_buy(stock, qty):\n",
    "    global sellmonitor_freq\n",
    "    global num_of_purchased\n",
    "    \n",
    "    \n",
    "    # only execute during the market hours\n",
    "    if timechecknow() <= 390:\n",
    "        # count the existing number of stocks before buying-out\n",
    "        onhold_df = robin_login()\n",
    "\n",
    "        if stock in onhold_df['index'].tolist():\n",
    "            num_of_stocks_before = int(float(onhold_df[onhold_df['index'] == stock]['quantity'].values[0]))\n",
    "            print(f'\\n\\n\\n• Stock: {stock} - Existing stock / {num_of_stocks_before} shares')\n",
    "        else:\n",
    "            num_of_stocks_before = 0\n",
    "            print(f'\\n\\n\\n• Stock: {stock} - New stock / {num_of_stocks_before} shares')\n",
    "     \n",
    "        try:\n",
    "            # execue buy order\n",
    "            login = rs.login('aicpasean@gmail.com', sean.robin_api())\n",
    "            order_buy_market(stock, qty)\n",
    "            rs.logout()  \n",
    "            print('\\n'+current_time()+' - The buying order has been executed.\\n')\n",
    "            time.sleep(10)\n",
    "\n",
    "        except:\n",
    "            message = 'Except >>> Purchasing was not successful.'\n",
    "            print(message)\n",
    "            alarm()\n",
    "            voice_message(message)\n",
    "\n",
    "        # count the existing number of stocks after buying-out\n",
    "        onhold_df = robin_login()\n",
    "        \n",
    "        if stock in onhold_df['index'].tolist():\n",
    "            num_of_stocks_after = int(float(onhold_df[onhold_df['index'] == stock]['quantity'].values[0]))\n",
    "            print(f'• Stock: {stock} - Existing stock / {num_of_stocks_after} shares\\n\\n\\n')\n",
    "        else:\n",
    "            num_of_stocks_after = 0\n",
    "            print(f'• Stock: {stock} - New stock / {num_of_stocks_after} shares\\n\\n\\n')\n",
    "\n",
    "        if num_of_stocks_after - num_of_stocks_before >= 1:\n",
    "            \n",
    "            # purchased list update\n",
    "            print('\\n'+current_time()+'\\n')\n",
    "            trade_list('purchased', stock)\n",
    "\n",
    "            # append to the blacklist; only purchase once a day\n",
    "            with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "                buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "            buymonitor_blacklist.append(stock)\n",
    "            with open(\"buymonitor_blacklist.txt\", 'w') as f:\n",
    "                for s in buymonitor_blacklist:  \n",
    "                    f.write(str(s) + '\\n') \n",
    "            \n",
    "            # plot gain & loss\n",
    "            robin_plot()  \n",
    "\n",
    "            # email\n",
    "            message = \"\"\"\\\n",
    "Subject: QED [{}] {} Stock Purchased at ${}\"\"\".format(stock, qty, '%.2f' %analysis_price(stock)[4])\n",
    "            emailsend_to_server(message)\n",
    "            \n",
    "            # sell monitor cycle update   \n",
    "            num_of_purchased = len(onhold_df['index'])\n",
    "            sellmonitor_freq = int((sellmonitor_freq_limit) - (num_of_purchased - len(dead_stock))) ### adjust       \n",
    "            print('\\n\\nSell Monitor Freq has been updated as every' + '\\033[1m' + f' {sellmonitor_freq} ' + '\\033[0m' + 'cycles.\\n\\n')\n",
    "\n",
    "        else:\n",
    "            message = f'We have cash balance less than {dollar_limit_order_dollar} dollars'\n",
    "            print('Except >>> ' + message + '. \\n\\n\\n')\n",
    "            emailsend_to_server(message)\n",
    "            \n",
    "            # sell monitor cycle update   \n",
    "            sellmonitor_freq = 1\n",
    "            \n",
    "    else:\n",
    "        message = f'The market has been closed. The purchasing order has not been executed.'\n",
    "        print('Except >>> ' + message)\n",
    "        alarm()\n",
    "        voice_message(message)\n",
    "\n",
    "\n",
    "def buymonitor():    \n",
    "    global cyclecount\n",
    "    print(f\"#{cyclecount}th Cycle\")\n",
    "    \n",
    "    with open(f'purchased_{currentdate}.txt', 'r') as f:\n",
    "        purchased = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    with open(\"portfolio_superpass.txt\", 'r') as f:\n",
    "        portfolio_superpass = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "     \n",
    "    with open(\"portfolio.txt\", 'r') as f:\n",
    "        portfolio = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    portfolio = [i for i in portfolio if i not in buymonitor_blacklist and i not in portfolio_superpass and i not in purchased]\n",
    "    \n",
    "    sector_valuation_ticker = pd.read_csv('sector_valuation.csv')\n",
    "    sector_valuation_ticker = sector_valuation_ticker[sector_valuation_ticker['Date'] == currentdate]\n",
    "    sector_valuation_ticker = sector_valuation_ticker.reset_index(drop = True)\n",
    "    line_cut = sector_valuation_ticker.Time.unique()[-1:][0] # get the most recent valuation \n",
    "    sector_valuation_ticker = sector_valuation_ticker[sector_valuation_ticker['Time'] == line_cut]\n",
    "    \n",
    "    # get the target tickers\n",
    "    sector_valuation_ticker_meancheck = sector_valuation_ticker[sector_valuation_ticker['valuation_mean'] == True].Symbol.unique()\n",
    "    sector_valuation_ticker_snpcheck = sector_valuation_ticker[sector_valuation_ticker['valuation_snp'] == True].Symbol.unique()\n",
    "\n",
    "    # the sector true contains 'mean-True' and 'snp-True'\n",
    "    sector_valuation_ticker_symbol = np.concatenate((sector_valuation_ticker_meancheck, sector_valuation_ticker_snpcheck), axis = 0)\n",
    "    sector_valuation_ticker_symbol = [i for n, i in enumerate(sector_valuation_ticker_symbol) if i not in sector_valuation_ticker_symbol[:n]] #duplicated\n",
    "    \n",
    "    try:    \n",
    "        if rsi('^GSPC')[2] == False: # rsi[2] == 'Sell alarm'\n",
    "            \n",
    "            # if 'sell alarm' is False, 'or' condition applies\n",
    "            cross_true = []\n",
    "            for i in sector_valuation_ticker_symbol:\n",
    "                if i in sector_valuation_ticker_meancheck or i in sector_valuation_ticker_snpcheck:\n",
    "                    cross_true.append(i)\n",
    "                else:\n",
    "                    pass\n",
    "            sector_valuation_ticker_symbol = cross_true\n",
    "        else:\n",
    "            \n",
    "            # if 'sell alarm' is True, 'and' condition applies\n",
    "            cross_true = []\n",
    "            for i in sector_valuation_ticker_symbol:\n",
    "                if i in sector_valuation_ticker_meancheck and i in sector_valuation_ticker_snpcheck:\n",
    "                    cross_true.append(i)\n",
    "                else:\n",
    "                    pass\n",
    "            sector_valuation_ticker_symbol = cross_true   \n",
    "    except:\n",
    "        print('\\nExcept >>> Error triggered from buymonitor() ==> rsi(^GSPC)[2] == False\\n')\n",
    "        alarm()   \n",
    "\n",
    "    # value check only for Sector is True\n",
    "    portfolio_sector_true = [i for i in portfolio if i in sector_valuation_ticker_symbol] \n",
    "    portfolio_sector_true = sorted(portfolio_sector_true)   \n",
    "    \n",
    "    with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]    \n",
    "    portfolio_sector_true = [i for i in portfolio_sector_true if i not in buymonitor_blacklist]\n",
    "    with open(\"portfolio_sector_true.txt\", 'w') as f:\n",
    "        for s in portfolio_sector_true:      \n",
    "            f.write(str(s) + '\\n')   \n",
    "\n",
    "    # cpi rate average line analysis for 'GLD' and 'USO'    \n",
    "    cpi = pd.read_csv(f\"predict_aim_sourcedata_monitoring_^GSPC_{currentdate}.csv\")\n",
    "    cpi = cpi['Cpi'].tail(120).mean() < cpi['Cpi'].tail(60).mean()\n",
    "    \n",
    "    # when S&P500 has a wider swing, proportionally wide the buy limit within 5%\n",
    "    buy_limit_rate = 4+analysis_price('^GSPC')[5]/10\n",
    "    \n",
    "    buy_snp_t_y = [np.round(buy_limit_rate/2, 2)] \n",
    "    buy_gold_t_y = [np.round(buy_limit_rate/2.2, 2)] \n",
    "    buy_oil_t_y = [np.round(buy_limit_rate/2.2, 2)] \n",
    "    buy_bit_t_y = [np.round(buy_limit_rate/2.2, 2)] \n",
    "    \n",
    "    \n",
    "# Monitoring Starts from here\n",
    "\n",
    "# Super Pass monitoring    \n",
    "    # import required data\n",
    "    portfolio_superpass = [i for i in portfolio_superpass if i not in buymonitor_blacklist and i not in purchased]\n",
    "    \n",
    "    print('\\n\\n\\n' + '\\033[1m' + format(sum(buy_snp_t_y)) + '\\033[0m' + '% limit Buy - Super Pass monitoring: \\n\\n' + '\\033[1m' + format(portfolio_superpass) + '\\033[0m', '\\n')\n",
    "    for stock in portfolio_superpass:\n",
    "        if timechecknow() > 400:\n",
    "            break\n",
    "        if analysis_price('^GSPC')[2] < -0.5 and daily_news_sentiment_all() < min_market_sentiment:\n",
    "            break            \n",
    "        try:\n",
    "            print('Ticker: ', stock, end = '                                                                                \\r')\n",
    "            analysis = analysis_price(stock)\n",
    "            \n",
    "            # check if the candle sign exists when the loop break executed\n",
    "            token_exist = exists(f\"candle_token_{stock}.txt\")\n",
    "            if token_exist == True:\n",
    "                with open(f'candle_token_{stock}.txt', 'r') as f:\n",
    "                    candle_token_time = float([line.rstrip('\\n') for line in f][0])\n",
    "                if timechecknow() - candle_token_time < 7:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nFresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 3\n",
    "                else:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nUnfresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 0\n",
    "            else:\n",
    "                token_exist = 0\n",
    "\n",
    "    # Buy condition: Super Pass\n",
    "            if analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_snp_t_y) and analysis[6] == True and analysis[9] == True:\n",
    "            \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "                \n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and sentiment_analysis(stock) > 0 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1:\n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "         \n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_buy:             \n",
    "\n",
    "                            try:\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "\n",
    "                                print(f\"\"\"\\\n",
    "                                \\n\\n\\n$$$$$ Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars $$$$$\\n\\n\\n\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars\n",
    "                                \"\"\")\n",
    "\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "\n",
    "                            except:                            \n",
    "                                print(f\"\"\"\\\n",
    "                                Except >>> Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>>   \n",
    "                                \n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print(f\"Exception >>> Buy - Super Pass - Ticker: {stock}\")\n",
    "            alarm()\n",
    "            pass\n",
    " \n",
    "\n",
    "    # import required data\n",
    "    print('\\n\\n\\n' + '\\033[1m' + format(sum(buy_snp_t_y)) + '\\033[0m' + '% limit Buy - Sector/True monitoring: \\n\\n' + '\\033[1m' + format(portfolio_sector_true) + '\\033[0m', '\\n')\n",
    "\n",
    "    for stock in portfolio_sector_true:\n",
    "        if timechecknow() > 400:\n",
    "            break\n",
    "        if analysis_price('^GSPC')[2] < -0.5 and daily_news_sentiment_all() < min_market_sentiment:\n",
    "            break\n",
    "        try:\n",
    "            print('Ticker: ', stock, end = '                                                                                \\r')\n",
    "            analysis = analysis_price(stock)\n",
    "    \n",
    "            # check if the candle sign exists when the loop break executed\n",
    "            token_exist = exists(f\"candle_token_{stock}.txt\")\n",
    "            if token_exist == True:\n",
    "                with open(f'candle_token_{stock}.txt', 'r') as f:\n",
    "                    candle_token_time = float([line.rstrip('\\n') for line in f][0])\n",
    "                if timechecknow() - candle_token_time < 7:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nFresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 3\n",
    "                else:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nUnfresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 0\n",
    "            else:\n",
    "                token_exist = 0\n",
    "\n",
    "    # Buy condition: S&P 500\n",
    "            if analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_snp_t_y):\n",
    "            \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "\n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and sentiment_analysis(stock) > 0 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1:\n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "\n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_buy:   \n",
    "\n",
    "                            try:\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "\n",
    "                                print(f\"\"\"\\\n",
    "                                \\n\\n\\n$$$$$ Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars $$$$$\\n\\n\\n\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars\n",
    "                                \"\"\")\n",
    "\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "\n",
    "                            except:                            \n",
    "                                print(f\"\"\"\\\n",
    "                                Except >>> Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "                        \n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass                        \n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass   \n",
    "        except:\n",
    "            print(f\"Exception >>> Buy - Sector/True monitoring - Ticker: {stock}\")\n",
    "            alarm()\n",
    "            pass\n",
    "        \n",
    "\n",
    "    # always execute ML_True tickers - separate monitoring\n",
    "    sector_valuation_ticker_valuecheck = sector_valuation_ticker[sector_valuation_ticker['valuation_rsq'] > sector_valuation_ticker.valuation_rsq.mean()].Symbol.unique()\n",
    "    portfolio_valuation_true = [i for i in portfolio if i in sector_valuation_ticker_valuecheck and i not in portfolio_sector_true] \n",
    "    portfolio_valuation_true = sorted(portfolio_valuation_true)   \n",
    "\n",
    "    with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "    portfolio_valuation_true = [i for i in portfolio_valuation_true if i not in buymonitor_blacklist]\n",
    "    with open(\"portfolio_sector_valuation_true.txt\", 'w') as f:\n",
    "        for s in portfolio_valuation_true:      \n",
    "            f.write(str(s) + '\\n')   \n",
    "\n",
    "    # import required data\n",
    "    print('\\n\\n\\n' + '\\033[1m' + format(sum(buy_snp_t_y)) + '\\033[0m' + '% limit Buy - Valuation/True monitoring(excl.Sector/True): \\n\\n' + '\\033[1m' + format(portfolio_valuation_true) + '\\033[0m', '\\n')\n",
    "\n",
    "    for stock in portfolio_valuation_true:\n",
    "        if timechecknow() > 400:\n",
    "            break\n",
    "        if analysis_price('^GSPC')[2] < -0.5 and daily_news_sentiment_all() < min_market_sentiment:\n",
    "            break\n",
    "        try:\n",
    "            print('Ticker: ', stock, end = '                                                                                \\r')\n",
    "            analysis = analysis_price(stock)\n",
    "            \n",
    "            # check if the candle sign exists when the loop break executed\n",
    "            token_exist = exists(f\"candle_token_{stock}.txt\")\n",
    "            if token_exist == True:\n",
    "                with open(f'candle_token_{stock}.txt', 'r') as f:\n",
    "                    candle_token_time = float([line.rstrip('\\n') for line in f][0])\n",
    "                if timechecknow() - candle_token_time < 7:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nFresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 3\n",
    "                else:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nUnfresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 0\n",
    "            else:\n",
    "                token_exist = 0\n",
    "\n",
    "    # Buy condition: S&P 500\n",
    "            if analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_snp_t_y):\n",
    "            \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "\n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and sentiment_analysis(stock) > 0 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1: \n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "\n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_buy:   \n",
    "\n",
    "                            try:\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "\n",
    "                                print(f\"\"\"\\\n",
    "                                \\n\\n\\n$$$$$ Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars $$$$$\\n\\n\\n\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars\n",
    "                                \"\"\")\n",
    "\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "\n",
    "                            except:\n",
    "                                print(f\"\"\"\\\n",
    "                                Except >>> Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")   \n",
    "\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>>   \n",
    "                                \n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass   \n",
    "        except:\n",
    "            print(f\"Exception >>> Buy - Valuation/True monitoring - Ticker: {stock}\")\n",
    "            alarm()\n",
    "            pass\n",
    "\n",
    "\n",
    "# Inverse tickers        \n",
    "    with open(\"inv_tickers.txt\", 'r') as f:\n",
    "        inv_tickers = [line.rstrip('\\n') for line in f]\n",
    "    \n",
    "    with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    inv_tickers = [i for i in inv_tickers if i not in buymonitor_blacklist and i not in purchased]\n",
    "\n",
    "    print('\\n\\n\\n' + 'Inverse Ticker Monitoring', '\\n')\n",
    "    for stock in inv_tickers:\n",
    "        if timechecknow() > 400:\n",
    "            break\n",
    "        if analysis_price('^GSPC')[2] < -0.5 and daily_news_sentiment_all() < min_market_sentiment:\n",
    "            break\n",
    "        try:\n",
    "            print('Ticker: ', stock, end = '                                                                                \\r')\n",
    "            analysis = analysis_price(stock)\n",
    "            \n",
    "            # check if the candle sign exists when the loop break executed\n",
    "            token_exist = exists(f\"candle_token_{stock}.txt\")\n",
    "            if token_exist == True:\n",
    "                with open(f'candle_token_{stock}.txt', 'r') as f:\n",
    "                    candle_token_time = float([line.rstrip('\\n') for line in f][0])\n",
    "                if timechecknow() - candle_token_time < 7:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nFresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 3\n",
    "                else:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nUnfresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 0\n",
    "            else:\n",
    "                token_exist = 0\n",
    "\n",
    "    # Buy condition : Gold \n",
    "            if stock == \"GLD\" and analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_gold_t_y) and analysis_price('DX-Y.NYB')[1] == False and cpi == True and analysis[6] == True and analysis[9] == True:\n",
    "            \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "                \n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1:\n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "\n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_etf_buy:   \n",
    "\n",
    "                            try:\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "\n",
    "                                print(f\"\"\"\\\n",
    "                                \\n\\n\\n$$$$$ Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars $$$$$\\n\\n\\n\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars\n",
    "                                \"\"\")\n",
    "\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "\n",
    "                            except:\n",
    "                                print(f\"\"\"\\\n",
    "                                Except >>> Hi Sean. We have an error while buying {stock} Re-trying\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "                                \n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    # Buy condition : Oil\n",
    "            elif stock == \"USO\" and analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_oil_t_y) and analysis_price('DX-Y.NYB')[1] == False and cpi == True and analysis[6] == True and analysis[9] == True:\n",
    "     \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "                \n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1:\n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "              \n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_etf_buy:\n",
    "\n",
    "                            try:\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "\n",
    "                                print(f\"\"\"\\\n",
    "                                \\n\\n\\n$$$$$ Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars $$$$$\\n\\n\\n\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars\n",
    "                                \"\"\")\n",
    "\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "\n",
    "                            except:                            \n",
    "                                print(f\"\"\"\\\n",
    "                                Except >>> Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>>      \n",
    "                                \n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "    # Buy condition : BitCoin\n",
    "            elif stock == \"BTC-USD\" and analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_bit_t_y) and analysis_price('DX-Y.NYB')[2] < analysis[2] and analysis_price('DX-Y.NYB')[3] > analysis_price('DX-Y.NYB')[4] and analysis_price('DX-Y.NYB')[1] == False and analysis[6] == True and analysis[9] == True:\n",
    "          \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "                \n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1:\n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "            \n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_etf_buy:\n",
    "\n",
    "                            # email\n",
    "                            message = \"\"\"\\\n",
    "Subject: QED [{}] Buy sign at {}% {}\"\"\".format(stock, '%.2f' %analysis_price(stock)[2], stock)\n",
    "                            emailsend_to_server(message)\n",
    "\n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass   \n",
    "        except:\n",
    "            print(f\"Exception >>> Buy - L/T holding and Whitelist monitoring - Ticker: {stock}\")\n",
    "            alarm()\n",
    "            pass\n",
    "\n",
    "\n",
    "# ETF monitoring    \n",
    "    # import required data\n",
    "    with open(\"etf_portfolio.txt\", 'r') as f:\n",
    "        etf_portfolio = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    with open(\"buymonitor_blacklist.txt\", 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]\n",
    "   \n",
    "    etf_portfolio = [i for i in etf_portfolio if i not in buymonitor_blacklist and i not in purchased]\n",
    "    \n",
    "    print('\\n\\n\\n' + '\\033[1m' + format(sum(buy_snp_t_y)) + '\\033[0m' + '% limit Buy - ETF monitoring: \\n\\n' + '\\033[1m' + format(etf_portfolio) + '\\033[0m', '\\n')\n",
    "    for stock in etf_portfolio:\n",
    "        if timechecknow() > 400:\n",
    "            break\n",
    "        if analysis_price('^GSPC')[2] < -0.5 and daily_news_sentiment_all() < min_market_sentiment:\n",
    "            break\n",
    "        try:\n",
    "            print('Ticker: ', stock, end = '                                                                                \\r')\n",
    "            analysis = analysis_price(stock)\n",
    "            \n",
    "            # check if the candle sign exists when the loop break executed\n",
    "            token_exist = exists(f\"candle_token_{stock}.txt\")\n",
    "            if token_exist == True:\n",
    "                with open(f'candle_token_{stock}.txt', 'r') as f:\n",
    "                    candle_token_time = float([line.rstrip('\\n') for line in f][0])\n",
    "                if timechecknow() - candle_token_time < 7:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nFresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 3\n",
    "                else:\n",
    "                    os.remove(f'candle_token_{stock}.txt')\n",
    "                    print ('\\n\\nUnfresh Candle - File removed: ' + '\\033[1m' + f'candle_token_{stock}.txt' + '\\033[0m' + ' | Checking Buy Condition...\\n\\n')\n",
    "                    token_exist = 0\n",
    "            else:\n",
    "                token_exist = 0\n",
    "\n",
    "    # Buy condition: ETF\n",
    "            if analysis[2] > -0.3 and analysis[8] == False and analysis[7]+analysis[10]+analysis[11]+analysis[12] != False and analysis[2] < sum(buy_snp_t_y) and analysis_price('^GSPC')[5] < analysis[5]:\n",
    "      \n",
    "                print(f'Candle Check - Ticker: {stock}\\n')\n",
    "                candle_result = volume_vs_sign(stock)\n",
    "                \n",
    "                if candle_result+token_exist > 1:\n",
    "                    print(f'\\n > Ticker: {stock}\\n > Candle Result: {candle_result}\\n')\n",
    "                \n",
    "                if candle_result+token_exist >= 3 and bollinger_sign_gen(stock)[0] == 1 and strategy_analysis_main(stock, 'off', 'off')[0] + strategy_analysis_main(stock, 'off', 'off')[0] >= 1:\n",
    "                    focus_record(stock)\n",
    "                    initial_val_result = valuecheck(stock)[0]+valuecheck(stock)[1]*3\n",
    "                    if initial_val_result >= 0:\n",
    "         \n",
    "                        if (valuecheck(stock)[0]+valuecheck(stock)[1]*3) >= valuation_conversion_value_etf_buy:          \n",
    "\n",
    "                            try:\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "\n",
    "                                print(f\"\"\"\\\n",
    "                                \\n\\n\\n$$$$$ Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars $$$$$\\n\\n\\n\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We are buying {order_qty} stocks of {stock} at {int(analysis_price(stock)[4])} dollars\n",
    "                                \"\"\")\n",
    "\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>> \n",
    "\n",
    "                            except:                            \n",
    "                                print(f\"\"\"\\\n",
    "                                Except >>> Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "                                voice_message(f\"\"\"\\\n",
    "                                Hi Sean. We have an error while buying {stock}. Re-trying\n",
    "                                \"\"\")\n",
    "\n",
    "                                order_qty = int(1 + dollar_limit_order_dollar / analysis_price(stock)[4])\n",
    "                                robinExecution_buy(stock, order_qty) #<<<<<<< buy order >>>>>>>   \n",
    "                                \n",
    "                        else:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                pass\n",
    "        except:\n",
    "            print(f\"Exception >>> Buy - ETF - Ticker: {stock}\")\n",
    "            alarm()\n",
    "            pass\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The Engine#3 gets ready.' + '\\033[0m', '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544414d9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "## Engine #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "70cb39a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today_to_add = date.today()\n",
    "pd.set_option('display.max_column', None)\n",
    "magic = pd.read_csv('magic.csv')\n",
    "   \n",
    "    \n",
    "def robin_equity_check(): \n",
    "    global num_of_purchased\n",
    "    global performance_updated_df\n",
    "    global stock_return_percent\n",
    "    global predict_proba_value\n",
    "    \n",
    "    try:\n",
    "        robin = robin_login()\n",
    "        stock_return_percent = perf_cal(robin)\n",
    "        performance_updated_df = robin[['index', 'equity', 'equity_change', 'percent_change']]\n",
    "        performance_updated_df.sort_values('percent_change', ascending = False, inplace = True)\n",
    "        performance_updated_df = performance_updated_df.reset_index(drop = True)\n",
    "        equity_change = np.round(performance_updated_df['equity_change'].astype(float).sum(), 2)\n",
    "\n",
    "        performance_updated_df.to_csv('performance_updated_df.csv', index = False)\n",
    "\n",
    "        # filter out stocks within the min. holding period\n",
    "        current_index = sorted(robin['index'].tolist())\n",
    "        holding_period = pd.read_csv('holding_period.csv')\n",
    "        holding_period_index = sorted(holding_period[holding_period['days'] >= min_holding_period]['index'].tolist())\n",
    "        holding_period_index = [i for i in holding_period_index if i in current_index]\n",
    "\n",
    "        print('\\n\\n===[ Portfolio Summary ]===================================================================\\n')\n",
    "        print(performance_updated_df) \n",
    "        print(\"\\n• Equity change: \" + '\\033[1m' + f\"             $ {equity_change}\" + '\\033[0m')\n",
    "        print(\"\\n• Performance average: \" + '\\033[1m' + f\"         {stock_return_percent} %\" + '\\033[0m')\n",
    "        print(\"\\n• Number of stocks on hand: \" + '\\033[1m' + f\"    {int(num_of_purchased)} stocks\" + '\\033[0m')\n",
    "        print(\"\\n• Loss-cut Min. Holding Period: \" + '\\033[1m' + f\"{min_holding_period} days\" + '\\033[0m')\n",
    "        print(\"\\n• Stocks on hold: \" + '\\033[1m' + f\"       {current_index}\" + '\\033[0m')\n",
    "        print(\"\\n• Stocks aged the limit: \" + '\\033[1m' + f\"{holding_period_index}\" + '\\033[0m')\n",
    "        print('\\n===================================================================[ Portfolio Summary ]===\\n')\n",
    "\n",
    "        vars = ['price', 'quantity', 'average_buy_price', 'equity', 'percent_change', \n",
    "                'intraday_percent_change', 'equity_change', 'pe_ratio', 'percentage']\n",
    "        for var in vars:\n",
    "            robin[var] = pd.to_numeric(robin[var])\n",
    "        robin['quantity'] = robin['quantity'].astype(int)\n",
    "        robin.to_csv('robin_all.csv', index = False)\n",
    "        portfolio_list = robin['index'].values.tolist()\n",
    "        losscut = robin[['index','percent_change']]\n",
    "\n",
    "# Mid Gain Setting ###########################################################################################################\n",
    "        def mid_gain_cal(ticker):\n",
    "            ticker_gain = yfinance_df_rsi(ticker)\n",
    "            ticker_gain = (ticker_gain[-1:]['Adj Close'].mean() - ticker_gain[-(min_holding_period+1):-min_holding_period]['Adj Close'].mean()) / ticker_gain[-(min_holding_period+1):-min_holding_period]['Adj Close'].mean()\n",
    "            return ticker_gain\n",
    "\n",
    "        active_tickers = [ticker for ticker in robin['index'].tolist() if ticker not in dead_stock]\n",
    "        if len(active_tickers) == 0:\n",
    "            mid_gain_avg = realize_gain_max/100    \n",
    "        else:\n",
    "            mid_gain_list = [mid_gain_cal(ticker) for ticker in active_tickers]\n",
    "            mid_gain_avg = np.median(mid_gain_list)\n",
    "\n",
    "        portfolio_sector = pd.read_csv('./snp500/snp_sector_index.csv')\n",
    "        portfolio_sector = portfolio_sector.loc[portfolio_sector.Symbol.isin(robin['index'].tolist())]\n",
    "        portfolio_sector = portfolio_sector['GICS Sector'].tolist()\n",
    "\n",
    "        etf_index = etf_index_dic.loc[etf_index_dic.sector.isin(portfolio_sector)].ticker.tolist()\n",
    "        etf_gain_list = [mid_gain_cal(ticker) for ticker in etf_index]\n",
    "        etf_gain_avg = np.average(etf_gain_list)\n",
    "\n",
    "        realgainrate_mid = np.round((mid_gain_avg*mid_gain_adjust_percent+etf_gain_avg*mid_gain_market_reflect_percent_by_etf)*100, 2) # reflect market trend 20%\n",
    "  \n",
    "        def strategy_analysis_mid_gain(tickers):\n",
    "            global min_SuccessRate\n",
    "            print('\\n\\n' + 'Mid-Gain Percentage Calculating' + '\\n')\n",
    "            min_SuccessRate_original = min_SuccessRate\n",
    "            min_SuccessRate = int(min_SuccessRate * 0.8)\n",
    "            \n",
    "            strategy_analysis_gain = []\n",
    "            for ticker in tickers:\n",
    "                strategy_analysis_result = strategy_analysis_main(ticker, 'off', 'off')[1]\n",
    "                if strategy_analysis_result != -10:\n",
    "                    strategy_analysis_gain.append(strategy_analysis_result)\n",
    "            min_SuccessRate = min_SuccessRate_original\n",
    "            return np.average(strategy_analysis_gain)*100*(2/3) #<--------- adjust\n",
    "        \n",
    "        realgainrate_mid = np.round(realgainrate_mid + strategy_analysis_mid_gain(robin['index'].values.tolist()), 2)\n",
    "        print(f'\\n\\n• Mid-Gain Percentage Before Adjusting : {realgainrate_mid}\\n')\n",
    "       \n",
    "        print(f'• Mid-Gain Adjust Percentage - Portfolio:   {mid_gain_adjust_percent}\\n\\n• Mid-Gain Adjust Percentage - Market(ETF): {mid_gain_market_reflect_percent_by_etf}\\n\\n• Initial Realize Gain Mid: {realgainrate_mid}\\n\\n• Realize Gain Max: {realize_gain_max}\\n\\n• Realize Gain Min: {realize_gain_min}')\n",
    "        print('\\n===========================================================================================\\n')\n",
    "\n",
    "        # threshold - max. gain\n",
    "        if realgainrate_mid > realize_gain_max:\n",
    "            realgainrate_mid = realize_gain_max\n",
    "            message = f'\\n\\n\\nImportant >>> Mid Realize Gain is higher than the limit, reset at {realize_gain_max} %\\n\\n\\n'\n",
    "            print(message)\n",
    "\n",
    "        # proba value reset by market trend before adjusting    \n",
    "        if realgainrate_mid < -0.5:\n",
    "            predict_proba_value = predict_proba_value + 0.025\n",
    "            print(f'\\n\\n\\nImportant >>> Mid Realize Gain is less than the limit, Predict Proba Value increase by 2.5 %\\n\\n\\n')\n",
    "            print(f'\\n\\n===> New Probe Value: {np.round(predict_proba_value, 2)} <===\\n\\n')\n",
    "        elif realgainrate_mid < -1:\n",
    "            predict_proba_value = predict_proba_value + 0.05\n",
    "            print(f'\\n\\n\\nImportant >>> Mid Realize Gain is less than the limit, Predict Proba Value increase by 5 %\\n\\n\\n')\n",
    "            print(f'\\n\\n===> New Probe Value: {np.round(predict_proba_value, 2)} <===\\n\\n')\n",
    "        elif realgainrate_mid < -1.5:\n",
    "            predict_proba_value = predict_proba_value + 0.1\n",
    "            print(f'\\n\\n\\nImportant >>> Mid Realize Gain is less than the limit, Predict Proba Value increase by 10 %\\n\\n\\n')\n",
    "            print(f'\\n\\n===> New Probe Value: {np.round(predict_proba_value, 2)} <===\\n\\n')\n",
    "        else:\n",
    "            pass    \n",
    "        \n",
    "        # threshold - min. gain\n",
    "        if realgainrate_mid < realize_gain_min:\n",
    "            realgainrate_mid = realize_gain_min\n",
    "            message = f'\\n\\n\\nImportant >>> Mid Realize Gain is smaller than the limit, reset at {realize_gain_min} %\\n\\n\\n'\n",
    "            print(message)\n",
    "            \n",
    "#             emailsend_to_server(message)\n",
    "    \n",
    "        # if event day triggering a huge volatility, cut the gain\n",
    "        if upcoming_remain == 0:\n",
    "            analysis_market = analysis_price('^GSPC')\n",
    "            if analysis_market[1]+analysis_market[7]+analysis_market[10]+analysis_market[11]+analysis_market[12] == 0:\n",
    "                realgainrate_mid = realgainrate_mid * event_risk_ratio\n",
    "            else:\n",
    "                print('\\n\\n\\nAn event is scheduled today, but the S&P 500 index is not in a downturn. No adjusting Mid-Term Margin.\\n\\n\\n')\n",
    "        else:\n",
    "            pass\n",
    "##############################################################################################################################\n",
    "        \n",
    "        # get the loss-cut percentage\n",
    "        losscutpercentage = -np.round(realgainrate_mid, 2) #<<<<<<<<<<<<<<<<<< Important <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "        with open(\"losscutpercentage.txt\", 'w') as f:\n",
    "            f.write(str(losscutpercentage))\n",
    "\n",
    "        losscut_index = losscut[losscut['percent_change'] <= losscutpercentage].sort_values('percent_change').reset_index(drop = True)['index'].tolist()\n",
    "        losscut_index = [i for i in losscut_index if i in holding_period_index]\n",
    "        \n",
    "        # ETF tickers\n",
    "        etf = ['XLE', 'XLF', 'XLRE', 'XLB', 'XLK', 'XLP', 'XLV', 'XLI', 'XLY', 'XLU', 'XLC', 'QQQ', 'SPY', 'SPYD', 'SPYG', 'SPYV']  \n",
    "\n",
    "        # Big Sell (excl. ETF)\n",
    "        snp_sector = pd.read_csv('./snp500/snp_sector_index.csv')\n",
    "        try:\n",
    "            on_hand_non_etf = robin['index'].values\n",
    "            on_hand_non_etf = [i for i in on_hand_non_etf if i not in etf and i not in inv_tickers and i not in dead_stock]\n",
    "            \n",
    "            tickers_to_sell_big = []\n",
    "            for i in sorted(on_hand_non_etf):\n",
    "                print(f'Big Ticker Analyzing: {i}', end = '           \\r')\n",
    "                analysis = analysis_price(i)\n",
    "                if rsi(i)[0] == True and analysis_price(snp_sector[snp_sector['Symbol'] == i]['Ticker'].values[0])[5] < analysis[5] and volatility_check(i) < volatility_check('^GSPC') and analysis[1]+analysis[7]+analysis[10]+analysis[11]+analysis[12] >= 3:\n",
    "                    tickers_to_sell_big.append(i)\n",
    "            print(end = '                                                              \\r')\n",
    "        except:\n",
    "            print('Except >>> tickers_to_sell_big - RSI Check Error: ', i)\n",
    "            alarm()\n",
    "            pass\n",
    "        tickers_to_sell_big.extend(dead_stock) #temp #####################################################\n",
    "        tickers_to_sell_big = [i for n, i in enumerate(tickers_to_sell_big) if i not in tickers_to_sell_big[:n]] # duplicated\n",
    "        \n",
    "        with open(\"tickers_to_sell_big.txt\", 'w') as f:\n",
    "            for s in tickers_to_sell_big:  \n",
    "                f.write(str(s) + '\\n')     \n",
    "        realgainrate_big = np.round(realgainrate_mid*1.1, 2) #L/T gain%\n",
    "        with open(\"realgainrate_big.txt\", 'w') as f:\n",
    "            f.write(str(realgainrate_big))\n",
    "\n",
    "        # Inverse Sell \n",
    "        tickers_to_sell_inv = [i for i in robin['index'].values.tolist() if i in inv_tickers]\n",
    "        with open(\"tickers_to_sell_inv.txt\", 'w') as f:\n",
    "            for s in tickers_to_sell_inv:  \n",
    "                f.write(str(s) + '\\n')   \n",
    "        realgainrate_inv = np.round(realgainrate_mid*0.9, 2) #Inv gain%\n",
    "        with open(\"realgainrate_inv.txt\", 'w') as f:\n",
    "            f.write(str(realgainrate_inv))\n",
    "\n",
    "        # Loss-cut tickers\n",
    "        tickers_to_sell_small = []\n",
    "        for s in losscut_index:\n",
    "            if s not in tickers_to_sell_big:\n",
    "                tickers_to_sell_small.append(s)\n",
    "\n",
    "        # Exclude 'Sold' list from tickers_to_sell_small\n",
    "        with open(f'sold_{currentdate}.txt', 'r') as f:\n",
    "            sold = [line.rstrip('\\n') for line in f]\n",
    "        for i in sold:\n",
    "            if i in tickers_to_sell_small:\n",
    "                tickers_to_sell_small.remove(i)\n",
    "\n",
    "        with open(\"tickers_to_sell_small.txt\", 'w') as f:\n",
    "            for s in tickers_to_sell_small:  \n",
    "                f.write(str(s) + '\\n')    \n",
    "                \n",
    "        realgainrate_small = np.round(losscutpercentage*2, 2) #loss cut%  #<<<<<<<<<<<<<<<<<<<<<<<<< Important >>>>>>>>>>>>>>>>\n",
    "        with open(\"realgainrate_small.txt\", 'w') as f:\n",
    "            f.write(str(realgainrate_small))\n",
    "            \n",
    "        print('\\n===[ Realize Gain or Loss ]================================================================')\n",
    "        print(\"\\n • Loss-cut Min. Holding Period: \" + '\\033[1m' + f\"{min_holding_period} days\" + '\\033[0m')\n",
    "        print('\\n', f\"• Loss Cut Execution : \" + '\\033[1m' + f\"{realgainrate_small} % (Max. Loss Cut) <---x2---  {losscutpercentage} % (Monitoring Start)\" + '\\033[0m')\n",
    "        print('___________________________________________________________________________________________')\n",
    "        for i in tickers_to_sell_small:\n",
    "            adj_losscut = np.round(realgainrate_small * (1-(volatility_check(i) - volatility_check('^GSPC'))), 2)\n",
    "            print('\\n', \"  >>> Tickers to monitor Loss Cut: \" + '\\033[1m' + f\" {i} \" + '\\033[0m' + \" at \" + '\\033[1m' + f\"{adj_losscut} % <===  {robin[robin['index'] == i]['percent_change'].values[0]} %\" + '\\033[0m')\n",
    "            print('___________________________________________________________________________________________')         \n",
    "\n",
    "        # Mid Sell\n",
    "        tickers_to_sell_mid = []\n",
    "        for s in portfolio_list:\n",
    "            if s not in tickers_to_sell_big and s not in tickers_to_sell_small and s not in tickers_to_sell_inv and s not in etf:\n",
    "                tickers_to_sell_mid.append(s)\n",
    "        with open(\"tickers_to_sell_mid.txt\", 'w') as f:\n",
    "            for s in tickers_to_sell_mid:  \n",
    "                f.write(str(s) + '\\n')     \n",
    "        # gain% is already set above\n",
    "        with open(\"realgainrate_mid.txt\", 'w') as f:\n",
    "            f.write(str(realgainrate_mid))\n",
    "\n",
    "        # ETF Sell \n",
    "        tickers_to_sell_etf = [i for i in robin['index'].values.tolist() if i in etf and i not in tickers_to_sell_small]\n",
    "        with open(\"tickers_to_sell_etf.txt\", 'w') as f:\n",
    "            for s in tickers_to_sell_etf:  \n",
    "                f.write(str(s) + '\\n')   \n",
    "        realgainrate_etf = np.round(realgainrate_mid*0.9, 2) #ETF gain%\n",
    "        with open(\"realgainrate_etf.txt\", 'w') as f:\n",
    "            f.write(str(realgainrate_etf))\n",
    "\n",
    "        print('\\n',f\"• Tickers to sell BIG: {tickers_to_sell_big} at \" + '\\033[1m' + f\"Max. {np.round(realgainrate_big, 2)} %\" + '\\033[0m')\n",
    "        print('\\n',f\"• Tickers to sell MID: {tickers_to_sell_mid} at \" + '\\033[1m' + f\"Max. {np.round(realgainrate_mid, 2)} %\" + '\\033[0m')\n",
    "        print('\\n',f\"• Tickers to sell INV: {tickers_to_sell_inv} at \" + '\\033[1m' + f\"Max. {np.round(realgainrate_inv, 2)} %\" + '\\033[0m')\n",
    "        print('\\n',f\"• Tickers to sell ETF: {tickers_to_sell_etf} at \" + '\\033[1m' + f\"Max. {np.round(realgainrate_etf, 2)} %\" + '\\033[0m')\n",
    "        print('\\n',f\"• Tickers to Loss-Cut: {tickers_to_sell_small} at \" + '\\033[1m' + f\"Max. {np.round(realgainrate_small, 2)} %\" + '\\033[0m')\n",
    "        print('\\n================================================================[ Realize Gain or Loss ]===\\n')\n",
    "        print('\\n\\n\\n')\n",
    "    except:\n",
    "        print('Except >>> Robin Equity Check', end = '                                                                \\r')\n",
    "        robin_equity_check()\n",
    "\n",
    "    return robin\n",
    "\n",
    "\n",
    "def robin_plot(): # equity check\n",
    "    try:\n",
    "        \n",
    "        def gain_loss_plot(df):\n",
    "            gain = df[df['change'] >= 0].sort_values('change',ascending = False).reset_index(drop = True)\n",
    "            loss = df[df['change'] < 0].sort_values('change',ascending = False).reset_index(drop = True)\n",
    "\n",
    "            plt.figure(figsize = (15,5))\n",
    "            plt.bar(gain['index'], gain['change']*100, alpha = 0.95, color = sb.color_palette()[0])\n",
    "            plt.bar(loss['index'], loss['change']*100, alpha = 0.95, color = sb.color_palette()[3])\n",
    "            plt.title('Robinhood - Total Return(%)')\n",
    "            plt.grid(axis = 'x')\n",
    "            plt.grid(axis = 'y')\n",
    "            plt.show()\n",
    "\n",
    "        # SEC Gain & Loss\n",
    "        krw = exchange_check()\n",
    "        currentprice = yfinance_df_min('005930.KS')['Adj Close'].values[0]\n",
    "        cost = 78325\n",
    "        shares = 5000\n",
    "        sec_gain_loss = ((currentprice - cost) * shares) / krw\n",
    "        sec_values = (currentprice * shares) / krw\n",
    "        print('\\n')\n",
    "        print('\\033[1m' + 'Portfolio' + '\\033[0m', '\\n')\n",
    "        print('- SEC Gain & Loss: ' + '\\033[1m' + str(int(sec_gain_loss)) + '\\033[0m', 'USD', '\\n')\n",
    "\n",
    "        login = rs.login('aicpasean@gmail.com', sean.robin_api())\n",
    "        my_stocks = rs.build_holdings()\n",
    "        rs.logout()\n",
    "        robin = pd.DataFrame(my_stocks)\n",
    "        robin = robin.T.reset_index()\n",
    "\n",
    "        mkv = []\n",
    "        for i in range(len(robin.equity.values)):\n",
    "            value = float(robin.equity.values[i])\n",
    "            mkv.append(value)  \n",
    "        percentage = []\n",
    "        for i in range(len(robin.percent_change.values)):\n",
    "            value = float(robin.percent_change.values[i])\n",
    "            percentage.append(value)  \n",
    "        percentage = np.array(percentage)*0.01\n",
    "        gain_loss = sum(mkv) - sum(mkv / (percentage + 1)) \n",
    "        print('- Robinhood Gain & Loss: ' + '\\033[1m' + str(int(gain_loss)) + '\\033[0m', 'USD')\n",
    "        print('___________________________________\\n')\n",
    "        print('> Total Gain & Loss: ' + '\\033[1m' + str(int(sec_gain_loss + gain_loss)) + '\\033[0m', 'USD', '\\n')\n",
    "        print('> Total Asset: ' + '\\033[1m' + str(int(sec_values + sum(mkv))) + '\\033[0m', 'USD', '\\n')\n",
    "\n",
    "        # plot - Robinhood\n",
    "        portfolio_symbols = robin.reset_index()['index'].values\n",
    "        df = pd.DataFrame({'index': portfolio_symbols, 'change' : percentage})\n",
    "\n",
    "        df.to_csv('robinhood_gain_loss.csv', index = False)\n",
    "        gain_loss_plot(df)\n",
    "\n",
    "        # plot only alive tickers\n",
    "        df_plot = []\n",
    "        for ticker, change in df.values:\n",
    "            if ticker not in dead_stock:\n",
    "                df_plot.append([ticker, change])       \n",
    "                \n",
    "        if len(df_plot) != 0:\n",
    "            df = pd.DataFrame(np.array(df_plot), columns = ['index', 'change'])\n",
    "            df.change = df.change.astype(float)\n",
    "            print('\\n\\n', '\\033[1m' + 'Active Tickers' + '\\033[0m', '\\n')\n",
    "            gain_loss_plot(df)\n",
    "    \n",
    "    except:\n",
    "        print('\\nExcept >>> robin_plot()\\n\\n')\n",
    "        alarm()\n",
    "        robin_plot()\n",
    "    \n",
    "\n",
    "def volume_speed(ticker):\n",
    "    with open(\"averageline.txt\", 'r') as f:\n",
    "        averageline = [line.rstrip('\\n') for line in f]\n",
    "        averageline = int(averageline[0])\n",
    "\n",
    "    # Market hour - 09:30 ~ 16:00, 6.5 hours or 390 minutes\n",
    "    minutepassed = timechecknow()\n",
    "\n",
    "    df = yfinance_df(ticker)\n",
    "    drp = df.loc[~(df!=0).all(axis=1)].index.values\n",
    "    df = df.drop(drp)  \n",
    "\n",
    "    # volume speed check    \n",
    "    ticker_volume_mean = df.tail(averageline)['Volume'].mean()   \n",
    "    ticker_volume_current = df.tail(1)['Volume'].mean()\n",
    "    current_speed_volume = ticker_volume_current / minutepassed\n",
    "    mean_speed_volume = ticker_volume_mean / 390\n",
    "    volumeSpeed_vs_mean = current_speed_volume / mean_speed_volume \n",
    "\n",
    "    return current_speed_volume, volumeSpeed_vs_mean\n",
    "\n",
    "        \n",
    "def volume_vs_sign(stock):\n",
    "    \n",
    "    time.sleep(0.3)\n",
    "    # remove candle_analysis.csv for the previous date\n",
    "    candel_exists = exists('candle_analysis_{}_%s.csv'.format(stock) %mostrecentdate)\n",
    "    if candel_exists == True:\n",
    "        print('The old version of the candle will be removed from candle_gathering(). Ticker: ', stock)\n",
    "        import os\n",
    "        os.remove('candle_analysis_{}_%s.csv'.format(stock) %mostrecentdate)\n",
    "    else:\n",
    "        pass\n",
    "    candel_exists = exists('candle_analysis_{}_%s.csv'.format(stock) %currentdate)\n",
    "    if candel_exists == True:\n",
    "        pass\n",
    "    else:\n",
    "        # create initial data for today\n",
    "        # Market hour - 09:30 ~ 16:00, 6.5 hours or 390 minutes\n",
    "        minutepassed = timechecknow()\n",
    "        candle = yfinance_df(stock).tail(1)[['Date', 'Adj Close','Volume']]\n",
    "    \n",
    "        candle['Minutepassed'] = minutepassed\n",
    "        candle['VolumeSpeed'] = volume_speed(stock)[0]\n",
    "        candle['VolumeSpeed_vs_mean'] = volume_speed(stock)[1]\n",
    "        candle.to_csv('candle_analysis_{}_%s.csv'.format(stock) %currentdate)\n",
    "        \n",
    "    # Market hour - 09:30 ~ 16:00, 6.5 hours or 390 minutes\n",
    "    minutepassed = timechecknow()\n",
    "\n",
    "    # recent\n",
    "    candle = yfinance_df(stock).tail(1)[['Date', 'Adj Close','Volume']]\n",
    "    candle['Minutepassed'] = minutepassed\n",
    "    candle['VolumeSpeed'] = volume_speed(stock)[0]\n",
    "    candle['VolumeSpeed_vs_mean'] = volume_speed(stock)[1]\n",
    "\n",
    "    # accumulated\n",
    "    candle_accu = pd.read_csv('candle_analysis_{}_%s.csv'.format(stock) %currentdate, index_col = 0)\n",
    "    candle_accu['Date'] = pd.to_datetime(candle_accu['Date']) #type change to datetime\n",
    "\n",
    "    # merge\n",
    "    candle = pd.concat([candle_accu, candle]).reset_index(drop = True)\n",
    "    candle = candle[['Date', 'Minutepassed', 'Adj Close', 'Volume', 'VolumeSpeed', 'VolumeSpeed_vs_mean']]\n",
    "    \n",
    "    # Volume change\n",
    "    volume_change = []\n",
    "    for i in range(len(candle.Volume)):\n",
    "        if i-1 < 0:\n",
    "            volume_change.append(0)\n",
    "        else:\n",
    "            volumeChange = ((candle.Volume[i] - candle.Volume[i-1]) / candle.Volume[i-1]) * 100\n",
    "            volume_change.append(volumeChange)\n",
    "    candle['Volume_change'] = volume_change\n",
    "    \n",
    "    # Price change\n",
    "    price_change = []\n",
    "    for i in range(len(candle['Adj Close'])):\n",
    "        if i-1 < 0:\n",
    "            price_change.append(0)\n",
    "        else:\n",
    "            priceChange = ((candle['Adj Close'][i] - candle['Adj Close'][i-1]) / candle['Adj Close'][i-1]) * 100\n",
    "            price_change.append(priceChange)\n",
    "    candle['Price_change'] = price_change\n",
    "    \n",
    "    # Volume speed acceleration\n",
    "    volume_speed_change = []\n",
    "    for i in range(len(candle.VolumeSpeed)):\n",
    "        if i-1 < 0:\n",
    "            volume_speed_change.append(0)\n",
    "        else:\n",
    "            with np.errstate(divide='ignore'):\n",
    "                volumeSpeedChange = (candle.VolumeSpeed[i] - candle.VolumeSpeed[i-1]) / (candle.Minutepassed[i] - candle.Minutepassed[i-1])\n",
    "            volume_speed_change.append(volumeSpeedChange)\n",
    "    candle['Volume_speed_acceleration'] = volume_speed_change   \n",
    "    # drop non numeric rows\n",
    "    candle = candle[pd.to_numeric(candle['Volume_speed_acceleration'], errors='coerce').notnull()].reset_index(drop = True)\n",
    "    \n",
    "    # Fill 0 as 1 for the change\n",
    "    candle['Volume_change'] = candle['Volume_change'].replace(0, 0.000001)\n",
    "    candle['Price_change'] = candle['Price_change'].replace(0, 0.000001)\n",
    "\n",
    "    # Volume Change per Price Change\n",
    "    candle['VolumeC_per_PriceC'] = np.absolute(candle['Volume_change'] / candle['Price_change'])\n",
    "\n",
    "    # save\n",
    "    candle.Minutepassed.drop_duplicates(inplace = True)\n",
    "    candle.to_csv('candle_analysis_{}_%s.csv'.format(stock) %currentdate)\n",
    "\n",
    "    return candle_sign_gen(candle, stock)\n",
    "\n",
    "########################################################################################################################################################################\n",
    "def candle_sign_gen(candle, stock):\n",
    "    with open(\"averageline.txt\", 'r') as f:\n",
    "        averageline = [line.rstrip('\\n') for line in f]\n",
    "        averageline = int(averageline[0])\n",
    "    \n",
    "    actual_price_change = analysis_price(stock)[2]\n",
    "    averageline_min_price = yfinance_df(stock).tail(averageline)['Adj Close'].min()\n",
    "\n",
    "# buy-sign \n",
    "    # Volume(+++) + Price(+)\n",
    "    min_passed = candle[-1:]['Minutepassed'].mean()\n",
    "    VC_per_PC = candle[-1:]['VolumeC_per_PriceC'].mean()\n",
    "    PC = candle[-1:]['Price_change'].mean()\n",
    "    VS_vs_MEAN = candle[-1:]['VolumeSpeed_vs_mean'].mean()\n",
    "    VS_Accel = np.absolute(candle[-1:]['Volume_speed_acceleration'].mean())\n",
    "    AVG_Price = candle[-1:]['Adj Close'].mean()\n",
    "\n",
    "    if VC_per_PC >= VC_per_PC_limit_V and PC >= PC_limit_V and VS_vs_MEAN >= VS_vs_MEAN_limit and VS_Accel >= VS_Accel_limit and AVG_Price <= averageline_min_price*2:\n",
    "\n",
    "        print(f'\\n\\n===[Volume Buy Sign: {stock}]=========================================================\\n')  \n",
    "        print(current_time()+'\\n')\n",
    "        print(f' > Minute Passed: {min_passed}\\n')\n",
    "        print(f' > Actual Price Change: {actual_price_change}\\n')\n",
    "        print(f' • Volume Change vs Price Change: {VC_per_PC} >= Limit: {VC_per_PC_limit_V}\\n')\n",
    "        print(f' • Price Change: {PC} >= Limit: {PC_limit_V}\\n')\n",
    "        print(f' • Volume Speed vs Mean: {VS_vs_MEAN} >= Limit: {VS_vs_MEAN_limit}\\n')\n",
    "        print(f' • Volume Speed Acceleration: {VS_Accel} >= Limit: {VS_Accel_limit}\\n')\n",
    "        print(f' • Candle Price vs Period Min Price x2: {AVG_Price} <= {averageline_min_price*2}\\n')\n",
    "        print('==================================================================================\\n\\n\\n\\n\\n')        \n",
    "\n",
    "        return 3    \n",
    "\n",
    "# realize gain\n",
    "    elif PC >= PC_limit_V and VS_vs_MEAN >= VS_vs_MEAN_limit and VS_Accel >= VS_Accel_limit:\n",
    "        \n",
    "#         if timechecknow() < 400:            \n",
    "#             print(f'\\n\\n===[Realize Gain Sign: {stock}]===================================================\\n')  \n",
    "#             print(current_time()+'\\n')\n",
    "#             print(f' > Actual Price Change: {actual_price_change}\\n')\n",
    "#             print(f' x Volume Change vs Price Change: {VC_per_PC} >= Limit: {VC_per_PC_limit_V}\\n')\n",
    "#             print(f' • Price Change: {PC} >= Limit: {PC_limit_V}\\n')\n",
    "#             print(f' • Volume Speed vs Mean: {VS_vs_MEAN} >= Limit: {VS_vs_MEAN_limit}\\n')\n",
    "#             print(f' • Volume Speed Acceleration: {VS_Accel} >= Limit: {VS_Accel_limit}\\n')\n",
    "#             print(f' x Candle Price vs Period Min Price x2: {AVG_Price} <= {averageline_min_price*2}\\n')\n",
    "#             print('==================================================================================\\n')   \n",
    "           \n",
    "        return 2    \n",
    "\n",
    "# Volatile\n",
    "    #2 Volume(+++) + Price(-)\n",
    "    elif VC_per_PC <= VC_per_PC_limit_V and VS_vs_MEAN <= VS_vs_MEAN_limit:\n",
    "\n",
    "#         print(f'\\n\\n===[Volatile Sign: {stock}]===========================================================\\n')    \n",
    "#         print(current_time()+'\\n')\n",
    "#         print(f' > Actual Price Change: {actual_price_change}\\n')\n",
    "#         print(f' • Volume Change vs Price Change: {VC_per_PC} <= Limit: {VC_per_PC_limit_V}\\n')\n",
    "#         print(f' • Volume Speed vs Mean: {VS_vs_MEAN} <= Limit: {VS_vs_MEAN_limit}\\n')\n",
    "#         print('==================================================================================\\n')    \n",
    "\n",
    "        return 1\n",
    "\n",
    "\n",
    "# sell-sign\n",
    "    #3 Price(-, --) + Volume(++) + VolumeSpeedAccel(+) \n",
    "    elif VC_per_PC > np.sqrt(VC_per_PC_limit_V) and PC < -0.3 and VS_vs_MEAN < VS_vs_MEAN_limit and VS_Accel >= VS_Accel_limit:\n",
    "\n",
    "        message = f'Candle Alert: {stock} is short selling at -30%.'\n",
    "        webbrowser.open(\"Nuclear Launch Detected.mp3\")\n",
    "        time.sleep(2.5)\n",
    "        print('\\n\\n\\n (-)(-)(-)', message, '(-)(-)(-)\\n\\n\\n')\n",
    "        voice_message(message)\n",
    "\n",
    "        return -3\n",
    "\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    print('\\n', '\\033[1m' + 'The Engine#4 gets ready.' + '\\033[0m', '\\n\\n\\n')\n",
    "#     robin_equity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae3aed",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "hide_input": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd7008b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68e0fb70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# monitoring time count\n",
    "import time\n",
    "tstart = time.time()\n",
    "\n",
    "# portfolio monitoring start within schedules():\n",
    "def schedules():\n",
    "    global cyclecount\n",
    "    global bluechips\n",
    "    global corr\n",
    "    global proba_initial_value\n",
    "    global yf_counter\n",
    "    \n",
    "    # reset the counter checking the number of request to yfinance\n",
    "    yf_counter = 0\n",
    "\n",
    "    # cycle work starts from here\n",
    "    tend = time.time()\n",
    "    tpss = tend - tstart\n",
    "\n",
    "    cyclecount+=1\n",
    "\n",
    "    print('\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')\n",
    "    print(np.round(tpss/60), ' minutes elapsed >> {}th << cycle start'.format(cyclecount))\n",
    "    print('_____________________________________________________________________________')\n",
    "    \n",
    "    # scheduled monitoring starts\n",
    "    # time passed\n",
    "    print('\\n'+current_time()+'\\n')\n",
    "    \n",
    "    with open(f'purchased_{currentdate}.txt', 'r') as f:\n",
    "        purchased = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    with open(f'sold_{currentdate}.txt', 'r') as f:\n",
    "        sold = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    with open(\"portfolio_original.txt\", 'r') as f:\n",
    "        portfolio_original = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "    with open(\"etf_portfolio.txt\", 'r') as f:\n",
    "        etf_portfolio = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "    portfolio_original+=etf_portfolio\n",
    "        \n",
    "        \n",
    "    # sell monitor cycle check    \n",
    "    if sellmonitor_freq == 1:\n",
    "        print('\\033[1m' + '\\n\\n\\nThe number of holding stocks has reached the limit. No buy monitor executes.\\n\\n' + '\\033[0m')\n",
    "    elif sellmonitor_freq < 1:\n",
    "        message = 'Sell Monitor frequency is less than 1. Code cell breaks.'\n",
    "        print('\\n\\n\\n' + '\\033[1m' + 'Except >>> ' + message + '\\n\\n' + '\\033[0m')\n",
    "        alarm()    \n",
    "        # code cell break ###############\n",
    "        class StopExecution(Exception):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "\n",
    "        raise StopExecution\n",
    "        # code cell break ###############\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    # S&P 500 Percentage Change Check\n",
    "    snp_now = yfinance_df_setting('^GSPC').tail(2)\n",
    "    snp_now = (snp_now['Adj Close'].values[1] - snp_now['Adj Close'].values[0])/snp_now['Adj Close'].values[0]*100\n",
    "\n",
    "    # display list\n",
    "    print('\\033[1m' + f\"\\n\\n• S&P 500 Today: {np.round(snp_now,2)}%\\n\"+ '\\033[0m')    \n",
    "    print(f'• Purchased List: {purchased}\\n')\n",
    "    print(f'• Sold List:      {sold}\\n')  \n",
    "    print(\"• Sell monitoring cycle: \" + '\\033[1m' + f\"{sellmonitor_freq} cycles\\n\" + '\\033[0m')\n",
    "    print(f'• Proba Initial Value: {np.round(proba_initial_value, 2)}\\n')\n",
    "    print(f'• Min. Proba Value:    {np.round(predict_proba_value, 2)}\\n\\n\\n')\n",
    "    \n",
    "    # magic quote\n",
    "    print(magic.quote.sample(np.random.randint(1,magic.shape[0],1)[0]).values[0].strip(' '))\n",
    "    print('\\n\\n\\n')\n",
    "    \n",
    "    # Market hour - 09:30 ~ 16:00, 6.5 hours or 390 minutes\n",
    "    minutepassed = timechecknow()\n",
    "    if minutepassed > 390:  \n",
    "        \n",
    "        robin_plot() \n",
    "        \n",
    "        # after market MUST #######################\n",
    "        performance_review()        \n",
    "        buy_blacklistcheck_norm(portfolio_original)\n",
    "        ###########################################\n",
    "        \n",
    "        from os.path import exists\n",
    "        perf_review_exists = exists('perf_review %s.csv' %currentdate)\n",
    "        df = pd.read_csv('daily_summary_target_tickers.csv')\n",
    "        print(f'\\n> Date Verifying: perf_review ==================> {perf_review_exists}')\n",
    "        print(f'> Date Verifying: daily_summary_target_tickers => {df[-1:].Date.values[0] == currentdate}')\n",
    "        \n",
    "        if df[-1:].Date.values[0] == currentdate and perf_review_exists == True:\n",
    "            print('\\n\\nThe performance review data gets ready.\\n\\n\\n\\n\\n')\n",
    "        \n",
    "            message = \"\"\"\\\n",
    "Subject: The market has been closed. The Performance review data gets ready.\"\"\"\n",
    "            \n",
    "        else:\n",
    "            print('\\n\\nExcept >>> The performance review data has not been updated.')\n",
    "            \n",
    "            message = \"\"\"\\\n",
    "Subject: Except>> The market has been closed. The performance review data has not been updated.\"\"\"\n",
    "        \n",
    "        peertickers()    \n",
    "        \n",
    "        real_performance_selma()\n",
    "        \n",
    "        print(f'\\n\\n{current_time()}  The market has been closed. After market works start\\n\\n')\n",
    "        time.sleep(5)\n",
    "        \n",
    "        \n",
    "# after market works\n",
    "        rsi_portfolio()\n",
    "        \n",
    "        with open(f\"qed_monitoring_done_{currentdate}.txt\", 'w') as f:\n",
    "            f.write(str(timechecknow()))  \n",
    "        \n",
    "#         print('\\n', '\\033[1m' + '>>> Value Ticker Sorting .....' + '\\033[1m', '\\n')\n",
    "#         voice_message(\"\"\"\\\n",
    "#             Value ticker sorting starts\"\"\")\n",
    "#         value_snp_pick()\n",
    "#         print(f'\\n\\n{current_time()}  After market works have been completed. End of QED.\\n\\n')\n",
    "#         voice_message(\"\"\"\\\n",
    "#             After market works have been completed.\"\"\")\n",
    "\n",
    "#         with open(f\"qed_monitoring_done_{currentdate}.txt\", 'w') as f:\n",
    "#             f.write(str(timechecknow()))  \n",
    "            \n",
    "        ##################################### End of QED Monitoring ###########################################\n",
    "\n",
    "        \n",
    "    elif minutepassed < -60:\n",
    "        print('Standby Mode', end = '          \\r')\n",
    "        time.sleep(3600)           \n",
    "        \n",
    "    elif minutepassed < -30:\n",
    "        print('Standby Mode', end = '          \\r')\n",
    "        time.sleep(300)        \n",
    "        \n",
    "    elif minutepassed < 0:\n",
    "        print('Standby Mode', end = '          \\r')\n",
    "        time.sleep(60)          \n",
    "                \n",
    "    else:\n",
    "#         if minutepassed > 220 and minutepassed < 225:\n",
    "#             webbrowser.open(\"baron1.m4a\")\n",
    "#             sean_index_plot()\n",
    "#             real_performance_selma()\n",
    "#             sectorcheck_main(1)\n",
    "    \n",
    "        if cyclecount%30 == 0:  \n",
    "#             robin_plot()\n",
    "            exrate()  \n",
    "            super_pass_list = super_pass_filtering(superpass_buysign_lookbackdays)\n",
    "#             for stock_ticker in super_pass_list:\n",
    "#                 sector_stock_plot(stock_ticker)  \n",
    "            \n",
    "        elif cyclecount%sellmonitor_freq == 0:  # edit this when loss-cut excuted for big-stocks\n",
    "            daily_news_sentiment_all(display = 'on')\n",
    "            macrosnp() # snp500 drop % check and adjust Proba value\n",
    "            # moving average plot for holding tickers\n",
    "            for ticker in sorted(robin_login()['index'].tolist()):\n",
    "#                 try:\n",
    "#                     sector_stock_plot(ticker)\n",
    "#                 except:\n",
    "#                     pass\n",
    "                dmv_plot(ticker)\n",
    "#                 rsi_plot(ticker)               \n",
    "            sellmonitor() #icld. robin_equity_check() to update: losscut rate, realize gain rate\n",
    "            \n",
    "        else:\n",
    "            portfolio_backtest()\n",
    "            buymonitor()\n",
    "            candle_gathering()\n",
    "               \n",
    "            with open(\"_QED Time Checker.txt\", 'w') as f:\n",
    "                f.write(str(current_time()))  \n",
    "\n",
    "                \n",
    "def main():\n",
    "    print('The engine has been initialized.\\n\\n\\n')\n",
    "    \n",
    "    ## initial modules\n",
    "#     webbrowser.open(\"baron.m4a\")\n",
    "\n",
    "    if timechecknow() < 60: ###### Adjust time here when delayed start\n",
    "        if str(yfinance_df_setting('^GSPC')[-1:].Date.values[0])[:10] == currentdate:\n",
    "            \n",
    "            voice_message(\"\"\"\\\n",
    "                Market starts\"\"\")\n",
    "            \n",
    "#             # realize gain or loss\n",
    "#             robin_plot()\n",
    "            # moving average plot for holding tickers\n",
    "            for ticker in sorted(robin_login()['index'].tolist()):\n",
    "                try:\n",
    "                    sector_stock_plot(ticker)\n",
    "                except:\n",
    "                    pass\n",
    "                dmv_plot(ticker)\n",
    "                rsi_plot(ticker)          \n",
    "                \n",
    "#             sellmonitor() #icld. robin_equity_check() to update: losscut rate, realize gain rate\n",
    "\n",
    "            # portfolio creation\n",
    "            main_creation()   # start when the current date == yFinance Date\n",
    "        \n",
    "            # proba value reset - L/T\n",
    "            proba_value_reset()\n",
    "            \n",
    "            # proba value reset - S/T\n",
    "            macrosnp()\n",
    "        \n",
    "            try:\n",
    "                snp_predict = valuecheck('^GSPC')\n",
    "            except:\n",
    "                snp_predict = valuecheck('^GSPC')\n",
    "            sean_index_plot()\n",
    "            \n",
    "            if snp_predict[1] == 1:\n",
    "                voice_message(\"\"\"\\\n",
    "                    S&P 500 will increase\"\"\")\n",
    "            elif snp_predict[1] > 0.3:\n",
    "                voice_message(\"\"\"\\\n",
    "                    S&P 500 will volatile\"\"\")        \n",
    "            else:\n",
    "                voice_message(\"\"\"\\\n",
    "                    S&P 500 will decrease\"\"\")\n",
    "            time.sleep(10)\n",
    "            \n",
    "            # parameter setting by tf\n",
    "            main_param()\n",
    "            \n",
    "        else:\n",
    "            print('> The last line != currentdate\\n')\n",
    "            if timechecknow() < -30:\n",
    "                print('Stand by Mode', end = '          \\r')\n",
    "                voice_message(\"\"\"\\\n",
    "                              Opening bell stand by for 30 minutes\"\"\")                \n",
    "                time.sleep(1800)\n",
    "            elif timechecknow() < -10:\n",
    "                print('Stand by Mode', end = '          \\r')\n",
    "                voice_message(\"\"\"\\\n",
    "                              Opening bell stand by for 10 minutes\"\"\")                \n",
    "                time.sleep(600)\n",
    "            elif timechecknow() < -5:\n",
    "                print('Stand by Mode', end = '          \\r')\n",
    "                voice_message(\"\"\"\\\n",
    "                              Opening bell stand by for 5 minutes\"\"\")                \n",
    "                time.sleep(300)\n",
    "            else:\n",
    "                print('Stand by Mode', end = '          \\r')\n",
    "                time.sleep(60)   \n",
    "            main()\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    # initial secter valuation\n",
    "    sector_valuation_ticker = pd.read_csv('sector_valuation.csv')\n",
    "    if sector_valuation_ticker[-1:].Date.values[0] != currentdate:\n",
    "        sectorcheck_main(1)\n",
    "        \n",
    "        # initial fbeta score check with buymonitor_whitelist  \n",
    "        with open(\"portfolio.txt\", 'r') as f:\n",
    "            portfolio = [line.rstrip('\\n') for line in f]\n",
    "        prevaluation_tickers = []\n",
    "        if len(portfolio) >= 5:\n",
    "            for i in range(5):\n",
    "                prevaluation_tickers.append(random.choice(portfolio))\n",
    "        valuation(prevaluation_tickers)\n",
    "        \n",
    "        # delete past files\n",
    "        from os.path import exists\n",
    "        predict_valuation_today_exists = exists('predict_valuation_today %s.csv' %mostrecentdate)\n",
    "        if predict_valuation_today_exists == True:\n",
    "            import os\n",
    "            os.remove('predict_valuation_today %s.csv' %mostrecentdate)\n",
    "        else:\n",
    "            pass    \n",
    "        \n",
    "        peertickers()  \n",
    "        exrate()\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #     alarm_open(3)    \n",
    "    \n",
    "    if upcoming_remain == 0:\n",
    "        voice_message(f\"\"\"\\\n",
    "        We have an event scheduled today. Be careful Sean\n",
    "        \"\"\")\n",
    "        time.sleep(5)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    voice_message(\"\"\"\\\n",
    "        Engine starts\"\"\")\n",
    "    \n",
    "#     # realize gain or loss\n",
    "#     robin_plot()\n",
    "    daily_news_sentiment_all(display = 'on')\n",
    "    \n",
    "    while True:\n",
    "        # check if monitoring done\n",
    "        from os.path import exists\n",
    "        qed_monitoring_done = exists(f\"qed_monitoring_done_{currentdate}.txt\")\n",
    "        \n",
    "        if qed_monitoring_done == True:\n",
    "            break\n",
    "        else:\n",
    "            schedules()\n",
    "\n",
    "            \n",
    "# count\n",
    "cyclecount = 0\n",
    "day_trading_count = 0\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    if day_check() != 'Saturday' and day_check() != 'Sunday' and timechecknow() < 420:\n",
    "        \n",
    "#         webbrowser.open(\"Nuclear Launch Detected.mp3\")\n",
    "#         alarm_open(1)\n",
    "\n",
    "        voice_message(\"\"\"\\\n",
    "            Engine gets ready\"\"\")\n",
    "    \n",
    "        exrate()\n",
    "    \n",
    "        # trade list update\n",
    "        blank = 'blank'\n",
    "        trade_list('purchased', blank)\n",
    "        trade_list('sold', blank)\n",
    "\n",
    "        with open(f'purchased_{currentdate}.txt', 'r') as f:\n",
    "            purchased = [line.rstrip('\\n') for line in f]\n",
    "\n",
    "        with open(f'sold_{currentdate}.txt', 'r') as f:\n",
    "            sold = [line.rstrip('\\n') for line in f]\n",
    "        \n",
    "        print(f'\\n\\n• Purchased List: {purchased}\\n')\n",
    "        print(f'• Sold List:      {sold}\\n\\n')  \n",
    "      \n",
    "        # proba value reset - L/T\n",
    "        proba_value_reset()\n",
    "    \n",
    "        # execute when delayed start\n",
    "#         main_creation()     # <<<<<<<<<<<<<<<<<   \n",
    "#         sectorcheck_main(1) # <<<<<<<<<<<<<<<<<\n",
    "\n",
    "        main()\n",
    "        \n",
    "    else:\n",
    "        print('\\n', '\\033[1m' + 'The market is closed today.' + '\\033[0m', '\\n')\n",
    "        \n",
    "# import FinanceDataReader as fdr\n",
    "# fdr.DataReader('GOOGL').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a63ba2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9cbb94d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# with open(\"portfolio_original.txt\", 'r') as f:\n",
    "#     portfolio_original = [line.rstrip('\\n') for line in f] \n",
    "# performance_review()        \n",
    "# buy_blacklistcheck_norm(portfolio_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a12e3",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9658e23",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "After Market Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f73c0e72",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if day_check() != 'Saturday' and day_check() != 'Sunday' and timechecknow() > 390:\n",
    "    # plt.style.use('classic')\n",
    "    yf_day_plus = datetime.today() + timedelta(days = 1)\n",
    "    yf_day_plus = str(yf_day_plus)[:10]\n",
    "    \n",
    "else:\n",
    "    print('Market is not closed. Code Cell Break')\n",
    "    alarm()\n",
    "    \n",
    "    # code cell break ###############\n",
    "    class StopExecution(Exception):\n",
    "        def _render_traceback_(self):\n",
    "            pass\n",
    "\n",
    "    raise StopExecution\n",
    "    # code cell break ###############\n",
    "    \n",
    "\n",
    "\n",
    "# Original #####################################################################################################################\n",
    "\n",
    "# def stock_data(ticker):\n",
    "#     try:\n",
    "#         time.sleep(0.1)\n",
    "#         start_date = str(datetime.today() - timedelta(days = 300))[:10]\n",
    "#         return data.DataReader(ticker, data_source, start_date, retry_count=10).reset_index()\n",
    "#     except:\n",
    "#         return yf.download(ticker, end = yf_day_plus, prepost = True, progress=False, show_errors=False).reset_index()\n",
    "#         pass\n",
    "    \n",
    "################################################################################################################################\n",
    "    \n",
    "# Sub ##########################################################################################################################\n",
    "    \n",
    "# def yfinance_clean(df):\n",
    "#     find = df.all().reset_index().rename(columns = {0:'result'}).set_index('result').reset_index()\n",
    "#     find = find[find['result'] == False]['index'].values.tolist()\n",
    "#     for i in find:\n",
    "#         df[i] = df[i].replace(0, df[i].tail(20).mean())\n",
    "#     return df\n",
    "    \n",
    "def stock_data(ticker):\n",
    "\n",
    "    try:\n",
    "        start_date = str(datetime.today() - timedelta(days = 300))[:10]   \n",
    "        df = yf.download(ticker, start = start_date, end = yf_day_plus, prepost = True, progress=False).reset_index()\n",
    "        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "        df['Date'] = df.Date.astype('datetime64[ns]')        \n",
    "#         return yfinance_clean(df)  \n",
    "        return df\n",
    "\n",
    "    except:\n",
    "        return stock_data(ticker)\n",
    "   \n",
    "################################################################################################################################\n",
    "  \n",
    "\n",
    "\n",
    "# code cell warpping\n",
    "from IPython.display import HTML\n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Modules Imported\"></form>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4f3f1a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3505d80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Original #####################################################################################################################\n",
    "\n",
    "# def yfinance_df_setting(ticker):\n",
    "#     return data.DataReader(ticker, data_source, start_date).reset_index()\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "# Sub ##########################################################################################################################\n",
    "    \n",
    "def yfinance_df_setting(ticker):\n",
    "    try:\n",
    "        # start date is since 1970, which is default value\n",
    "        start_date = '1970-01-01'\n",
    "        df = yf.download(ticker, start = start_date, end = yf_day_plus, prepost = True, progress=False, show_errors=False).reset_index()\n",
    "        df['Date'] = df['Date'].dt.strftime('%Y-%m-%d')\n",
    "        df['Date'] = df.Date.astype('datetime64[ns]')\n",
    "        return df\n",
    "#         return yfinance_clean(df)  # when Date type is datetime\n",
    "    except ValueError:\n",
    "        return data.DataReader(ticker, data_source, start_date, retry_count=100, pause=0.5).reset_index()\n",
    "        pass\n",
    "\n",
    "################################################################################################################################\n",
    "\n",
    "def robin_equity_check():\n",
    "    login = rs.login('aicpasean@gmail.com', sean.robin_api())\n",
    "    my_stocks = rs.build_holdings()\n",
    "    rs.logout()\n",
    "    robin = pd.DataFrame(my_stocks)\n",
    "    robin = robin.T.reset_index()\n",
    "    vars = ['price', 'quantity', 'average_buy_price', 'equity', 'percent_change', \n",
    "            'intraday_percent_change', 'equity_change', 'pe_ratio', 'percentage']\n",
    "    for var in vars:\n",
    "        robin[var] = pd.to_numeric(robin[var])\n",
    "    robin['quantity'] = robin['quantity'].astype(int)\n",
    "    robin.to_csv('robin_all.csv', index = False)\n",
    "    return robin\n",
    "     \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    with open(\"mostrecentdate.txt\", 'r') as f:\n",
    "        mostrecentdate = int([line.rstrip('\\n') for line in f][0])\n",
    "    with open(\"currentdate.txt\", 'r') as f:\n",
    "        currentdate = int([line.rstrip('\\n') for line in f][0])\n",
    "    with open(\"averageline.txt\", 'r') as f:\n",
    "        averageline = [line.rstrip('\\n') for line in f]\n",
    "        averageline = int(averageline[0])\n",
    "    with open(\"portfolio_original.txt\", 'r') as f:\n",
    "        portfolio = [line.rstrip('\\n') for line in f]\n",
    "    with open(\"buymonitor_blacklist_original.txt\", 'r') as f:\n",
    "        buymonitor_blacklist = [line.rstrip('\\n') for line in f]   \n",
    "\n",
    "    mostrecentdate = str(mostrecentdate)[0:4]+'-'+str(mostrecentdate)[4:6]+'-'+str(mostrecentdate)[6:8]\n",
    "    currentdate = str(currentdate)[0:4]+'-'+str(currentdate)[4:6]+'-'+str(currentdate)[6:8]\n",
    "\n",
    "    from os.path import exists\n",
    "    perf_review_exists = exists('perf_review %s.csv' %currentdate)\n",
    "    df = pd.read_csv('daily_summary_target_tickers.csv')\n",
    "    \n",
    "    print(f'> Date Verifying: perf_review ==================> {perf_review_exists}')\n",
    "    print(f'> Date Verifying: daily_summary_target_tickers => {df[-1:].Date.values[0] == currentdate}')\n",
    "    \n",
    "    print('\\n\\n • mostrecentdate: ', '\\033[1m' + f\"{mostrecentdate}\" + '\\033[0m')\n",
    "    print('\\n • currentdate:    ', '\\033[1m' + f\"{currentdate}\" + '\\033[0m')\n",
    "    \n",
    "    if df[-1:].Date.values[0] == currentdate and perf_review_exists == True:\n",
    "        df = df[df['Date'] == currentdate]\n",
    "        df = df.tail(df['index'].nunique())[['index', 'Change', 'Today', 'Yesterday', 'Average']].sort_values('Change', ascending = False).reset_index(drop=True)\n",
    "        srank = pd.read_csv('srank.csv')\n",
    "        srank = srank[['Symbol']].reset_index()\n",
    "        srank = srank.rename(columns = {'index' : 'Initial_Rank'})\n",
    "        result = pd.merge(df, srank, how = 'inner', left_on = 'index', right_on = 'Symbol').reset_index()\n",
    "        result = result.rename(columns = {'level_0' : 'Final_Rank'})\n",
    "        result['Rank_Gap'] = result['Initial_Rank'] - result['Final_Rank'] \n",
    "        result_review = result[['Symbol', 'Change', 'Final_Rank', 'Initial_Rank', 'Rank_Gap', 'Today', 'Yesterday', 'Average']]\n",
    "        \n",
    "    else:\n",
    "        print('Except >>> Performance data has not been updated.')\n",
    "        print('Code cell break')\n",
    "              \n",
    "        # code cell break ###############\n",
    "        class StopExecution(Exception):\n",
    "            def _render_traceback_(self):\n",
    "                pass\n",
    "\n",
    "        raise StopExecution\n",
    "        # code cell break ###############"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23a50b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "## Percentage Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c296bf3c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "snp_today = yfinance_df_setting('^GSPC').tail(2)\n",
    "snp_today = (snp_today['Adj Close'].values[1] - snp_today['Adj Close'].values[0])/snp_today['Adj Close'].values[0]*100\n",
    "print('\\033[1m' + f\"\\nS&P 500 Today: {np.round(snp_today,2)}%\"+ '\\033[0m')\n",
    "\n",
    "plt.figure(figsize = (16,18))\n",
    "gain = df[df['Change'] > 0].sort_values('Change')\n",
    "loss = df[df['Change'] < 0].sort_values('Change')\n",
    "\n",
    "plt.barh(loss['index'], loss['Change'], alpha = 0.95, color = sb.color_palette()[3])\n",
    "plt.barh(gain['index'], gain['Change'], alpha = 0.95, color = sb.color_palette()[0])\n",
    "\n",
    "plt.axvline(x = snp_today, color = 'black', linewidth = 2)\n",
    "\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c573773",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "### S&P 500 Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4d77c9b5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = '^GSPC'\n",
    "rsi_plot(i)\n",
    "print('\\n\\n\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f53b5b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8b65f1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "### Top Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08b9c7a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_perf_tickers = df[df['Change'] > 0]['index'].tolist()\n",
    "top_perf_tickers = [i for i in top_perf_tickers if i not in inv_tickers]\n",
    "top_perf_tickers = top_perf_tickers[:3]\n",
    "bottom_perf_tickers = df[df['Change'] < 0]['index'].tolist()\n",
    "bottom_perf_tickers = [i for i in bottom_perf_tickers if i not in inv_tickers]\n",
    "bottom_perf_tickers = bottom_perf_tickers[-3:]\n",
    "\n",
    "if len(top_perf_tickers) != 0:\n",
    "    for ticker in top_perf_tickers:\n",
    "        try:\n",
    "            rsi_plot(ticker)\n",
    "\n",
    "            try:\n",
    "                sector_stock_plot(ticker)\n",
    "            except:\n",
    "                print('\\033[1m' + '\\nExcept >>> Sector does not exist' + '\\033[0m')\n",
    "                pass\n",
    "\n",
    "            volume_price_period_plot(ticker)\n",
    "\n",
    "            try:\n",
    "                candle_review(ticker)\n",
    "            except:\n",
    "                print('\\033[1m' + 'Except >>> Candle does not exist' + '\\033[0m')\n",
    "                pass\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42c3776",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d294f9b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "### Bottom Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6244c6f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if len(bottom_perf_tickers) != 0:\n",
    "    for ticker in bottom_perf_tickers:\n",
    "        try:\n",
    "            rsi_plot(ticker)\n",
    "            try:\n",
    "                sector_stock_plot(ticker)\n",
    "            except:\n",
    "                print('\\033[1m' + '\\nExcept >>> Sector does not exist' + '\\033[0m')\n",
    "                pass\n",
    "\n",
    "            volume_price_period_plot(ticker)\n",
    "\n",
    "            try:\n",
    "                candle_review(ticker)\n",
    "            except:\n",
    "                print('\\033[1m' + 'Except >>> Candle does not exist' + '\\033[0m')\n",
    "                pass    \n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322eecd9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "### Inverse Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87d2567e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ticker in inv_tickers:\n",
    "    try:\n",
    "        rsi_plot(ticker)\n",
    "        try:\n",
    "            sector_stock_plot(ticker)\n",
    "        except:\n",
    "            print('\\033[1m' + '\\nExcept >>> Sector does not exist' + '\\033[0m')  \n",
    "            pass\n",
    "\n",
    "        volume_price_period_plot(ticker)\n",
    "\n",
    "        try:\n",
    "            candle_review(ticker)\n",
    "        except:\n",
    "            print('\\033[1m' + 'Except >>> Candle does not exist' + '\\033[0m')\n",
    "            pass    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21968a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6665465",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "## Sean Rank Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0292a1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('\\033[1m' + 'Portfolio Score Gap Review (excl. ETF)' + '\\033[0m')\n",
    "result_gap = result_review.sort_values('Rank_Gap', ascending = False)\n",
    "df = result_gap\n",
    "\n",
    "plt.figure(figsize = (20,9))\n",
    "plt.bar(df['Symbol'], df['Rank_Gap'], alpha = 0.95, color = sb.color_palette()[0])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65919680",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a430e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "## **Portfolio Return**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee21d4e1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('daily_summary_target_tickers.csv')\n",
    "df = df.groupby(['Date']).mean().reset_index()[['Date', 'Change']]\n",
    "df['Date'] = df['Date'].astype('datetime64[ns]')\n",
    "\n",
    "snp = yfinance_df_setting('^GSPC')[['Date', 'Adj Close']]\n",
    "\n",
    "# delete rows during off-days: i.g, vacation # \n",
    "# snp = snp.drop(index = snp.query('Date > \"2022-07-13\" & Date < \"2022-08-19\"').index).reset_index(drop = True)\n",
    "#############################################################################\n",
    "\n",
    "snp_change = []\n",
    "for i in range(snp.shape[0]):\n",
    "    if i == 0:\n",
    "        snp_change.append(0)\n",
    "    else:\n",
    "        snp_change.append((snp['Adj Close'][i] - snp['Adj Close'][i-1])/snp['Adj Close'][i-1]*100)\n",
    "snp['snp_change'] = snp_change\n",
    "\n",
    "portfolio_performance = pd.merge(df, snp, how = 'inner', on = 'Date')\n",
    "portfolio_performance.to_csv('portfolio_performance.csv', index = False)\n",
    "\n",
    "portfolio_performance = portfolio_performance[-averageline:]\n",
    "\n",
    "vola_port = np.std(portfolio_performance['Change']) * np.sqrt(portfolio_performance.shape[0]) / 100\n",
    "vola_snp = np.std(portfolio_performance['snp_change']) * np.sqrt(portfolio_performance.shape[0]) / 100\n",
    "vola_ratio = vola_port / vola_snp\n",
    "\n",
    "portfolio_performance['Date'] = portfolio_performance['Date'].astype(str)\n",
    "plt.figure(figsize = (16,18))\n",
    "plt.subplot(2,1,2)\n",
    "plt.errorbar(x = portfolio_performance['Date'], y = portfolio_performance['Change'], label = 'Portfolio +/-')\n",
    "plt.errorbar(x = portfolio_performance['Date'], y = portfolio_performance['snp_change'], label = 'S&P 500 +/-')\n",
    "plt.title('Portfolio vs S&P 500')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend()\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "print(f\"\\nTotol Period: {portfolio_performance.shape[0]} days\")\n",
    "print(\"______________________________________\")\n",
    "print(f\"\\n• Volatility Portfolio:        {'%.5f' %vola_port}\\n\")\n",
    "print(f\"• Volatility S&P 500:          {'%.5f' %vola_snp}\")\n",
    "print(f\"\\n• Volatility Portfolio to S&P: \" + '\\033[1m' + f\"{'%.3f' %vola_ratio}\" + '\\033[0m')\n",
    "print(\"______________________________________\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55182b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "> Adjust S&P(snp_change) rows by dropping indexes during off-days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65027cbc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd7897",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "## **Fbeta Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2bae9105",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "day_plus = datetime.today() - timedelta(days = 10)\n",
    "day_plus = str(day_plus)[:10]\n",
    "\n",
    "prt = pd.read_csv('predict_valuation.csv')\n",
    "prt = prt[prt['date'] >= day_plus]\n",
    "\n",
    "prt = prt.groupby(['date', 'symbol']).mean().reset_index()\n",
    "df = pd.read_csv('daily_summary_target_tickers.csv')\n",
    "df = df.groupby(['Date', 'index']).mean().reset_index()\n",
    "\n",
    "prt_result = pd.merge(df, prt, how = 'inner', left_on = 'index', right_on = 'symbol')[['index', 'Date', 'Change', 'valuation', 'up_down']].sort_values('Date').reset_index(drop = True)\n",
    "prt_result['value_score'] = prt_result['up_down']\n",
    "\n",
    "# H0: Bearish (negative)\n",
    "# H1: Bullish (positive)\n",
    "\n",
    "accuracy = []\n",
    "for i in prt_result['index'].values.tolist():\n",
    "    if prt_result[prt_result['index'] == i].Change.values[0] >= 0 and prt_result[prt_result['index'] == i].value_score.values[0] == 1:\n",
    "        accuracy.append('True_Positive')\n",
    "        \n",
    "    elif prt_result[prt_result['index'] == i].Change.values[0] < 0 and prt_result[prt_result['index'] == i].value_score.values[0] == 1:\n",
    "        accuracy.append('False_Positive') # Type I Error (Alpha) More Serious Error, Required Higher Precision than Recall\n",
    "        \n",
    "    elif prt_result[prt_result['index'] == i].Change.values[0] < 0 and prt_result[prt_result['index'] == i].value_score.values[0] < 1:\n",
    "        accuracy.append('True_Negative')\n",
    "        \n",
    "    elif prt_result[prt_result['index'] == i].Change.values[0] >= 0 and prt_result[prt_result['index'] == i].value_score.values[0] < 1:\n",
    "        accuracy.append('False_Negative') # Type II Error (Beta) \n",
    "    \n",
    "    else:\n",
    "        accuracy.append('NA')\n",
    "prt_result['accuracy'] = accuracy\n",
    "\n",
    "confusion_matrix = prt_result.accuracy.value_counts()\n",
    "\n",
    "FN = confusion_matrix[0]\n",
    "TN = confusion_matrix[1]\n",
    "TP = confusion_matrix[2]\n",
    "FP = confusion_matrix[3]\n",
    "\n",
    "Accuracy = (TP + TN) / confusion_matrix.sum() * 100\n",
    "Precision = TP / (TP + FP) * 100\n",
    "Specificity = TN / (FP + TN) * 100\n",
    "Error_Rate = (FP + FN) / confusion_matrix.sum() * 100\n",
    "Recall = TP / (TP + FN) * 100\n",
    "Fbeta = (1+(0.5*0.5))*(Precision*Recall)/((0.5*0.5*Precision)+Recall)\n",
    "\n",
    "lb = pd.read_csv('predict_valuation.csv').up_down.mean()*100\n",
    "print(f\"predict_valuation - up_down Mean() *Classifier: {'%.2f'%lb}%\\n\")\n",
    "\n",
    "print(\"_____________________\")\n",
    "print(f\"\\n• Accuracy:    \" + '\\033[1m' + f\"{'%.2f' %Accuracy}%\" + '\\033[0m')\n",
    "print(f\"• Error Rate:  \" + '\\033[1m' + f\"{'%.2f' %Error_Rate}%\" + '\\033[0m')\n",
    "\n",
    "print(f\"\\n• Precision*:  \" + '\\033[1m' + f\"{'%.2f' %Precision}%\" + '\\033[0m')\n",
    "print(f\"• Specificity: \" + '\\033[1m' + f\"{'%.2f' %Specificity}%\" + '\\033[0m')\n",
    "\n",
    "print(f\"• Recall:      \" + '\\033[1m' + f\"{'%.2f' %Recall}%\" + '\\033[0m')\n",
    "print(f\"\\n• Fbeta:       \" + '\\033[1m' + f\"{'%.2f' %Fbeta}%\" + '\\033[0m')\n",
    "print(\"_____________________\")\n",
    "\n",
    "pd.DataFrame(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ec3516d0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prt_score_today = {}\n",
    "\n",
    "prt_score_today['Date'] = currentdate\n",
    "prt_score_today['Accuracy'] = Accuracy\n",
    "prt_score_today['Precision'] = Precision\n",
    "prt_score_today['Specificity'] = Specificity\n",
    "prt_score_today['Error_Rate'] = Error_Rate\n",
    "prt_score_today['Recall'] = Recall\n",
    "prt_score_today['Fbeta'] = Fbeta\n",
    "\n",
    "prt_score_today = pd.DataFrame(prt_score_today, index = ['Score']).reset_index(drop = True)\n",
    "prt_score_today['Date'] = prt_score_today['Date'].astype('datetime64[ns]')\n",
    "prt_score_today.to_csv(f\"fbeta_score_today_{currentdate}.csv\", index = False)\n",
    "\n",
    "# combine previous with current \n",
    "from os.path import exists\n",
    "sourcedata_exists = exists(f\"fbeta_score_{mostrecentdate}.csv\")\n",
    "if sourcedata_exists == True:\n",
    "    prt_score_mostrecent = pd.read_csv(f\"fbeta_score_{mostrecentdate}.csv\")\n",
    "    prt_score_current = pd.read_csv(f\"fbeta_score_today_{currentdate}.csv\")\n",
    "    prt_score = pd.concat([prt_score_mostrecent, prt_score_current], ignore_index=True)\n",
    "    prt_score = prt_score.drop_duplicates().sort_values('Date').reset_index(drop = True)\n",
    "    # export\n",
    "    prt_score.to_csv(f\"fbeta_score_{currentdate}.csv\", index = False)\n",
    "\n",
    "    import os\n",
    "    os.remove(f\"fbeta_score_{mostrecentdate}.csv\")\n",
    "    os.remove(f\"fbeta_score_today_{currentdate}.csv\")\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# import \n",
    "prt_score = pd.read_csv(f\"fbeta_score_{currentdate}.csv\")\n",
    "prt_score['Date'] = prt_score['Date'].astype(str)\n",
    "prt_score = prt_score.sort_values('Date').reset_index(drop = True)\n",
    "\n",
    "# plot\n",
    "prt_score = prt_score[-averageline:]\n",
    "\n",
    "plt.figure(figsize = (16,30))\n",
    "plt.subplot(3,1,1)\n",
    "plt.errorbar(x = prt_score['Date'], y = prt_score['Accuracy'])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('Accuracy Score (%)')\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.errorbar(x = prt_score['Date'], y = prt_score['Precision'])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('Precision (%)')\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.errorbar(x = prt_score['Date'], y = prt_score['Fbeta'])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.title('Fbeta (%)')\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# import webbrowser\n",
    "# webbrowser.open(\"assist_end.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68274680",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c51a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "## Asset Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b236ec80",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "### Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52907d31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "asset_review = pd.read_csv('asset_review %s.csv' %currentdate)\n",
    "\n",
    "# re-excute 'Day_Change' for the possible miss-calculation\n",
    "perf_review = pd.read_csv('perf_review %s.csv' %currentdate)\n",
    "perf_review = perf_review.sort_values(['index', 'date']).reset_index(drop = True)\n",
    "\n",
    "day_change = []\n",
    "for i in range(len(perf_review.index)):\n",
    "    if i == 0:\n",
    "        day_change.append(0)\n",
    "    elif perf_review['index'][i-1] == perf_review['index'][i]:\n",
    "        daychange = perf_review['percent_change'][i] - perf_review['percent_change'][i-1]\n",
    "        day_change.append(daychange)\n",
    "    elif perf_review['index'][i-1] != perf_review['index'][i]:\n",
    "        day_change.append(0)\n",
    "    else:\n",
    "        pass\n",
    "perf_review['day_change'] = day_change\n",
    "perf_review['date'] = perf_review['date'].astype('datetime64[ns]')\n",
    "\n",
    "perf_review.to_csv('perf_review %s.csv' %currentdate, index = False) \n",
    "perf_review.to_csv('./Backup_perf_review/perf_review %s.csv' %currentdate, index = False) \n",
    "\n",
    "print('Data has been imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e920542",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Total Equity Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d97922b1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "total_gain_loss = asset_review.groupby('date').sum()[['equity', 'equity_change', 'tax_shorterm']].reset_index()\n",
    "total_gain_loss = total_gain_loss.tail(20)\n",
    "\n",
    "print('\\nTotal Equity (USD):        '+'\\033[1m' + str('%.2f'%total_gain_loss.tail(1).equity.values[0]) + '\\033[0m', '\\n')\n",
    "print('Total Net Equity (USD):    '+'\\033[1m' + str('%.2f'%(total_gain_loss.tail(1).equity.values[0]*0.9)) + '\\033[0m', '\\n')\n",
    "print('Total Equity Change (USD): '+'\\033[1m' + str('%.2f'%total_gain_loss.tail(1).equity_change.values[0]) + '\\033[0m', '\\n')\n",
    "plt.figure(figsize = (16,9))\n",
    "\n",
    "plt.errorbar(x = total_gain_loss['date'], y = total_gain_loss['equity'])\n",
    "plt.bar(total_gain_loss['date'], total_gain_loss['equity'], alpha = 0.1, label = 'Total Amount')\n",
    "\n",
    "plt.errorbar(x = total_gain_loss['date'], y = total_gain_loss['equity_change'])\n",
    "plt.bar(total_gain_loss['date'], total_gain_loss['equity_change'], alpha = 0.1, label = 'Gain & Loss', color = sb.color_palette()[1])\n",
    "\n",
    "plt.bar(total_gain_loss['date'], total_gain_loss['tax_shorterm'], alpha = 0.1, label = 'Short-Term Tax', color = sb.color_palette()[3])\n",
    "\n",
    "plt.xticks(rotation = 90)\n",
    "plt.legend()\n",
    "# plt.yscale('log')\n",
    "plt.title('Total Equity Change (USD)')\n",
    "\n",
    "# for i in range(total_gain_loss['equity'].shape[0]):\n",
    "#     count = total_gain_loss['equity'][i]\n",
    "#     plt.text(i, count + 10000, int(count), ha = 'center', va = 'top');  \n",
    "    \n",
    "# for i in range(total_gain_loss['equity_change'].shape[0]):\n",
    "#     count = total_gain_loss['equity_change'][i]\n",
    "#     plt.text(i, count + 10000, int(count), ha = 'center', va = 'top');  \n",
    "    \n",
    "# for i in range(total_gain_loss['tax_shorterm'].shape[0]):\n",
    "#     count = total_gain_loss['tax_shorterm'][i]\n",
    "#     plt.text(i, count + 10000, int(count), ha = 'center', va = 'top');      \n",
    "\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f12fcd",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Total Equity Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1615132b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "ticker_gain_loss = asset_review[asset_review['date'] == currentdate].sort_values('equity', ascending = False).reset_index(drop = True)\n",
    "\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.bar(ticker_gain_loss['index'], ticker_gain_loss['equity'], alpha = 0.85, label = 'Equity')\n",
    "plt.bar(ticker_gain_loss['index'], ticker_gain_loss['tax_shorterm'], alpha = 0.35, label = 'Short-Term Tax')\n",
    "\n",
    "plt.title('Total Equity Breakdown (USD)')\n",
    "\n",
    "for i in range(ticker_gain_loss['index'].shape[0]):\n",
    "    count = ticker_gain_loss['equity'][i]\n",
    "    plt.text(i, count + 7000, int(count), ha = 'center', va = 'top');  \n",
    "\n",
    "for i in range(ticker_gain_loss['index'].shape[0]):\n",
    "    count = ticker_gain_loss['tax_shorterm'][i]\n",
    "    plt.text(i, count, int(count), ha = 'center', va = 'top');      \n",
    "    \n",
    "plt.legend()\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4e945",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Equity Breardown Gain & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5e19c0b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "ticker_gain_loss = ticker_gain_loss.sort_values('percent_change',ascending = False).reset_index(drop = True)\n",
    "\n",
    "plt.bar(ticker_gain_loss['index'], ticker_gain_loss['percent_change'], alpha = 0.95, color = sb.color_palette()[0])\n",
    "plt.title('Total Return(%)')\n",
    "\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "for i in range(ticker_gain_loss['index'].shape[0]):\n",
    "    count = ticker_gain_loss['percent_change'][i]\n",
    "    pct_string = '{:0.1f}%'.format(count)\n",
    "    plt.text(i, count, pct_string, ha = 'center', va = 'top'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9950fa",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Holding Period (days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "127254f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if day_check() != 'Saturday' and day_check() != 'Sunday' and timechecknow() > 390:\n",
    "    # plus 1 days\n",
    "    df = pd.read_csv('holding_period.csv')\n",
    "    df['days'] = df['days'] + 1 #<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# remove non-existing tickers\n",
    "holding_tickers = robin_equity_check()['index'].values.tolist()\n",
    "df = df.loc[df['index'].isin(holding_tickers)].reset_index(drop = True)\n",
    "\n",
    "# add new tickers\n",
    "for i in holding_tickers:\n",
    "    if i not in df['index'].values.tolist():\n",
    "        df.loc[df.shape[0], ['index', 'days']] = i, 1\n",
    "\n",
    "df.to_csv('holding_period.csv', index = False)\n",
    "df.to_csv('./Backup_holding_period/holding_period %s.csv' %currentdate, index = False)\n",
    "\n",
    "# plot\n",
    "df = df.loc[-df['index'].isin(dead_stock)]\n",
    "\n",
    "if len(df) != 0:\n",
    "    plt.figure(figsize = (16,9))\n",
    "    sb.barplot(x = df['days'], y = df['index']);\n",
    "    plt.axvline(x = min_holding_period, color = 'black', linewidth = 7)\n",
    "    plt.grid()\n",
    "else:\n",
    "    print('No Active Stocks')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdd86b6",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Equity Change History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82da97a3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "columns = shutil.get_terminal_size().columns\n",
    "\n",
    "tickers_as_of = pd.read_csv('robin_all.csv')['index'].values\n",
    "asset_review = asset_review.loc[asset_review['index'].isin(tickers_as_of)].reset_index(drop = True)\n",
    "asset_review['date'] = asset_review['date'].astype('datetime64[ns]')\n",
    "\n",
    "i = 1\n",
    "if len(asset_review['index'].unique()) >= 8:\n",
    "    plt.figure(figsize = [21,60])\n",
    "if len(asset_review['index'].unique()) >= 4 and len(asset_review['index'].unique()) < 8:\n",
    "    plt.figure(figsize = [21,40])\n",
    "if len(asset_review['index'].unique()) < 4:\n",
    "    plt.figure(figsize = [21,20])\n",
    "    \n",
    "print('\\033[1m'+ 'Equity-Breakdown Amount Change (USD)'.center(columns) +'\\033[0m')\n",
    "for ticker in asset_review['index'].unique():\n",
    "    plt.subplot(asset_review['index'].nunique(), 2, i)\n",
    "    plt.errorbar(x = asset_review[asset_review['index'] == ticker]['date'], y = asset_review[asset_review['index'] == ticker]['equity'])\n",
    "    plt.title(ticker)\n",
    "    plt.xticks(rotation = 45)\n",
    "#     plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83249d0f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Equity Realize Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e68728c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "columns = shutil.get_terminal_size().columns\n",
    "\n",
    "i = 1\n",
    "if len(asset_review['index'].unique()) >= 8:\n",
    "    plt.figure(figsize = [21,60])\n",
    "if len(asset_review['index'].unique()) >= 4 and len(asset_review['index'].unique()) < 8:\n",
    "    plt.figure(figsize = [21,40])\n",
    "if len(asset_review['index'].unique()) < 4:\n",
    "    plt.figure(figsize = [21,20])\n",
    "\n",
    "print('\\033[1m'+ 'Equity-Breakdown Price Change (USD)'.center(columns) +'\\033[0m')\n",
    "for ticker in asset_review['index'].unique():\n",
    "    plt.subplot(asset_review['index'].nunique(), 2, i)\n",
    "    plt.errorbar(x = asset_review[asset_review['index'] == ticker]['date'], y = asset_review[asset_review['index'] == ticker]['price'], label = 'Current Price')\n",
    "    plt.errorbar(x = asset_review[asset_review['index'] == ticker]['date'], y = asset_review[asset_review['index'] == ticker]['average_buy_price'], label = 'Buy Price')\n",
    "    plt.errorbar(x = asset_review[asset_review['index'] == ticker]['date'], y = asset_review[asset_review['index'] == ticker]['target_sell_price'], label = 'Sell Price')\n",
    "    plt.legend(bbox_to_anchor = (1.02, 0.9), loc = 'center', borderaxespad = 0)\n",
    "    plt.title(ticker)\n",
    "    plt.xticks(rotation = 45)\n",
    "#     plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8b1c0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Performance Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a18101",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### **Return vs S&P 500**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc0e11",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "`excluded dead stocks from asset_performance.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3761e047",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def asset_return_plot(asset_performance, day_cut):\n",
    "    asset_performance = asset_performance[day_cut:]\n",
    "        \n",
    "    vola_port = np.std(asset_performance['Asset Change%']) * np.sqrt(asset_performance.shape[0]) / 100\n",
    "    vola_snp = np.std(asset_performance['S&P Change% today']) * np.sqrt(asset_performance.shape[0]) / 100\n",
    "\n",
    "    return_port = np.round(asset_performance['Asset Change%'].mean(), 2)\n",
    "    return_snp = np.round(asset_performance['S&P Change% today'].mean(), 2)\n",
    "\n",
    "    print(\"\\nTotol Period: \" + '\\033[1m' + f\"{asset_performance.shape[0]} days\" + '\\033[0m')\n",
    "    print(\"______________________________________\")\n",
    "    print(f\"\\n• Volatility Asset:          {'%.3f' %vola_port}\\n\")\n",
    "    print(f\"• Volatility S&P 500:        {'%.3f' %vola_snp}\")\n",
    "    print(\"______________________________________\\n\")\n",
    "    print(\"______________________________________\")\n",
    "    print('\\033[1m' + f'\\n• Average Asset Change:     {return_port} %\\n' + '\\033[0m')\n",
    "    print(f\"• S&P 500 Performance:      {return_snp} %\")\n",
    "    print(\"______________________________________\\n\\n\\n\")\n",
    "    \n",
    "    plt.figure(figsize = (16,9))\n",
    "    asset_performance['Date'] = asset_performance['Date'].astype(str)\n",
    "    plt.errorbar(x = asset_performance['Date'], y = asset_performance['Asset Change%'], label = 'Asset +/-')\n",
    "    plt.errorbar(x = asset_performance['Date'], y = asset_performance['S&P Change% today'], label = 'S&P 500 +/-')\n",
    "    plt.title(f'Agg. Asset Change%     vs     S&P Change% Today - {np.absolute(i)} days')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.legend()\n",
    "    plt.grid(axis = 'x')\n",
    "    plt.grid(axis = 'y')\n",
    "\n",
    "df = pd.read_csv(f\"asset_review {currentdate}.csv\")\n",
    "dead_stock.append('005930.KS')\n",
    "df = df[df['index'].isin(dead_stock) == False] #<<<<<<<<<<<<<<<<<<<<<<< exclude dead stocks from asset_performance.csv\n",
    "\n",
    "percent_change_df = df.groupby(['date']).mean().reset_index()[['date', 'percent_change']]\n",
    "equity_change_df = df.groupby(['date']).sum().reset_index()[['date', 'equity_change']]\n",
    "df = pd.merge(percent_change_df, equity_change_df, how = 'inner', on = 'date')\n",
    "\n",
    "df['date'] = df['date'].astype('datetime64[ns]')\n",
    "asset_performance = pd.merge(df, snp, how = 'inner', left_on = 'date', right_on = 'Date')\n",
    "asset_performance.rename(columns = {'percent_change' : 'Agg. Change%'}, inplace = True)\n",
    "asset_performance['Asset Change%'] = asset_performance['Agg. Change%'].diff()\n",
    "asset_performance.rename(columns = {'snp_change' : 'S&P Change% today'}, inplace = True)\n",
    "asset_performance.to_csv('asset_performance.csv', index = False)\n",
    "\n",
    "dmv_list = [-120, -60, -30, -20, -10, -5]\n",
    "for i in dmv_list:\n",
    "    asset_return_plot(asset_performance, i)\n",
    "\n",
    "display(asset_performance[dmv_list[-1]:])\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c040dc0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Corr: Today's Return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199489ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "- perf_review cuts by 'averageline' - **70 days** for below cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "093c41ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "perf_review = perf_review.sort_values('date').reset_index(drop = True).tail(averageline)\n",
    "\n",
    "corr_analysis = perf_review.corr().reset_index()[['index','day_change']]\n",
    "corr_analysis['day_change'] = corr_analysis['day_change']\n",
    "corr_analysis = corr_analysis.sort_values('day_change', ascending = False).dropna().reset_index(drop = True)\n",
    "corr_analysis = corr_analysis.loc[corr_analysis['index'].isin([\n",
    "    'sean_score', \n",
    "    '1Y Perf', \n",
    "    'Price / Cash Flow',\n",
    "    '10Y Total Return', \n",
    "    '10Y Perf', \n",
    "    'P/E TTM', \n",
    "    '5Y Perf',\n",
    "    '5Y Total Return',\n",
    "    'EV / EBITDA', \n",
    "    'Price / Sales',\n",
    "    'Last Price Vs. 10D SMA', \n",
    "    'Quick Ratio', \n",
    "    '3Y Perf',\n",
    "    '3Y Total Return', \n",
    "    'Week Vol / Shares', \n",
    "    'Current Ratio',\n",
    "    'EV / Sales', \n",
    "    'P/E FWD',\n",
    "    'Revenue 3Y', \n",
    "    'Revenue FWD', \n",
    "    'EBITDA FWD', \n",
    "    'Yield FWD', \n",
    "    'Yield TTM', \n",
    "    'Revenue YoY', \n",
    "    'FCF 3Y', \n",
    "    '4Y Avg Yield',\n",
    "    'EBITDA 3Y', \n",
    "    '24M Beta', \n",
    "    'Covered Ratio', \n",
    "    'Profitability Grade_score', \n",
    "    'Return on Assets', \n",
    "    'Asset Turnover',\n",
    "    'Profit Margin',\n",
    "    'Payout Ratio', \n",
    "    'FCF Margin',\n",
    "    'Div Rate FWD', \n",
    "    'EPS 3Y', \n",
    "    '60M Beta', \n",
    "    'Div Rate TTM', \n",
    "    'EPS Revision Grade_score',\n",
    "    'Valuation Grade_score', \n",
    "    'Float %', \n",
    "    'Net Income Margin',\n",
    "    'EBIT Margin', \n",
    "    '1M Perf', \n",
    "    'Price / Book', \n",
    "    'Net Income / Employee',\n",
    "    'YTD Perf', \n",
    "    'EBITDA Margin', \n",
    "    'Momentum Grade_score', \n",
    "    'Institutional Percent', \n",
    "    'EPS FWD', \n",
    "    'Wall St. Score', \n",
    "    'PEG FWD', \n",
    "    '6M Perf', \n",
    "    'EPS Estimate',\n",
    "    'EPS Actual',\n",
    "    'Debt to FCF', \n",
    "    'Div Growth 3Y',\n",
    "    'LT Debt to Total Capital', \n",
    "    'Return on Total Capital',\n",
    "    'Div Growth 5Y', \n",
    "    'PEG TTM',\n",
    "    'Last Price Vs. 100D SMA', \n",
    "    'Last Price Vs. 200D SMA', \n",
    "    'Last Price Vs. 50D SMA', \n",
    "    'avg_score',\n",
    "    'Growth Grade_score', \n",
    "    'SA Authors Score', \n",
    "    'Debt to Equity',\n",
    "    'Return on Equity', \n",
    "    'EPS YoY', \n",
    "    'Quant Score', \n",
    "    'EBITDA YoY'])].reset_index(drop = True)\n",
    "\n",
    "plt.figure(figsize = (16,30))\n",
    "plt.bar(corr_analysis['day_change'], corr_analysis['index'], alpha = 0.85)\n",
    "\n",
    "plt.title('corr_analysis')\n",
    "# plt.xticks(rotation = 90)\n",
    "\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e619ac5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Sean Score by ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85618e74",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "- perf_review **ISIN** by the current portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "22d3ddd2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "perf_review = perf_review.loc[perf_review['Symbol'].isin(robin_equity_check()['index'].values)]\n",
    "\n",
    "g = sb.FacetGrid(data = perf_review, hue = 'Symbol', height = 10, aspect = 1.5)\n",
    "g.map(sb.regplot, 'Symbol', 'sean_score', fit_reg = False);\n",
    "g.add_legend();\n",
    "plt.title('Stock Symbol vs Sean Score\\n')\n",
    "plt.xlabel('Stock symbol')\n",
    "plt.ylabel('Sean Score')\n",
    "# plt.xscale('log')\n",
    "# plt.yscale('log')\n",
    "\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef07de2",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458a92e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d06b0f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "- Top Corr. Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "341f8d62",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "corr_analysis_topindex = corr_analysis.head(5)['index'].values.tolist()\n",
    "print(corr_analysis_topindex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893cf9b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Day return vs  1st variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0506b1be",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "perf_review['intercept'] = 1\n",
    "lm = sm.OLS(perf_review['day_change'], perf_review[['intercept', corr_analysis_topindex[0]]])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eee5133",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9));\n",
    "sb.regplot(data = perf_review , x = corr_analysis_topindex[0], y = 'day_change', scatter_kws = {'alpha': 1/2}, ci = 95);\n",
    "plt.title(f\"Day Return vs {corr_analysis_topindex[0]} (R² = {'%.2f' %results.rsquared})\")\n",
    "plt.xlabel(corr_analysis_topindex[0])\n",
    "plt.ylabel('Day Return (%)')\n",
    "# plt.ylim(-np.percentile(perf_review['day_change'], 95), np.percentile(perf_review[corr_analysis_topindex[0]], 95))\n",
    "\n",
    "plt.grid(axis='x')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c2d5e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Day return vs  2nd variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cb5b62d3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "perf_review['intercept'] = 1\n",
    "lm = sm.OLS(perf_review['day_change'], perf_review[['intercept', corr_analysis_topindex[1]]])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "752e90de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9));\n",
    "sb.regplot(data = perf_review , x = corr_analysis_topindex[1], y = 'day_change', scatter_kws = {'alpha': 1/2}, ci = 95);\n",
    "plt.title(f\"Day Return vs {corr_analysis_topindex[1]} (R² = {'%.2f' %results.rsquared})\")\n",
    "plt.xlabel(corr_analysis_topindex[1])\n",
    "plt.ylabel('Day Return (%)')\n",
    "# plt.ylim(-np.percentile(perf_review['day_change'], 95), np.percentile(perf_review[corr_analysis_topindex[0]], 95))\n",
    "\n",
    "plt.grid(axis='x')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efbed69",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Day return vs  3rd variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf60587b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "perf_review['intercept'] = 1\n",
    "lm = sm.OLS(perf_review['day_change'], perf_review[['intercept', corr_analysis_topindex[2]]])\n",
    "results = lm.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9352281a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9));\n",
    "sb.regplot(data = perf_review , x = corr_analysis_topindex[2], y = 'day_change', scatter_kws = {'alpha': 1/2}, ci = 95);\n",
    "plt.title(f\"Day Return vs {corr_analysis_topindex[2]} (R² = {'%.2f' %results.rsquared})\")\n",
    "plt.xlabel(corr_analysis_topindex[2])\n",
    "plt.ylabel('Day Return (%)')\n",
    "# plt.ylim(-np.percentile(perf_review['day_change'], 95), np.percentile(perf_review[corr_analysis_topindex[0]], 95))\n",
    "\n",
    "plt.grid(axis='x')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cf27b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e926e7fc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "#### Score Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c07f4cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "score_review = perf_review[['day_change', corr_analysis_topindex[0], corr_analysis_topindex[1], corr_analysis_topindex[2], corr_analysis_topindex[3], corr_analysis_topindex[4]]].reset_index(drop=True)\n",
    "score_review['intercept'] = 1\n",
    "lm = sm.OLS(score_review['day_change'], score_review[['intercept', corr_analysis_topindex[0], corr_analysis_topindex[1], corr_analysis_topindex[2], corr_analysis_topindex[3], corr_analysis_topindex[4]]])\n",
    "result = lm.fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "426e0cd3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "score_plot = perf_review.groupby('Symbol').mean()[[corr_analysis_topindex[0], corr_analysis_topindex[1], corr_analysis_topindex[2], corr_analysis_topindex[3], corr_analysis_topindex[4]]].reset_index()\n",
    "symbols = score_plot.Symbol.values.tolist()\n",
    "standard = score_plot.drop(columns = ['Symbol']).values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "standard_var = sc.fit_transform(standard)\n",
    "standard_var = standard_var + np.max(standard_var)\n",
    "\n",
    "score_plot = pd.DataFrame(standard_var, columns = [corr_analysis_topindex[0], corr_analysis_topindex[1], corr_analysis_topindex[2], corr_analysis_topindex[3], corr_analysis_topindex[4]])\n",
    "score_plot['Symbol'] = symbols\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('always', category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from math import pi\n",
    "\n",
    "def radarplot(ticker):\n",
    "    print('\\033[1m'+ ticker +'\\033[0m')\n",
    "\n",
    "    categories = list(score_plot)[0:5]\n",
    "    values = score_plot[score_plot['Symbol'] == ticker].mean().values.flatten().tolist()\n",
    "    values += values[:1] # repeat the first value to close the circular graph\n",
    "    angles = [n / float(len(categories)) * 2 * pi for n in range(len(categories))]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5, 5),\n",
    "                           subplot_kw=dict(polar=True))\n",
    "\n",
    "    plt.xticks(angles[:-1], categories, color='grey', size=12)\n",
    "    plt.yticks(np.arange(1, 6), ['1', '2', '3', '4', '5'],\n",
    "               color='grey', size=12)\n",
    "    plt.ylim(0, 5)\n",
    "    ax.set_rlabel_position(30)\n",
    "\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'skyblue', alpha=0.4)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "for i in score_plot.Symbol:\n",
    "    radarplot(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7df922b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "score_plot['Average'] = (score_plot[score_plot.columns.tolist()[0]] + score_plot[score_plot.columns.tolist()[1]] + score_plot[score_plot.columns.tolist()[2]] + score_plot[score_plot.columns.tolist()[3]] + score_plot[score_plot.columns.tolist()[4]])/5\n",
    "score_plot = score_plot.sort_values('Average', ascending = False)\n",
    "ex_highest_corr_index = score_plot.head(1)['Symbol'].tolist()\n",
    "with open(\"ex_highest_corr_index.txt\", 'w') as f:\n",
    "    for s in ex_highest_corr_index:\n",
    "        f.write(str(s) + '\\n')  \n",
    "\n",
    "plt.figure(figsize = (16,9));\n",
    "plt.bar(score_plot['Symbol'], score_plot['Average'])\n",
    "plt.grid(axis='x')\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719179d5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## **ETF Tickers by Return**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c648fdba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Return: ETF vs Asset (`Add New ETF`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eee0f5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true
   },
   "source": [
    "`icld. Dead Stocks`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6400b2f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "day_plus = datetime.today() - timedelta(days = 7)\n",
    "day_plus = str(day_plus)[:10]\n",
    "\n",
    "portfolios = pd.read_csv('daily_summary_target_tickers.csv')\n",
    "portfolios = portfolios[portfolios['Date'] >= day_plus]\n",
    "\n",
    "portfolios = portfolios.groupby(['Date']).mean().reset_index()[['Date', 'Change']]\n",
    "portfolios['Date'] = portfolios['Date'].astype('datetime64[ns]')\n",
    "\n",
    "def price_change(ticker, symbol):\n",
    "    price_record = yfinance_df_setting(ticker)[['Date', 'Adj Close']]\n",
    "    price_change = []\n",
    "    for i in range(price_record.shape[0]):\n",
    "        if i == 0:\n",
    "            price_change.append(0)\n",
    "        else:\n",
    "            price_change.append((price_record['Adj Close'][i] - price_record['Adj Close'][i-1])/price_record['Adj Close'][i-1]*100)\n",
    "    price_record[f\"{symbol}\"] = price_change\n",
    "    return price_record[['Date', symbol]]\n",
    "\n",
    "\n",
    "#### Add new ETF from below ########################################################################\n",
    "\n",
    "snp =                    price_change('^GSPC', 'SnP')\n",
    "Energy =                 price_change('XLE', 'Energy')\n",
    "Financials =             price_change('XLF', 'Financials')\n",
    "Real_Estate =            price_change('XLRE', 'Real Estate')\n",
    "Materials =              price_change('XLB', 'Materials')\n",
    "Information_Technology = price_change('XLK', 'Information Technology')\n",
    "Consumer_Staples =       price_change('XLP', 'Consumer Staples')\n",
    "Health_Care =            price_change('XLV', 'Health Care')\n",
    "Industrials =            price_change('XLI', 'Industrials')\n",
    "Consumer_Discretionary = price_change('XLY', 'Consumer Discretionary')\n",
    "Utilities =              price_change('XLU', 'Utilities')\n",
    "Communication_Services = price_change('XLC', 'Communication Services')\n",
    "QQQ                    = price_change('QQQ', 'Invesco QQQ')\n",
    "SPY                    = price_change('SPY', 'SPDR S&P 500')\n",
    "SPYD                   = price_change('SPYD', 'SPDR S&P 500 Dividend')\n",
    "SPYG                   = price_change('SPYG', 'SPDR S&P 500 Growth')\n",
    "SPYV                   = price_change('SPYV', 'SPDR S&P 500 Value')\n",
    "ROBO                   = price_change('ROBO', 'Robo Global Robotics and Automation Index ETF')\n",
    "XT                     = price_change('XT', 'iShares Exponential Technologies ETF')\n",
    "QTUM                   = price_change('QTUM', 'Defiance Quantum ETF')\n",
    "VOO                    = price_change('VOO', 'Vanguard 500 Index Fund')\n",
    "IVV                    = price_change('IVV', 'iShares Core S&P 500 ETF')\n",
    "SPLG                   = price_change('SPLG', 'SPDR Portfolio S&P 500 ETF')\n",
    "\n",
    "compare_index = [Energy,                 \n",
    "                 Financials,             \n",
    "                 Real_Estate,             \n",
    "                 Materials,              \n",
    "                 Information_Technology,\n",
    "                 Consumer_Staples,       \n",
    "                 Health_Care,         \n",
    "                 Industrials,        \n",
    "                 Consumer_Discretionary,\n",
    "                 Utilities,           \n",
    "                 Communication_Services, \n",
    "                 QQQ, \n",
    "                 SPY, \n",
    "                 SPYD, \n",
    "                 SPYG,\n",
    "                 SPYV,\n",
    "                 ROBO,\n",
    "                 XT,\n",
    "                 QTUM, \n",
    "                 VOO, \n",
    "                 IVV,\n",
    "                 SPLG]\n",
    "\n",
    "#### Add new ETF from above ########################################################################\n",
    "\n",
    "\n",
    "portfolio_performance = pd.merge(portfolios, snp, how = 'inner', on = 'Date')\n",
    "for index in compare_index:\n",
    "    portfolio_performance = pd.merge(portfolio_performance, index, how = 'inner', on = 'Date')\n",
    "portfolio_performance = portfolio_performance.sort_values('Date').reset_index(drop = True)\n",
    "\n",
    "# verify the last date on the portfolio_performance dataframe\n",
    "last_date_verfy = portfolio_performance[-1:].Date.astype(str).values[0]\n",
    "if last_date_verfy != currentdate:\n",
    "    print('\\nExcept >>> portfolio_performance has the last date != currentdate ==> ' + '\\033[1m' + f'{last_date_verfy}\\n' + '\\033[0m')\n",
    "    alarm()    \n",
    "    \n",
    "portfolio_performance.rename(columns = {'Change':'**Portfolio**'}, inplace = True)\n",
    "portfolio_performance.rename(columns = {'SnP':'*S&P 500*'}, inplace = True)\n",
    "\n",
    "asset_performance = pd.read_csv('asset_performance.csv')\n",
    "asset_performance['date'] = asset_performance['date'].astype('datetime64[ns]')\n",
    "asset_performance = asset_performance[['date', 'Asset Change%']]\n",
    "asset_performance.rename(columns = {'date':'Date', 'Asset Change%':'>>>>>>>> Asset Change <<<<<<<<'}, inplace = True)\n",
    "\n",
    "performance = pd.merge(asset_performance, portfolio_performance, how = 'inner', on = 'Date')\n",
    "performance.to_csv('performance_sourcedata.csv', index = False)\n",
    "performance.to_csv(f\"./Backup_performance_sourcedata/performance_sourcedata_{currentdate}.csv\", index = False)\n",
    "\n",
    "performance_stats = performance.describe().T.reset_index()\n",
    "performance_stats['Volatility'] = performance_stats['std'] * np.sqrt(performance_stats['count'][0])\n",
    "\n",
    "performance_summary = performance_stats[['index', 'mean', 'Volatility']].sort_values('mean', ascending = False).reset_index(drop = True)\n",
    "performance_summary.rename(columns = {'index': f\"{int(performance_stats['count'][0])} Days\", 'mean':'Return(avg)'}, inplace = True)\n",
    "performance_summary['Return to Vola'] = performance_summary['Return(avg)'] / performance_summary['Volatility']\n",
    "performance_summary.to_csv('performance_summary.csv', index = False)\n",
    "performance_summary.to_csv(f\"./Backup_performance_summary/performance_summary_{currentdate}.csv\", index = False)\n",
    "display(performance_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f8668a",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1e740c57",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "\n",
    "performance_summary = performance_summary.sort_values('Volatility', ascending = True).reset_index(drop = True)\n",
    "plt.bar(x = performance_summary[f\"{int(performance_stats['count'][0])} Days\"], height = performance_summary['Volatility'], color = 'green')\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel('Average Return')\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555d235",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2a4c6d3d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16,9))\n",
    "\n",
    "performance_summary = performance_summary.sort_values('Return(avg)', ascending = False).reset_index(drop = True)\n",
    "plt.bar(x = performance_summary[f\"{int(performance_stats['count'][0])} Days\"], height = performance_summary['Return(avg)'])\n",
    "plt.xticks(rotation = 90)\n",
    "plt.ylabel('Average Return')\n",
    "plt.grid(axis = 'x')\n",
    "plt.grid(axis = 'y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25c789",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### ETF Portfolio (`Add New ETF`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d44f40aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "symbol = f\"{int(performance_stats['count'][0])} Days\"\n",
    "asset = performance_summary[performance_summary[symbol] == '>>>>>>>> Asset Change <<<<<<<<']\n",
    "etf = performance_summary[performance_summary['Return(avg)'] > asset['Return(avg)'].values[0]][symbol].tolist()\n",
    "blacklist = ['**Portfolio**', '*S&P 500*']\n",
    "etf = [i for i in etf if i not in blacklist]\n",
    "\n",
    "etf_index = {}\n",
    "etf_index['*S&P 500*'] = '^GSPC'\n",
    "etf_index['Energy'] = 'XLE'\n",
    "etf_index['Financials'] = 'XLF'\n",
    "etf_index['Real Estate'] = 'XLRE'\n",
    "etf_index['Materials'] = 'XLB'\n",
    "etf_index['Information Technology'] = 'XLK'\n",
    "etf_index['Consumer Staples'] = 'XLP'\n",
    "etf_index['Health Care'] = 'XLV'\n",
    "etf_index['Industrials'] = 'XLI'\n",
    "etf_index['Consumer Discretionary'] = 'XLY'\n",
    "etf_index['Utilities'] = 'XLU'\n",
    "etf_index['Communication Services'] = 'XLC'\n",
    "etf_index['Invesco QQQ'] = 'QQQ'\n",
    "etf_index['SPDR S&P 500'] = 'SPY'\n",
    "etf_index['SPDR S&P 500 Dividend'] = 'SPYD'\n",
    "etf_index['SPDR S&P 500 Growth'] = 'SPYG'\n",
    "etf_index['SPDR S&P 500 Value'] = 'SPYV'\n",
    "etf_index['Robo Global Robotics and Automation Index ETF'] = 'ROBO'\n",
    "etf_index['iShares Exponential Technologies ETF'] = 'XT'\n",
    "etf_index['Defiance Quantum ETF'] = 'QTUM'\n",
    "etf_index['Vanguard 500 Index Fund'] = 'VOO'\n",
    "etf_index['iShares Core S&P 500 ETF'] = 'IVV'\n",
    "etf_index['SPDR Portfolio S&P 500 ETF'] = 'SPLG'\n",
    "\n",
    "etf_index_df = pd.DataFrame([etf_index])\n",
    "etf_index_df.to_csv('etf_index_df.csv', index = False)\n",
    "\n",
    "min_SuccessRate = int(min_SuccessRate * 0.9)\n",
    "etf_portfolio = []\n",
    "for i in etf:\n",
    "    strategy_analysis = strategy_analysis_main(etf_index[i], 'off', 'off')[0] + strategy_analysis_main(etf_index[i], 'off', 'off')[0]\n",
    "    if strategy_analysis >= 1 or strategy_analysis == -9:\n",
    "        print(i,':', etf_index[i])\n",
    "        etf_portfolio.append(etf_index[i])\n",
    "\n",
    "if day_check() != 'Saturday' and day_check() != 'Sunday' and timechecknow() > 390:\n",
    "    with open(\"etf_portfolio.txt\", 'w') as f:\n",
    "        for s in etf_portfolio:\n",
    "            f.write(str(s) + '\\n')  \n",
    "\n",
    "print(f'\\nTest Passed:\\n\\n{etf_portfolio}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835aadb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628f9cf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# file w/ the output\n",
    "!jupyter nbconvert --to ipynb SSS.QED.V5.0.ipynb \n",
    "\n",
    "\n",
    "# fils w/o the output\n",
    "from nbformat import read, write\n",
    "\n",
    "file_name = 'SSS.QED.V5.0'\n",
    "original_file_name = f'{file_name}.ipynb'\n",
    "new_file_name = f'{file_name}.backup.ipynb'\n",
    "\n",
    "def strip_output(nb):\n",
    "    for cell in nb.cells:\n",
    "        if hasattr(cell, \"outputs\"):\n",
    "            cell.outputs = []\n",
    "        if hasattr(cell, \"prompt_number\"):\n",
    "            del cell[\"prompt_number\"]\n",
    "            \n",
    "nb = read(open(original_file_name, encoding = 'utf8'), 4)\n",
    "strip_output(nb)\n",
    "write(nb, open(new_file_name, \"w\", encoding = 'utf8'), 4)\n",
    "\n",
    "\n",
    "# notification\n",
    "message = \"Subject: The performance review gets ready. Backup File Saved\"\n",
    "print(message)\n",
    "emailsend_to_server(message)  \n",
    "time.sleep(5)\n",
    "voice_message('The performance review gets ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b15ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": false
   },
   "source": [
    "____"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "1026.4px",
    "left": "21px",
    "top": "111.12px",
    "width": "254px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
